[
  {
    "title": "FFmpeg merges WebRTC support",
    "href": "https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00",
    "content": "projects\n/\nffmpeg.git\n/ commit\ncommit\ngrep\nauthor\ncommitter\npickaxe\n?\nsearch:\nre\nsummary\n|\nshortlog\n|\nlog\n| commit |\ncommitdiff\n|\ntree\n(parent:\nd4556c9\n) |\npatch\navformat/whip: Add WHIP muxer support for subsecond latency streaming\nauthor\nJack Lau\n<jacklau1222@qq.com>\nFri, 16 May 2025 12:15:05 +0000\n(20:15 +0800)\ncommitter\nSteven Liu\n<lq@chinaffmpeg.org>\nWed, 4 Jun 2025 03:17:07 +0000\n(11:17 +0800)\ncommit\n167e343bbe75515a80db8ee72ffa0c607c944a00\ntree\nb0810df5368369a80f749453e8ec0a26f2f70601\ntree\nparent\nd4556c98f02e4f2d3deb86efeb060ebe4659be96\ncommit\n|\ndiff\navformat/whip: Add WHIP muxer support for subsecond latency streaming\n0. WHIP Version 3.\n1. The WHIP muxer has been renamed and refined,\nwith improved logging context and error messages for SSL, DTLS, and RTC.\n2. Magic numbers have been replaced with macros and extracted to functions,\nand log levels have been altered for better clarity.\n3. DTLS curve list has been updated,\nand SRTP profile names have been refined for FFmpeg and OpenSSL.\n4. ICE STUN magic number has been refined,\nand RTP payload types have been updated based on Chrome's definition.\n5. Fixed frame size has been refined to rtc->audio_par->frame_size,\nand h264_mp4toannexb is now used to convert MP4/ISOM to annexb.\n6. OPUS timestamp issue has been addressed,\nand marker setting has been corrected after utilizing BSF.\n7. DTLS handshake and ICE handling have been optimized for improved performance,\nwith a single handshake timeout and server role to prevent ARQ.\n8. Consolidated ICE request/response handling and DTLS handshake into a single function,\nand fixed OpenSSL build errors to work with Pion.\n9. Merge TLS & DTLS implementation, shared BIO callbacks, read, write,\nprint_ssl_error, openssl_init_ca_key_cert,\ninit_bio_method function and shared same data structure\n10. Modify configure that whip is enabled only dtls is\nenabled(just support openssl for now) to fix build error\nCo-authored-by: winlin <winlinvip@gmail.com>\nCo-authored-by: yangrtc <yangrtc@aliyun.com>\nCo-authored-by: cloudwebrtc <duanweiwei1982@gmail.com>\nCo-authored-by: Haibo Chen <495810242@qq.com>\nCo-authored-by: Steven Liu <lq@chinaffmpeg.org>\nCo-authored-by: Jun Zhao <barryjzhao@tencent.com>\nSigned-off-by: Jack Lau <jacklau1222@qq.com>\nSigned-off-by: Steven Liu <lq@chinaffmpeg.org>\n13 files changed:\nconfigure\ndiff\n|\nblob\n|\nhistory\ndoc/muxers.texi\ndiff\n|\nblob\n|\nhistory\nlibavformat/Makefile\ndiff\n|\nblob\n|\nhistory\nlibavformat/allformats.c\ndiff\n|\nblob\n|\nhistory\nlibavformat/avio.c\ndiff\n|\nblob\n|\nhistory\nlibavformat/http.c\ndiff\n|\nblob\n|\nhistory\nlibavformat/http.h\ndiff\n|\nblob\n|\nhistory\nlibavformat/protocols.c\ndiff\n|\nblob\n|\nhistory\nlibavformat/srtp.h\ndiff\n|\nblob\n|\nhistory\nlibavformat/tls.c\ndiff\n|\nblob\n|\nhistory\nlibavformat/tls.h\ndiff\n|\nblob\n|\nhistory\nlibavformat/tls_openssl.c\ndiff\n|\nblob\n|\nhistory\nlibavformat/whip.c\n[new file with mode: 0644]\nblob\nFFmpeg git repo\nRSS\nAtom"
  },
  {
    "title": "Why I wrote the BEAM book",
    "href": "https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/",
    "content": "SERVICES\nCASES\nABOUT\nBLOG\nSERVICES\nCASES\nABOUT\nBLOG\nHappi Hacking AB\nKIVRA: 556912-2707\n106 31 Stockholm\n+46735047442\ninfo@happihacking.se\ncareers@happihacking.se\nLinkedIn\nHacker's Handbook\nWhy I Wrote the BEAM Book\nPost-mortems, coffee, and a decade of stubborn curiosity\nPosted:  2025-06-03\nCategories:\ndevelopment\n,\nErlang\n,\nBEAM\n,\ntheBeamBook\n,\nwriting\nWhy I wrote the Beam Book\nAfter ten years of keeping Klarna’s core system upright I know this: a 15\nmillisecond pause in the BEAM can stall millions of peak-shopping payments, trigger a 3 a.m. Christmas-Eve post-mortem, and earn you a very awake call from the CEO. I wrote\nThe BEAM Book\nso the next engineer fixes that pause before the coffee cools.\nOrigins\nI opened the project on 12 October 2012 with a lone DocBook file with four lines of text and an oversized sense of optimism.\nAfter two weeks, the commit log is mostly me adding structure, moving\nheadings, and updating metadata. Most of it is scaffolding. The actual\ncontent is still just a few hopeful lines.\nBy November I had abandoned DocBook for AsciiDoc, written a custom build\nscript, and convinced myself the book could be wrapped up in six months.\nThose early commits glow with energy: adds, rewrites, then more\nrewrites to fix the rewrites.\nDelusion is underrated.\nIn 2013 I managed to convince O’Reilly to publish. Moving the repo to their\nAtlas system sounded simple until Atlas began hiding my main file and\noverwriting half-finished chapters.\nThe Git history reads like a diary of frustration:\n“Moving files to top level to cope with Atlas,” “Atlas seems to be\noverwriting book.asciidoc”. Word count shot past 120 000 while actual\nprogress crawled. On 10 March 2015 I was literally “Smashing chapters into sections” just to keep the build green.\nThe quiet cancellation came two months later. No drama, just a polite call and a line through the contract. Relief mingled with embarrassment, I had spent two years rearranging files rather than finishing sentences.\nPragmatic Bookshelf took over that same year. I kept working in CVS for\ntheir production system, but progress was slow. Eventually, they cancelled\ntoo. On 20 January 2017, I imported everything into a new repo in one\nmassive commit: 6,622 files, over a million lines.\nThe rewrite stalled, and so did the project.\nOn 23 March 2017 I started fresh with Asciidoctor in a private GitHub repo, copy-pasting\nonly the parts that still made sense. Two weeks later, on April 7, minutes before\na lecture at Chalmers, I flipped the repository public. Within twenty-four\nhours strangers fixed typos, added diagrams, and merged a Creative Commons\nBY-4.0 license.\nWhat Kept Me Going\nI kept going because I wanted to understand the BEAM properly. There’s\nvalue in following the real logic, not just the surface explanations.\nCommunity feedback made a difference. As soon as the repo was public,\npeople began sending corrections, examples, and improvements.\nSeeing the numbers of people starring the repo on GitHub kept me going.\nOne highlight:\nIssue #113 – “Please continue being awesome.”\nThat emoji-laced drive-by encouragement (August 2018) still pops into my\nhead whenever motivation dips.\nThe book started showing up as a reference in Erlang and BEAM conference\ntalks, sometimes several times in the same event. That was a clear signal\nthat others needed this as much as I did.\nEven Twitter (in the good old days of Twitter) played a role. Whenever\nsomeone mentioned the book or shared a\nlink, it was an extra nudge to keep at it.\nMostly, I just wanted a manual I could trust myself, a reference for the\nparts of the VM that matter when things go wrong. That’s reason enough to\nkeep writing, even after the third rewrite.\nWhat’s Inside the Book & Who It Helps\nThe book covers what I wish I’d had when building and operating large\nErlang systems:\nSchedulers and process management: How the BEAM schedules,\nprioritizes, and balances processes under real load.\nProcesses and their memory: How process heaps,\nstack, messages, and binaries are managed and\nwhy these details matter in production.\nGarbage collection and memory: What actually happens\nwith per-process and global garbage collectors, binary references,\nand memory leaks.\nTagging schemes and terms: How the BEAM represents data—integers,\nfloats, tuples, binaries, references—down to the tagging bits.\nThe compiler and the VM: How code is turned into instructions,\nwhat the compiler does (and doesn’t do), and how the emulator executes it.\nTracing and debugging: Practical use of dbg, erlang:trace,\nand other tools to follow messages, events, and identify bottlenecks.\nPerformance tuning: What matters when profiling real code,\nunderstanding reductions, and tracking down real-world latency problems.\nSystem architecture: How ERTS, the BEAM VM, and their subsystems\nactually work together in a running node.\nIf you build or operate Erlang or Elixir systems, especially under any kind\nof scale—this book is for you. It saves you from hunting through mailing\nlists, scattered docs, and code comments just to answer, “Why is the VM\nbehaving like this?”\nLessons Learned\nPersistence beats perfection. Two cancelled publishing deals look bad on a\nrésumé, but an unfinished idea looks worse.\nBoundaries matter. I made progress by blocking time for writing, turning\noff notifications, and treating focus like a real deadline. Fika at 14:30\nis non-negotiable.\nThe crowd helps. Making the repo public brought in corrections,\nencouragement, and the occasional nudge when motivation was low.\nScope is everything. I cut the details on dirty schedulers, the new JIT,\nand the debugger. Maybe those will end up in an appendix, but not in the\ncore.\nShip, then iterate. The BEAM changes every year. A living Git repo keeps\nup.\nA real deadline helps. This January, during my yearly review, I\ndecided to print the book in time for Code Beam Stockholm. I thought I had\nuntil autumn, turns out the conference was June 2. That’s how you find out\nwhat’s truly essential.\nDefinition of Done\nHolding the print in my hands, it finally feels finished, at least for now. Years of scattered commits are bound into something real, so I’m calling it done.\nGet Involved\nYou can now get the paperback—The BEAM Book 1.0 is live on Amazon. Buy it\nhere.\nAmazon\nIf you spot an error, want to improve something, or just want to see how it\nworks under the hood, star or fork the repo. File an issue or, even better,\nsubmit a pull request. Contributors are credited in the acknowledgments.\nGitHub: theBeamBook\nIf you read the book, please leave an honest review.\nAlgorithms notice real feedback more than marketing copy.\nIf your team wants a deep dive, I run hands-on BEAM internals\nworkshops, tailored for real systems, not just hello world.\nEmail me if that’s what you need.\nhappi@happihacking.com\n- Happi\nBack to blog index.\nHappi Hacking AB\nKIVRA: 556912-2707\n106 31 Stockholm\n+46735047442\ninfo@happihacking.se\ncareers@happihacking.se\nLinkedIn\nCopyright © 2019-2025 Happi Hacking AB\nSERVICES\nCASES\nABOUT\nBLOG"
  },
  {
    "title": "IRS Direct File on GitHub",
    "href": "https://chrisgiven.com/2025/05/direct-file-on-github/",
    "content": "Skip to main content\nby\nChris Given\nTop level navigation menu\nHome\nAbout\nArchive\nConnect\nDirect File on GitHub\nFriday, May 30, 2025\nThe IRS has now published\nthe vast majority of Direct File’s code on GitHub\nas open-source software. As a work of the U.S. government, Direct File is in the public domain. And now everyone can check it out.\nReleasing Direct File’s source code demonstrates that the IRS is fulfilling its obligations under the\nSHARE IT Act\n(three weeks ahead of schedule!). Now that Direct File has paved the way, I hope that more of the IRS’s code, paid for with taxpayer dollars, will soon be available to all of us.\nOpen sourcing Direct File has long been planned, and even longer desired. Explaining last May why open source is particularly important for Direct File,\nthe team wrote\n:\nThe IRS could take further steps to build public trust and enable independent assessment of its work. The Direct File product team was given the mandate to develop software that ensures every taxpayer receives the full benefit of any tax provisions for which they are eligible. Releasing components of Direct File as open-source software would enable the team to demonstrate this commitment.\nEstablishing trust with taxpayers was core to our approach for designing and building Direct File. By creating the most accurate option for filing, by making taxes accessible to all, by keeping taxpayer data secure, and now, by publicly sharing Direct File’s code, the Direct File team showed our dedication to earning taxpayers’ trust.\nPlease note: As of two weeks ago, I no longer work at the IRS. I am writing solely in my personal capacity.\nhttps://github.com/IRS-Public/direct-file"
  },
  {
    "title": "Show HN: GPT image editing, but for 3D models",
    "href": "https://www.adamcad.com/",
    "content": "See it in Action\nSee it in Action\nPricing\nCareers\nGet In Touch\nSign In\nSpeak anything into\nexistence\nSpeak anything into\nexistence\nAdamCAD is an AI Powered CAD platform that generates 3D designs in seconds\nTry AdamCAD Now\nTry AdamCAD Now\nHow AdamCAD works\nHow AdamCAD works\nText to CAD\nUse prompts in AdamCAD to describe and edit the 3D\nmodel you want to create.\nRefine & Export\nAdamCAD generates the 3D Model and a list of parameters based on your design to refine it further.\nImage to 3D\nAdamCAD's creative mode turns any image into a 3D generation in seconds.\nAdamCAD AI Co-Pilot\nAdamCAD is built to integrate with the CAD software professionals rely on.\nBuild anything with natural language\nBuild anything with natural language\nWhether for industrial design or mechanical engineering, Adam brings ideas to life.\nMockup a camshaft for a 4 stroke engine\n20-tooth spur gear with a 20° pressure angle, 2.5 mm pitch\nGenerate a wall mounted key holder\n3D printable phone stand\nSmall desktop plant pot for succulents\nDesign a futuristic mug\nCreate an enclosure for a raspberry pi 4\nMake a toothbrush holder\nMockup a camshaft for a 4 stroke engine\nMockup a camshaft for a 4 stroke engine\n20-tooth spur gear with a 20° pressure angle, 2.5 mm pitch\n20-tooth spur gear with a 20° pressure angle, 2.5 mm pitch\nGenerate a wall mounted key holder\n3D printable phone stand\nCreate a snap-fit case for an arduino nano\nCreate a snap-fit case for an arduino nano\nMake a toothbrush holder\nMake a toothbrush holder\nSmall desktop plant pot for succulents\nSmall desktop plant pot for succulents\nDesign a futuristic mug\nDesign a futuristic mug\nCreate an enclosure for a raspberry pi 4\nCreate an enclosure for a raspberry pi 4\nGenerate a wall mounted key holder\nGenerate a wall mounted key holder\n3D printable phone stand\n3D printable phone stand\nSign In\nSign In\n@makewithadam\nhello@adamcad.com\n© 2025  All Rights Reserved AdamCAD"
  },
  {
    "title": "VC money is fueling a global boom in worker surveillance tech",
    "href": "https://restofworld.org/2025/employee-surveillance-software-vc-funding/",
    "content": "Skip to content\nRest of World\nReporting Global Tech Stories\nGlobal\nLabor\nVC money is fueling a global boom in worker surveillance tech\nAbout\nLatest\nNewsletters\nExplore by\nRegions\nAfrica\nAsia\nChina\nEastern Europe\nLatin America\nSouth Asia\nMiddle East & North Africa\nSections\nAccess & Connectivity\nCreators & Communities\nE-commerce\nLabor\nMoney\nPolitics\nThe Platform Economy\nBeats\nTech  Giants\nChina Outside China\nEV Revolution\nInnovation\nLearn more\nAbout us\nTeam news\nWork with us\nSupport us\nPlatforms\nListen\nFollow us\nDark Mode\nLabor\nVC money is fueling a global boom in worker surveillance tech\nA funding surge has given rise to technologies to track, analyze and manage workers — often in countries with little regulation.\nRest of World/Getty Images\nRest of World/Getty Images\nBy\nGayathri Vaidyanathan\n3 June 2025\nStartups selling bossware products are mushrooming globally.\nThe \"Little Tech\" ecosystem is under-regulated and largely funded by venture capital.\nWorkers say they feel a loss of autonomy when they are managed by an algorithm rather than a human.\nTechnologies that promise to track, manage, and supervise workers, increasingly using artificial intelligence, are getting entrenched in the developing world, according to a new\nreport\nby Coworker.org, a labor rights nonprofit based in New York.\nAudits of more than 150 startups and regional companies based in Kenya, Nigeria, Colombia, Brazil, Mexico, and India showed workplace surveillance is expanding in scale and sophistication, the researchers said. While large\ncorporations\nare\nknown\nto develop surveillance technologies, a so-called Little Tech ecosystem of mostly unregulated, venture capital-funded startups and small vendors making these products has grown since Covid-19, the report found. The term “Little Tech” was popularized by the VC firm\nAndreessen Horowitz\n, which argued that excessive regulation was stifling innovation.\nAlgorithmic management\nand surveillance tools are getting even more intrusive in gig work, and are entering offices and the informal labor sector as well, Wilneida Negrón, director of research and policy at Coworker.org and a co-author of the report, told\nRest of World\n.\n“The pressure of the hyper-surveillance creates a lot of stress and creates a lot of uncertainty for workers. It brings a culture of suspiciousness,” she said.\nInvestments by Silicon Valley-based VC firms led to a boom in tech startups globally after Covid-19, Negrón said. This has carried over to companies building bossware products in the developing world, she said.\nThe technologies include biometric tracking, AI-powered productivity monitoring, and predictive analytics, the report found. Worker data is continuously collected and analyzed by algorithms with the stated aim to improve hiring, evaluate performance, and optimize processes.\nMost managers in wealthier nations say algorithmic management tools improve their decision-making, according to a 2024\nsurvey\nof over 6,000 employers by the Organisation for Economic Co-operation and Development. More than 90% of American managers used such tools, especially to reward or sanction employees.\nMany tools are first deployed in Latin America, where labor laws are less strictly enforced, according to Ayden Férdeline, a tech policy researcher in Berlin and a co-author of the report.\n“There is a Latin America testing ground for products,” he told\nRest of World\n. “If they are successful, they tend to be deployed in other jurisdictions, oftentimes with additional safeguards, sometimes not.”\nMany workers are unaware of how their information is collected and used, Férdeline said.\nSome gig workers in Kenya, Guatemala, and Brazil said bossware tools make them feel surveilled, and that they have less control over their work. In Porto Alegre, Brazil, Uber driver Carina Trindade told\nRest of World\nshe feels the app monitors her continuously, tracking her speed and braking patterns. The app has permissions\nto access\nher mic and camera, she said.\nUber spokesperson Gabriel Gabira said drivers have the option to record trips, and\nprivacy terms\nare followed to access the footage.\nIn Nairobi, Godfrey Sanya Wanga, a driver for ride-hailing firm SafeBoda, told\nRest of World\nhe felt the app undercharged a customer. “I really wanted to ask [the customer] to pay me more, but I remembered that I was being monitored and this would bring me trouble if the client reported me,” he said. SafeBoda did not respond to a request for comment.\nSeveral nations have data protection and privacy laws, including Brazil, Nigeria, and Kenya. But enforcement is inconsistent, the report said.\nHere are five current uses of algorithmic management tools. The companies mentioned below did not comment, unless otherwise stated.\n1.\nTimekeeping and attendance systems\nWhat: Platforms that track the attendance of workers, often using geolocation and biometrics to verify presence.\nExample:\nRankmi\n, based in Chile, uses biometrics and geolocation to track workers. The platform also\ngives\nworkers continuous performance feedback and evaluates job applicants using AI.\n2.\nBiometric and identity verification tools\nWhat: Tools that use fingerprint and facial-recognition checks, special digital signatures stored on a secure network, and official records to confirm a worker’s identity before granting access.\nExample: Cincel, based in Mexico, provides\nidentity verification\ntools that do various checks including biometrics, and also cross-check against government databases and\nblacklists\n.\n3.\nPerformance and productivity monitoring platforms\nWhat: Dashboards that score workers using tracked metrics such as keystrokes, transaction counts, customer interactions, and task completion times.\nExample:\nAhgora\n, based in Brazil, offers HR software that allows managers to continually “oversee team attendance in real-time” and that tracks productivity. It uses the data to offer predictions about work, such as potential issues with attendance, which can inform decision-making.\n4.\nAlgorithmic management and predictive analytics\nWhat: Platforms that\nautomate HR functions, such as hiring shortlists, performance reviews, attrition forecasting, and also unionization-risk scoring.\nExample: Visier’s AI-powered analytics platform\nanalyzes\nHR data and provides insights, including resignation risk. The platform is used by global firms including Deloitte, Accenture, and Tata Consultancy Services.\nAndrea Derler, principal of research and customer value at Visier, told\nRest of World\nthe platform only “processes data that organizations load into the platform, and we are not responsible for the way the data and insights we help provide is being used.”\n5.\nGig economy and field workforce tracking\nWhat: Apps that use the workers’ smartphones to dispatch and route deliveries. They use location, trip history, and ratings to allocate jobs and\nevaluate performance\n. Workers are managed mostly by platforms rather than humans.\nExample:\nRappi\ni\nRappi\nRappi, a Colombian company, has been providing delivery services across most Latin American countries since 2015.\nREAD MORE\n, a Colombian delivery app, tracks workers in real time. It has\nauto accept\n, where a rider can’t decline orders — and it’s mandatory to qualify for bonuses. Delivery worker Carolina Ramírez told\nRest of World\nshe works 14-hour days to earn a bonus of 100,000 pesos ($25) every week, leaving her little time for anything else. “My boss is the app. It’s unfair because to earn a good salary, I have to dedicate myself almost exclusively to this,” she said.\nGayathri Vaidyanathan\nis the Labor x Tech editor at Rest of World based in Stuttgart, Germany.\nStephanie Wangari, Pedro Nakamura and Laura Rodriguez Salamanca contributed to this report.\nRead more stories\nFeatures\nThe math tutor and the missing $533 million\nFrom Dubai, the enigmatic founder of Byju’s recounts how he made his empire — and vows to rebuild it.\nBy\nYudhijit Bhattacharjee\nInnovation\nChina is gaining ground in the global race to develop AI agents\nA new wave of startups promise to automate everyday tasks — from app development to travel planning — better than chatbots.\nBy\nKinling Lo\nInnovation\nIn a world first, Brazilians will soon be able to sell their digital data\nBrazil is piloting dWallet, a project that lets citizens earn money from their data. It is ahead of similar U.S.-based initiatives.\nBy\nGabriel Daros\nAbout us\nJobs\nPrivacy Policy\nPlatforms\nNewsletters\nDonate\nContact us\n© 2025 Rest of World"
  },
  {
    "title": "The Right to Repair Is Law in Washington State",
    "href": "https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state",
    "content": "Skip to main content\nAbout\nContact\nPress\nPeople\nOpportunities\nIssues\nFree Speech\nPrivacy\nCreativity and Innovation\nTransparency\nInternational\nSecurity\nOur Work\nDeeplinks Blog\nPress Releases\nEvents\nLegal Cases\nWhitepapers\nPodcast\nAnnual Reports\nTake Action\nAction Center\nElectronic Frontier Alliance\nVolunteer\nTools\nPrivacy Badger\nSurveillance Self-Defense\nCertbot\nAtlas of Surveillance\nCover Your Tracks\nStreet Level Surveillance\napkeep\nDonate\nDonate to EFF\nGiving Societies\nShop\nSponsorships\nOther Ways to Give\nMembership FAQ\nDonate\nDonate to EFF\nShop\nOther Ways to Give\nSearch form\nSearch\nEmail updates on news, actions,\nand events in your area.\nJoin EFF Lists\nCopyright (CC BY)\nTrademark\nPrivacy Policy\nThanks\nElectronic Frontier Foundation\nDonate\nPodcast Episode: Why Three is Tor's Magic Number\nElectronic Frontier Foundation\nAbout\nContact\nPress\nPeople\nOpportunities\nIssues\nFree Speech\nPrivacy\nCreativity and Innovation\nTransparency\nInternational\nSecurity\nOur Work\nDeeplinks Blog\nPress Releases\nEvents\nLegal Cases\nWhitepapers\nPodcast\nAnnual Reports\nTake Action\nAction Center\nElectronic Frontier Alliance\nVolunteer\nTools\nPrivacy Badger\nSurveillance Self-Defense\nCertbot\nAtlas of Surveillance\nCover Your Tracks\nStreet Level Surveillance\napkeep\nDonate\nDonate to EFF\nGiving Societies\nShop\nSponsorships\nOther Ways to Give\nMembership FAQ\nDonate\nDonate to EFF\nShop\nOther Ways to Give\nSearch form\nSearch\nThe Right to Repair Is Law in Washington State\nDEEPLINKS BLOG\nBy\nHayley Tsukayama\nand\nCory Doctorow\nJune 3, 2025\nThe Right to Repair Is Law in Washington State\nShare It\nShare on Twitter\nShare on Facebook\nCopy link\nThanks in part\nto your support\n, the right to repair is now law in Washington.\nGov. Bob Ferguson signed two bills guaranteeing Washingtonians' right to access tools, parts, and information so they can fix personal electronics, appliances, and wheelchairs. This is the epitome of common-sense legislation. When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.\nWhen you own something, you should have the final say about who fixes, adapts, or modifies it—and how.\nAdvocates in Washington have worked for years to pass a strong right-to-repair law in the state. In addition to Washington’s\nPublic Interest Research Group\n, the consumer electronics bill moved forward with a growing group of supporting organizations, including environmental advocates, consumer advocates, and manufacturers such as Google and Microsoft. Meanwhile, advocacy from groups including\nDisability Rights Washington\nand the\nHere and Now Project\nmade the case for the wheelchair's inclusion in the right-to-repair bill, bringing their personal stories to Olympia to show why this bill was so important.\nAnd it’s not just states that recognize the need for people to be able to fix their own stuff.  Earlier this month, U.S. Army Secretary Dan Driscoll\nissued a memo\nstating that the Army should “[identify] and propose contract modifications for right to repair provisions where intellectual property constraints limit the Army's ability to conduct maintenance and access the appropriate maintenance tools, software, and technical data – while preserving the intellectual capital of American industry.” The memo said that the Army should seek this in future procurement contracts and also to amend existing contracts to include the right to repair.\nThis is a bedrock of sound procurement with a long history in America. President Lincoln only bought rifles with standardized tooling to outfit the Union Army, for the obvious reason that it would be a little embarrassing for the Commander in Chief to have to pull his troops off the field because the Army’s sole supplier had decided not to ship this week’s delivery of ammo and parts. Somehow, the Department of Defense forgot this lesson over the ensuing centuries, so that today, billions of dollars in public money are spent on material and systems that the US military can only maintain by buying service from a “beltway bandit.”\nThis recognizes what millions of people have said repeatedly: limiting people’s ability to fix their own stuff stands in the way of needed repairs and maintenance. That’s true whether you’re a farmer with a broken tractor during harvest, a homeowner with a misbehaving washing machine or a cracked smartphone screen, a hospital med-tech trying to fix a ventilator, or a soldier struggling with a broken generator.\nThe right to repair is gaining serious momentum. All 50 states have now considered some form of right-to-repair legislation. Washington is the eighth state to pass one of these bills into law—let’s keep it up.\nShare It\nShare on Twitter\nShare on Facebook\nCopy link\nJoin EFF Lists\nDiscover more.\nEmail updates on news, actions, events in your area, and more.\nEmail Address\nPostal Code (optional)\nAnti-spam question: Enter the three-letter abbreviation for\nElectronic Frontier Foundation\n:\nDon't fill out this field (required)\nThanks, you're awesome! Please check your email for a confirmation link.\nOops something is broken right now, please try again later.\nDiscover more.\nEmail updates on news, actions, events in your area, and more.\nEmail Address\nPostal Code (optional)\nAnti-spam question: Enter the three-letter abbreviation for\nElectronic Frontier Foundation\n:\nDon't fill out this field (required)\nThanks, you're awesome! Please check your email for a confirmation link.\nOops something is broken right now, please try again later.\nShare It\nShare on Twitter\nShare on Facebook\nCopy link\nBack to top\nFollow EFF:\nmastodon\nfacebook\ninstagram\nx\nBlue Sky\nyoutube\nflicker\nlinkedin\ntiktok\nthreads\nCheck out our 4-star rating on\nCharity Navigator\n.\nContact\nGeneral\nLegal\nSecurity\nMembership\nPress\nAbout\nCalendar\nVolunteer\nVictories\nHistory\nInternships\nJobs\nStaff\nDiversity & Inclusion\nIssues\nFree Speech\nPrivacy\nCreativity & Innovation\nTransparency\nInternational\nSecurity\nUpdates\nBlog\nPress Releases\nEvents\nLegal Cases\nWhitepapers\nEFFector Newsletter\nPress\nPress Contact\nDonate\nJoin or Renew Membership Online\nOne-Time Donation Online\nGiving Societies\nCorporate Giving and Sponsorship\nShop\nOther Ways to Give\nCopyright (CC BY)\nTrademark\nPrivacy Policy\nThanks\nJavaScript license information"
  },
  {
    "title": "Cloud Run GPUs, now GA, makes running AI workloads easier for everyone",
    "href": "https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available",
    "content": "Serverless\nCloud Run GPUs, now GA, makes running AI workloads easier for everyone\nJune 2, 2025\nSteren Giannini\nDirector, Product Management\nYunong Xiao\nDirector, Engineering\nTry Gemini 2.5\nOur most intelligent model is now available on Vertex AI\nTry now\nDevelopers love\nCloud Run\n, Google Cloud’s serverless runtime,\nfor its simplicity, flexibility, and scalability. And today, we’re thrilled to announce that NVIDIA GPU support for Cloud Run is now generally available, offering a powerful runtime for a variety of use cases that’s also remarkably cost-efficient.\nNow, you can enjoy the following benefits across both GPUs and CPUs:\nPay-per-second billing\n: You are only charged for the GPU resources you consume, down to the second.\nScale to zero\n: Cloud Run automatically scales your GPU instances down to zero when no requests are received, eliminating idle costs. This is a game-changer for sporadic or unpredictable workloads.\nRapid startup and scaling\nGo from zero to an instance with a GPU and drivers installed in under 5 seconds, allowing your applications to respond to demand very quickly. For example, when scaling from zero (cold start), we achieved an impressive Time-to-First-Token of approximately 19 seconds for a gemma3:4b model (this includes startup time, model loading time, and running the inference)\nFull streaming support\n: Build truly interactive applications with out-of-the box support for HTTP and WebSocket streaming, allowing you to provide LLM responses to your users as they are generated.\nSupport for GPUs in Cloud Run is a significant milestone, underscoring our leadership in making GPU-accelerated applications simpler, faster, and more cost-effective than ever before.\n“Serverless GPU acceleration represents a major advancement in making cutting-edge AI computing more accessible. With seamless access to NVIDIA L4 GPUs, developers can now bring AI applications to production faster and more cost-effectively than ever before.”\n- Dave Salvator, director of accelerated computing products, NVIDIA\nTry Google Cloud for free\nStart building with $300 in free credits for new customers. All customers get free usage of 20+ products, up to monthly limits.\nGet started for free\nAI inference for everyone\nOne of the most exciting aspects of this GA release is that Cloud Run GPUs are now available to everyone for NVIDIA L4 GPUs, with\nno quota request required\n.This removes a significant barrier to entry, allowing you to immediately tap into GPU acceleration for your Cloud Run services. Simply use\n--gpu 1\nfrom the Cloud Run command line, or check the \"GPU\" checkbox in the console, no need to request quota:\nProduction-ready\nWith general availability, Cloud Run with GPU support is now covered by Cloud Run's\nService Level Agreement (SLA)\n, providing you with assurances for reliability and uptime. By default, Cloud Run offers\nzonal redundancy\n, helping to ensure enough capacity for your service to be resilient to a zonal outage; this also applies to Cloud Run with GPUs. Alternatively, you can turn off zonal redundancy and benefit from a\nlower price\nfor best-effort failover of your GPU workloads in case of a zonal outage.\nMulti-regional GPUs\nTo support global applications, Cloud Run GPUs are available in five Google Cloud\nregions\n: us-central1 (Iowa, USA), europe-west1 (Belgium), europe-west4 (Netherlands), asia-southeast1 (Singapore), and asia-south1 (Mumbai, India), with more to come.\nCloud Run also\nsimplifies deploying your services across multiple regions\n. For instance, you can deploy a service across the US, Europe and Asia with a single command, providing global users with lower latency and higher availability. For instance, here’s how to deploy\nOllama\n, one of the easiest way to run open models, on Cloud Run across three regions:\nLoading...\ngcloud run deploy my-global-service \\\r\n  --image ollama/ollama --port 11434 \\\r\n  --gpu 1 \\\r\n  --regions us-central1,europe-west1,asia-southeast1\nSee it in action: 0 to 100 NVIDIA GPUs in four minutes\nYou can witness the incredible scalability of Cloud Run with GPUs for yourself with\nthis live demo\nfrom Google Cloud Next 25, showcasing how we scaled from 0 to 100 GPUs in just four minutes.\nLoad testing a Stable Diffusion service running on Cloud Run GPUs to 100 GPU instances in four minutes.\nUnlock new use cases with NVIDIA GPUs on Cloud Run jobs\nThe power of Cloud Run with GPUs isn't just for real-time inference using request-driven Cloud Run services. We're also excited to announce the availability of GPUs on\nCloud Run jobs\n, unlocking new use cases, particularly for batch processing and asynchronous tasks:\nModel fine-tuning\n: Easily fine-tune a pre-trained model on specific datasets without having to manage the underlying infrastructure. Spin up a GPU-powered job, process your data, and scale down to zero when it’s complete.\nBatch AI inferencing\n: Run large-scale batch inference tasks efficiently. Whether you're analyzing images, processing natural language, or generating recommendations, Cloud Run jobs with GPUs can handle the load.\nBatch media processing\n: Transcode videos, generate thumbnails, or perform complex image manipulations at scale.\nSign up\nfor the private preview of GPUs on Cloud Run jobs.\nWhat Cloud Run customers are saying\nDon't just take our word for it. Here's what some early adopters of Cloud Run GPUs are saying:\n\"Cloud Run helps vivo quickly iterate AI applications and greatly reduces our operation and maintenance costs. The automatically scalable GPU service also greatly improves the efficiency of our AI going overseas.”\n- Guangchao Li, AI Architect, vivo\n\"L4 GPUs offer really strong performance at a reasonable cost profile. Combined with the fast auto scaling, we were really able to optimize our costs and saw an 85% reduction in cost. We've been very excited about the availability of GPUs on Cloud Run.\"\n- John Gill at\nNext'25\n, Sr. Software Engineer, Wayfair\n\"At Midjourney, we have found Cloud Run GPUs to be incredibly valuable for our image processing tasks. Cloud Run has a simple developer experience that lets us focus more on innovation and less on infrastructure management. Cloud Run GPU’s scalability also lets us easily analyze and process millions of images.\n\" - Sam Schickler, Data Team Lead, Midjourney\nGet started today\nCloud Run with GPU is ready to power your next generation of applications. Dive into the\ndocumentation\n, explore our\nquickstarts\n, and review our\nbest practices for optimizing model loading\n. We can't wait to see what you build!\nPosted in\nServerless\nApplication Modernization\nCompute\nRelated articles\nApplication Modernization\nFlipping out: Modernizing a classic pinball machine with cloud connectivity\nBy Drew Brown • 5-minute read\nApplication Development\nRun your AI inference applications on Cloud Run with NVIDIA GPUs\nBy Sagar Randive • 8-minute read\nServerless\nCloud Functions is now Cloud Run functions — event-driven programming in one unified serverless platform\nBy James Ma • 4-minute read\nContainers & Kubernetes\nFlexible committed-use discounts are now even more flexible\nBy William Denniss • 3-minute read"
  },
  {
    "title": "When memory was measured in kilobytes: The art of efficient vision",
    "href": "https://www.softwareheritage.org/2025/06/04/history_computer_vision/",
    "content": "Close\nMission\nHeritage\nScience\nIndustry\nApproach\nArchive\nFeatures\nBrowse\nSave Code Now\nSave Research Software\nBenefits\nGuidelines (HOWTO)\nSave Legacy Code\nSWH Acquisition Process\nSoftware Stories\nBrowser extensions\nMirror Network\nCommunity\nUsers\nAmbassadors\nDevelopers\nScientists\nStudents\nGrants\nSupport us\nSponsors\nInterest groups\nALIG members\nPartners\nMirrors\nTestimonials\nDonate\nAbout\nFAQ\nTeam\nWork with us\nCommunication kit\nNews\nBlog\nEvents\nNewsletter\nPublications\nAnnual Reports\nDonate\nEnglish\nFrançais\nEspañol\nHome\nDevelopment\nDocumentation\nOperational\nLogin\nSoftware Heritage\nMission\nHeritage\nScience\nIndustry\nApproach\nArchive\nFeatures\nBrowse\nSave Code Now\nSave Research Software\nBenefits\nGuidelines (HOWTO)\nSave Legacy Code\nSWH Acquisition Process\nSoftware Stories\nBrowser extensions\nMirror Network\nCommunity\nUsers\nAmbassadors\nDevelopers\nScientists\nStudents\nGrants\nSupport us\nSponsors\nInterest groups\nALIG members\nPartners\nMirrors\nTestimonials\nDonate\nAbout\nFAQ\nTeam\nWork with us\nCommunication kit\nNews\nBlog\nEvents\nNewsletter\nPublications\nAnnual Reports\nDonate\nEnglish\nFrançais\nEspañol\nJune 4, 2025\nWhen memory was measured in kilobytes: The art of efficient vision\nBy Mathilde Fichen\nIn the early days of computer vision, when memory was scarce and every byte counted, innovation thrived under constraint. “An Efficient Chain-Linking Algorithm,” developed at Inria in the late 1980s, is a brilliant example of this spirit. Now preserved and shared by Software Heritage, this compact yet powerful piece of C code showcases how elegance and efficiency went hand in hand in outlining the future of image processing—one pixel chain at a time.\nThe code resulted from research work carried out between 1985 and 1991 at Inria, by Gérard Giraudon (research and principal investigator), Philippe Garnesson (a PhD student), and Patrick Cipière (software engineer). Down in sunny Sophia Antipolis, a\ntech park\n20 minutes inland from Antibes, the team tackled computer vision with a distinctly local flavor. They called themselves PASTIS, a playful nod to the anise drink. Still, the acronym – Scene Analysis and Symbolic Image Processing Project (Projet d’Analyse de Scène et de Traitement d’Image Symbolique) – hinted at their serious mission.\nGérard Giraudon\n, circa 1980\nPreserving Inria legacy software\nThe effort to preserve this source code is part of a broader initiative to preserve the legacy codes of\nInria\n(France’s National Institute for Research in Digital Science and Technology), launched in 2023 in a joint effort of Software Heritage, Inria Alumni, and Inria. The project to preserve Inria’s legacy software started by reaching out to the institute’s community, past and present, through a survey (as detailed in our\niPres article\n.)  This initial outreach informed a dedicated,\nhands-on workshop in 2024\n, which kicked off the practical work of exploring these historical codes. The current focus is on securely archiving the important legacy software we’ve identified within Software Heritage. Sharing the stories behind these codes with the broader community is just as vital.\nThe recovery of some code has been surprisingly straightforward. For instance, the code for “An Efficient Chain-Linking Algorithm” was readily accessible thanks to Gérard Giraudon’s personal preservation efforts on a local drive. That small success story is a reminder of how important individual initiative is for preserving digital work. Each piece of software recovered isn’t just code; it’s a piece of research history, carrying the stories of the people who created it.\nInria alumni workshop, October 2024, Inria Paris.\nAn algorithm for outlining images\nThe chain-linking algorithm processes a 2D pixel matrix, typically the output of a contour detection step.\nRaw image used as input for the algorithm. Source: Chaînage efficace de contour, n° 605, Février 1987, Gérard Giraudon\nThe output is a list of contour chains. Each chain in this list is a sequence of pixel coordinates that define a continuous boundary, ready for further steps like polygonal approximation or shape analysis..\nProcessed image. Source : Chaînage efficace de contour, n° 605, Février 1987, Gérard Giraudon\nBasically, the chain-linking connects the edge pixels to create smooth outlines, just like tracing the shape with a pencil.\nComputer vision in the 80s\nThe mid-1980s aspiration of seeing robots hit a speed bump: computer vision algorithms weren’t fast enough. The core challenge? Real-time performance – the essential ingredient for giving robots camera “eyes” that could truly see and react. At the time, such a system was conceived as a pipeline of operations from image acquisition (one shot or video) to a decision-making system.\nThe goal was to extract meaningful information from raw image data, typically represented as an 8-bit matrix of pixel intensities, and convert it into a graph in list form (i.e., from matrix to list). This process begins with detecting and recognizing shapes within the image, either in 2D or 3D, depending on the available data. From there, the system identifies objects and analyzes their spatial relationships, ultimately constructing a graph of semantic connections between them. This graph captures not just what the objects are, but how they relate to one another within the observed scene. In some cases, the analysis extends over time (3D + t), allowing for the interpretation of motion and dynamic interactions in a sequence of frames.\nSolving key memory issues\nThe algorithm was first developed on a PerkinElmer Model 3250 computer, seen above in a brochure\nvia 1000bit.\nAnother challenge in early image processing was the limited amount of short-term memory (RAM) available in computers. This made it essential to focus on reducing the amount of data stored and processed at any given time, while preserving important information.\nDue to these constraints, the Efficient Chain-Linking Algorithm could only store three lines of the image at a time while reading it line by line. As each new line was read, the algorithm would build and extend chains of connected pixels on the fly, without knowing how those chains might continue in future lines. Once the entire image was scanned, a final processing stage merged pixel chains belonging to the same contour and resolved junctions or branching points. Importantly, this was done using just one full pass over the data, making it memory-efficient.\nBut the algorithm wasn’t the only clever bit. From a programming standpoint, the code’s true ingenuity lay in its dynamic memory allocation for storing the chain lists. Back then, predicting memory needs upfront was impossible, making Patrick Cipiere’s approach an elegant solution to an unpredictable challenge.\nSource code for memory allocation\n(excerpt)\nComputer vision today\nWith the dramatic advancements in computer vision, fueled by deep learning and the prevalence of large memory capacities, contemporary methods often involve storing the full image and constructing contour chains sequentially, possibly necessitating multiple passes over the data, one for each chain. Yet, even with today’s abundant memory, this algorithm retains its power: remarkable efficiency when every byte counts. By processing the image sequentially, storing only a few lines at a time, and building pixel chains incrementally without looking back, it offers a lean and effective alternative to the memory-hungry approaches now common.\nLinks and references\nThe preserved source code can be found on Software Heritage archive:\nhttps://archive.softwareheritage.org/browse/origin/directory/?origin_url=https://github.com/mathfichen/chainage_de_contour\nThe original 79-page paper in French, “Chaînage efficace de contour, n° 605, Février 1987, Gerard Giraudon”\nhttps://inria.hal.science/inria-00075949/document\nAn Efficient Chain-Linking Algorithm, G. Giraudon, in The 5th Scandinavian Conference on Image Analysis, Stockholm, June 1987.\nA Real Time Parallel Edge Following in Single Pass, GG, in Workshop on Computer Vision, Miami, 1987\nChaînage efficace de contour, G. Giraudon, 3ème Colloque Image-Cesta, Paris, Mai 1987\nUn standard pour une représentation d’objets de type liste dans le domaine du traitement d’images, n° 82, décembre 1988 – 3ieme édition, P. Garnesson, G. Giraudon.\nUn standard pour une représentation d’objets de type liste dans le domaine du traitement d’images, n° 82, décembre 1988 – 3ieme édition, P. Garnesson, G. Giraudon.\nhttps://inria.hal.science/inria-00070061v1\n« L’approximation polygonale, bilans et perspectives », Rapport de recherche n°1621 INRIA, juin 1991  Garnesson P. and Giraudon G.\nhttps://inria.hal.science/inria-00074940v1/document\nThis work has resulted in citations and a hardware implementation presented at the 16th GRETSI COLLOQUIUM — SEPTEMBER 15-19, 1997\nJune 4, 2025\ncomputer vision\n,\ncomputing history\n,\nSoftware Heritage Archive\nBack to top\nFollow us\nJoin our newsletter\nAbout\nMission\nPeople\nLegal\nContact\nJoin us!\nHeritage\nScience\nIndustry\nJobs\nCommunity\nDevelopers\nScientists\nBlog\nDonate\nIn collaboration with\nSearch:\nEnglish\nFrançais\n(\nFrench\n)\nEspañol\n(\nSpanish\n)"
  },
  {
    "title": "Preventing Flash of Incomplete Markdown when streaming AI responses",
    "href": "https://engineering.streak.com/p/preventing-unstyled-markdown-streaming-ai",
    "content": "Streak Engineering\nSubscribe\nSign in\nShare this post\nStreak Engineering\nPreventing Flash of Incomplete Markdown when streaming AI responses\nCopy link\nFacebook\nEmail\nNotes\nMore\nPreventing Flash of Incomplete Markdown when streaming AI responses\nBlake Kadatz\nJun 04, 2025\nShare this post\nStreak Engineering\nPreventing Flash of Incomplete Markdown when streaming AI responses\nCopy link\nFacebook\nEmail\nNotes\nMore\nShare\nYou’ve heard of\nFlash of Unstyled Content (FOUC)\nbefore, where you see unstyled HTML appear, then the CSS loads and the browser suddenly updates to display the correct style.\nA similar problem exists when streaming responses generated by AI that I call “Flash of Incomplete Markdown” (FOIM). I’ve reproduced this within OpenAI’s playground by throttling my connection speed to dialup internet speeds:\nWhile this is greatly exaggerated due to the slow speed, you can see the incomplete markdown appear in chunks. This occurs because OpenAI’s streaming API returns an event stream where it builds up a response message. These chunks are provided by what are called output text deltas:\nYou can read more about output text deltas in their\nAPI docs\n.\nI’ve seen this same behavior occur in a number of commercially launched products, so this isn’t some obscure effect. We experienced this in our product as well. One of the AI features Streak offers is the ability to ask a question about one of your deals, where the answer may be in any related email threads, various comments, meeting notes, and so on.\nYou could ask “When did the customer sign the contract?” and getting back an answer of “Last Thursday” is useful, but even more useful is providing a link to the email thread from Thursday where they sent an email saying “Signed contract attached”. The Streak user can click on that and be confident that the answer is correct. You can see that here as I was implementing the functionality in my dev environment:\nWhat should have been output as trivial\n[source]\nlinks ended up rendering the incomplete markdown using a lengthy link format of\nhttps://streak.com/a/boxes/{KEY}/itemtype/{KEY}\nand only when the browser received the closing\n)\nof the markdown link did the content collapse to show just\n[source]\n.\nHallucinations\nAdditionally, when we launched this internally to Streak employees someone reported that one of the links was incorrect. It turns out that OpenAI was hallucinating one of the URLs we provided as the citation by combining the first part of the box key with the last part of the comment key. The two keys had a common prefix and OpenAI returned a mangled URL. This resulted in a 404 error since the hallucinated link was invalid.\nWhile the incomplete markdown issue is somewhat annoying, providing the user with incorrect links is unacceptable. So I set out to solve the hallucination issue.\nReally short links\nWhat if instead of\nhttps://streak.com/a/boxes/{KEY}/itemtype/{KEY}\nwhere the keys are lengthy (the URL can be over 200 characters long), the link was simply “#REF3”? Due to the tiny amount of tokens that takes, it’d be significantly less likely that OpenAI would hallucinate by combining unrelated parts of the output due to some shared common prefix. We decided to change the link text to output Wikipedia style numbers, where the links would get output as:\n[1](#REF3)\nand so on for each reference\n1\n. This would work great, and we could simply keep track of which short references map to which original links and replace them on the fly. But where would we perform this substitution? The server streams the output text deltas from OpenAI’s API to the client and each delta is completely arbitrary, where one delta could be “#”, the next delta might be “REF”, with another delta of just “3”. Would we need to provide the client with the complete mapping ahead of time so it could replace the assembled markdown as it’s received? This gets complicated as we may have dozens of citations and the answer may only need to cite one of them.\nBring on the state machine\nIt turns out there’s an easier, far less complicated solution where we can dynamically detect the start of a markdown link and begin buffering the output on the server, not sending anything to the client until the link has completed. This is simple to do via a state machine. The four states are\nTEXT\n,\nLINK_TEXT\n,\nEXIT_LINK_TEXT\n, and\nLINK_URL\nwith the following behaviors:\nRegardless of state, the\n\\\ncharacter acts as an escape character, so we output the next character without any additional consideration.\nStart out in\nTEXT\nstate, outputting normal text that’s not a link, streaming tokens to the client.\nBeginning of markdown link via\n[\ncharacter: transition to\nLINK_TEXT\nstate and still stream tokens to the client.\nIf in\nLINK_TEXT\nstate and there’s a matching end of markdown link via\n]\ncharacter\n2\n: transition to\nEXIT_LINK_TEXT\nstate and still stream tokens to the client.\nIf in\nEXIT_LINK_TEXT\nstate and next character is\n(\n, transition to\nLINK_URL\nstate and begin buffering the URL\nwithout\nstreaming anything to the client. Otherwise, if the next character is anything else transition back to the\nTEXT\nstate and resume streaming.\nIf in\nLINK_URL\nstate and there’s a matching end of the link via\n)\ncharacter\n3\n, see if the buffered URL exists as a key in our URL map and, if it is, replace the URL with the full URL. Then output the URL to the client and transition back to the default\nTEXT\nstate.\nSince we are asking OpenAI to provide citations using a specific format, this simple processing is sufficient for our needs. The markdown spec is surprisingly robust\n4\n, allowing additional formats for links such as:\n[link](/uri \"title\")\n[link](</my uri>)\n[a](<b)c>)\nI haven’t implemented support for the full spec since we have well known formats for our link text and URLs, but should the need arise the state machine can easily be extended. Until then, YAGNI.\nImproved output\nYou can see the result of implementing the above:\nNo more raw link URLs being flashed to the user. Just nice, clean links that appear as the full URL has been processed. You can see in the event stream that the server streams each output text delta as it’s received from OpenAI, except if there’s a link where the server buffers the link, replaces the short URL with the full URL, and sends the complete link to the client in a single chunk:\nJust plain better\nThere are multiple benefits from taking this approach:\nInstead of sending what ended up being links which consume approximately 50 tokens, our short links are 3 tokens. Fewer tokens means a smaller context for OpenAI and a lower usage bill.\nLink hallucinations appear to be a thing of the past, which was the catalyst for making this change.\nThe streaming response from OpenAI is now much faster since it’s providing the response with our short URLs of only 3 tokens and the speed difference is noticeable to the user.\nSince we buffer the URLs server-side and only provide the completed markdown link once we reach the end of the markdown URL, we are preventing flashes of incomplete markdown even if the link isn’t our short reference link. All links benefit from this buffering approach.\nWhile our citation URLs don’t reveal sensitive information and are secured by the user’s Gmail account, there’s an additional privacy bonus in that this substitution never transmits the URL to OpenAI in the first place. For some organizations, this alone could be a win.\nFaster, better, and cheaper — can’t go wrong with that!\nEngineering at Streak\nWe work on many interesting challenges affecting millions of users and many terabytes of data. For more information, visit\nhttps://www.streak.com/careers\nThanks for reading Streak Engineering! Subscribe for free to receive new posts and support my work.\nSubscribe\n1\nThe format of the link reference is completely arbitrary. I had initially wanted to go with just a link URL of “1”, “2”, and so on, but OpenAI started getting confused between the number for the URL and the number it was outputting as the citation text, since not every URL is used. For example, the first citation [1] could be the link URL of “3” or “84”, and OpenAI couldn’t keep the numbers straight.\nAdditionally, I prefixed the URLs with “#” just in case there is ever a bug in replacing the links since linking a “#REF3” fragment will do nothing, versus a “REF3” link would be interpreted as a relative URL based on the current path, resulting in a 404 error.\n2\nMarkdown is slightly complicated by the fact that link text can be surrounded by matching brackets. So\n[example](https://example.com)\nis a valid regular link, but so is\n[example [a]](https://example.com)\n, so matching balanced\n[]\ncharacters is important.\n3\nSimilar to matching\n[]\nthe link URL can also have matching\n()\ncharacters.\n4\nYou can see the CommonMark spec at\nhttps://spec.commonmark.org/0.31.2/#links\nShare this post\nStreak Engineering\nPreventing Flash of Incomplete Markdown when streaming AI responses\nCopy link\nFacebook\nEmail\nNotes\nMore\nShare\nDiscussion about this post\nComments\nRestacks\nTop\nLatest\nNo posts\nReady for more?\nSubscribe\n© 2025 Rewardly, Inc.\nPrivacy\n∙\nTerms\n∙\nCollection notice\nStart writing\nGet the app\nSubstack\nis the home for great culture\nShare\nCopy link\nFacebook\nEmail\nNotes\nMore\nThis site requires JavaScript to run correctly. Please\nturn on JavaScript\nor unblock scripts"
  },
  {
    "title": "From Steam to Silicon: Patterns of Technological Revolutions",
    "href": "https://ianreppel.org/from-steam-to-silicon/",
    "content": "Ian Reppel\nhome\nsearch\ninsights\nfiction\nabout\n30 May 2025\n•\nIan Reppel\n•\n4  min\nFrom Steam to Silicon: Patterns of Technological Revolutions\nFrom agriculture to AI, each major economic revolution is driven by a new kind of value conversion: a way of transforming one type of value into another.\nThese revolutions are not defined by a single invention, but by a sequence of innovations: a core conversion, the infrastructure to scale and distribute various innovations, and the technologies that reduce the cost of distance and delays.\nWe shall analyse six major revolutions through this lens and show how each laid the groundwork for the next.\nTechnological revolutions\nThere are seven key components to each technological revolution:\nCore conversion\n: a new capability that transforms one form of value into another. Value can be either tangible (e.g. natural resources and goods) or \nintangible (e.g. trust, information, and actions).\nScaling infrastructure\n: physical systems that make the conversion repeatable and cost-efficient at scale.\nSpatiotemporal compression\n: tools that shrink the cost of distance or time in the economy by i) increasing connectivity, ii) reducing the latency of interactions, or iii) expanding the scale or speed of economic coordination.\nKey resources\n: physical or non-physical ingredients that are needed in the core conversion recipe.\nEconomic mode\n: each technological revolution has its own dominant way of creating and capturing value.\nCentralization/decentralization arc\n: core conversions start centralised to exploit economies of scale, but as complementary technologies lower transaction costs, production disperses.\nAdministrative innovations\n: standards and bureaucratic tools to support the technological revolution at scale and across geographical areas, which may or may not be from the era itself.\nEach technological revolution is also a bridge to the next revolution, in that its innovations enable the next one to thrive.\nTimeline\nIn the table below, I have summarized the pattern for the agricultural, financial, and four industrial revolutions:\nThe ((\ncore conversion\n)) is made efficient thanks to ((\nscaling infrastructure\n)) to turn ((\nkey resources\n)) into ((\neconomic mode\n)) products and services, first centrally in ((\ncentralization arc\n)) and later decentralized in ((\ndecentralization arc\n)). The economy is made more interconnected and reduces distance or time between interactions because of ((\nspatiotemporal compression\n)). This is all made possible because of supportive ((\nadministrative innovations\n)).\nMore details on the four industrial revolutions (IRs) can be found in\nLate to the Revolution\n.\nRevolution\nCore conversion\nScaling infrastructure\nSpatiotemporal compression\nKey resources\nEconomic mode\nCentralization/decentralization arc\nAdministrative innovations\nEnables next\nAgricultural (ca. 10 000 BCE)\nlabour → food\nplant/animal domestication\nirrigation\nplough\nsettlements\ntrade routes\nroads\ncrops\nlivestock\nsurplus (food)\n(temple) granaries → feudal land tenure\nwriting\naccounting\nstandardized measures\nSurplus enables cities and specialization\nFinancial (ca. 1600)\ntrust → credit\ncredible institutions (e.g. banks)\nprinted money\nbills of exchange (transferable IOUs)\nclearing systems\npromises (of future payments)\ncapital\nstate-chartered banks and stock exchanges → bond markets and merchant credit networks\nbookkeeping\ncodified commercial law\npooled risk\nCapital markets fund massive infrastructure initiatives (IR1)\nIR1 (1760–1840)\nheat → motion\nsteam engine\nrailroads\ncanals\ncoal\niron\nindustrial products (e.g. textiles)\nfactories → workshops / factory hubs\npatents (with time-limited monopolies)\nCountry-wide logistics needed for distributed power production (IR2)\nIR2 (1870–1914)\nchemical → motion\nelectrification\nICE\nassembly lines\ntelecommunication\nroad networks\noil\nsteel\ncopper\nmass consumer goods (e.g. appliances, cars)\npower plants → cars (ICE)\nstandardized time\n(time zones)\nlimited-liability companies\nElectrification and global logistics enable mass media and automation (IR3)\nIR3 (1950–2000)\nelectricity → information\nelectronics\ninternet (i.e. satellites, telecommunication)\nglobalization (i.e. airplanes, container ships)\nsilicon\nrare earths\naluminium\nenergy\ndigital services\nmass consumer goods\nmainframes → PCs\ndigital protocols (e.g. TCP/IP, APIs)\nNetworked computing enables intelligent autonomous systems (IR4)\nIR4 (2010–present)\ninformation → action\nAI\nmobile internet (4G+)\n(I)IoT\ndata\ncompute\nenergy\nautonomous agents\ncloud → edge\nalgorithmic/app marketplaces\nserialization formats\nAutomated decision-making enables planetary-scale (self-)optimization\nPrior to the agricultural revolution, (decentralized) family farms were the norm.\nIt is common for a technological revolution to centralize the core conversion initially to make it more efficient and scalable.\nAfterwards, the conversion is dispersed, such as with settlements connected to the same temple/palace granaries through trade networks and much later tenancy of land in feudal economies.\nWhy it matters\nEach core conversion is a textbook\ngeneral-purpose technology\n(GPT).\nMost mainstream models describe\nwhat\nhappens, but not\nwhy\n; they focus on the diffusion of specific technologies, but not the functional shift those technologies cause.\nThis model helps us ask:\nWhat kind of\nconversion\ndo we bet on next?\nWhat\ninfrastructure\nmust be built to scale it?\nWhat barriers in\nspace and time\nneed to be collapsed?\nWhat are the primary\nresources\nand who controls these?\nHow will the\neconomic mode\nshift once it becomes mainstream?\nWhere are we in the\ncentralization/decentralization arc\nand how can we disperse its benefits better?\nWhat\nadministrative\ntools are required to support it?\nThe answers to these questions help us not only explain the past, but anticipate what kind of futures are possible, and what each requires to succeed.\nThe model can also guide policy decisions: instead of wondering what the next gadget is, we can invest wisely in scaling infrastructure, tools for spatiotemporal compression to ensure the benefits from the core conversion technology are spread far and wide, and appropriate regulation to ensure interoperability.\nThe pattern of initial centralization and later decentralization is clear across all six technological revolutions, which means we can anticipate that after the initial infrastructure investments of today, we may need planetary sensor grids to ensure we can tackle challenges such as climate change appropriately.\nModern medicine, antibiotics, and vaccines have added many years to global life expectancy. While that is an immense welfare gain, it does not constitute a new technological revolution in this framework: there is no new value conversion. Each medical breakthrough still relies on the prevailing infrastructure and logistics. Medical gains do amplify revolutions, as healthier\npeople work longer, learn more, and innovate faster\n.\nWhere does quantum fit?\nQuantum computing\nsits\ninside\nIR4 as a supportive technology accelerator, similar to GPUs.\nFor specific computations in cryptography, chemistry, machine learning, and optimization, quantum computers are expected to deliver massive\nspeed-ups\n.\nCertain intractable problems may only be solvable on quantum devices in the future.\nThis may lead to novel materials and pharmaceuticals, provide significant performance boosts in training AIs, or improve global logistics and supply chains.\nLike steam in the 1770s and semiconductors in the 1950s, quantum computing is still in the lab, but it will be transformative once it is ready for the real world, which is about\n5–10 years\nfrom now.\nWhat’s next?\nEvery revolution is won or lost on the boring stuff: standards, infrastructure, and fair access.\nSteam needed rails, electricity needed wires, and IR5 will need a planetary-scale layer of data and compute that is open, resilient, and reachable by everyone.\nPolicy makers have three levers that have already been proven:\nMandate open, royalty-free standards for data, APIs, and edge hardware\n:\ninteroperability cuts compliance costs and boosts competition\n.\nTreat broadband and smart grids as 21st-century railroads\n:\na 10 percent increase in broadband penetration bumps GDP by up to 1.4%\nand\nsmart grids have a 10× ROI\n.\nBuild digital public infrastructure to flatten inequality\n:\ninstant payments slash merchant fees and pull millions into the formal economy\n.\nSo, invest in standards, digital infrastructure, and parity now, or spend the next decades paying a coordination tax that no technology can offset.\n☕ Buy Me a Coffee\nRelated posts\nLate to the Revolution: The High Cost of Tech Lag\n29 May 2025\nThe Penalty of Being Human\n01 May 2025\nLeadership Lessons from Jazz, Improv, and Japan\n12 Mar 2025\nLate to the Revolution: The High Cost of Tech Lag\nBack to top"
  },
  {
    "title": "Merlin Bird ID",
    "href": "https://merlin.allaboutbirds.org/",
    "content": ""
  },
  {
    "title": "A practical guide to building agents [pdf]",
    "href": "https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf",
    "content": ""
  },
  {
    "title": "DiffX – Next-Generation Extensible Diff Format",
    "href": "https://diffx.org/",
    "content": "Contents\nMenu\nExpand\nLight mode\nDark mode\nAuto light/dark, in light mode\nAuto light/dark, in dark mode\nHide navigation sidebar\nHide table of contents sidebar\nSkip to content\nToggle site navigation sidebar\nDiffX - Next-Generation Extensible Diff Format\nToggle Light / Dark / Auto color theme\nToggle table of contents sidebar\nDiffX - Next-Generation Extensible Diff Format\nThe Problems with Diffs\nDiffX File Format Specification\nToggle navigation of DiffX File Format Specification\n1. Introduction\n2. Section Definitions\n3. Section Hierarchy\n4. Encoding Rules\n5. Binary Diffs\n6. Example DiffX Files\npydiffx\nToggle navigation of pydiffx\nTutorials\nToggle navigation of Tutorials\nWriting DiffX Files using DiffXWriter\nStep 1. Create the Writer\nStep 2. Write a Preamble (Optional)\nStep 3. Write Metadata (Optional)\nStep 4. Begin a New Change\nStep 5. Begin a New File\nStep 6. Write a File’s Diff (Optional)\nStep 7: Rinse and Repeat\nPutting It All Together\nModule and Class References\nToggle navigation of Module and Class References\npydiffx\npydiffx.dom\npydiffx.dom.objects\npydiffx.dom.reader\npydiffx.dom.writer\npydiffx.errors\npydiffx.options\npydiffx.reader\npydiffx.sections\npydiffx.utils\npydiffx.utils.text\npydiffx.utils.unified_diffs\npydiffx.writer\nRelease Notes\nToggle navigation of Release Notes\n1.1.0 (September 18, 2022)\n1.0.1 (August 4, 2021)\n1.0 (August 1, 2021)\nFrequently Asked Questions\nGlossary\nBack to top\nView this page\nToggle Light / Dark / Auto color theme\nToggle table of contents sidebar\nDiffX - Next-Generation Extensible Diff Format\n¶\nIf you’re a software developer, you’ve probably worked with diff files. Git\ndiffs, Subversion diffs, CVS diffs.. Some kind of diff. You probably haven’t\ngiven it a second thought, really. You make some changes, run a command, a\ndiff comes out. Maybe you hand it to someone, or apply it elsewhere, or put it\nup for review.\nDiff files show the differences between two text files, in the form of\ninserted (\n+\n) and deleted (\n-\n) lines. Along with this, they contain some\nbasic information used to identify the file (usually just the name/relative\npath within some part of the tree), maybe a timestamp or revision, and maybe\nsome other information.\nMost people and tools work with\nUnified Diffs\n. They look like this:\n--- readme    2016-01-26 16:29:12.000000000 -0800\n+++ readme    2016-01-31 11:54:32.000000000 -0800\n@@ -1 +1,3 @@\nHello there\n+\n+Oh hi!\nOr this:\nIndex: readme\n===================================================================\nRCS file: /cvsroot/readme,v\nretrieving version 1.1\nretrieving version 1.2\ndiff -u -p -r1.1 -r1.2\n--- readme    26 Jan 2016 16:29:12 -0000        1.1\n+++ readme    31 Jan 2016 11:54:32 -0000        1.2\n@@ -1 +1,3 @@\nHello there\n+\n+Oh hi!\nOr this:\ndiff --git a/readme b/readme\nindex d6613f5..5b50866 100644\n--- a/readme\n+++ b/readme\n@@ -1 +1,3 @@\nHello there\n+\n+Oh hi!\nOr even this:\nIndex: readme\n===================================================================\n--- (revision 123)\n+++ (working copy)\nProperty changes on: .\n-------------------------------------------------------------------\nModified: myproperty\n## -1 +1 ##\n-old value\n+new value\nOr this!\n==== //depot/proj/logo.png#1 ==A== /src/proj/logo.png ====\nBinary files /tmp/logo.png and /src/proj/logo.png differ\nHere’s the problem\n¶\nUnified Diffs themselves are not a viable standard for modern development.\nThey only standardize parts of what we consider to be a diff, namely the\n---\n/\n+++\nlines for file identification,\n@@\n...\n@@\nlines for\ndiff hunk offsets/sizes, and\n-\n/\n+\nfor inserted/deleted lines.\nThey\ndon’t\nstandardize encodings, revisions, metadata, or even how\nfilenames or paths are represented!\nThis makes it\nvery\nhard for patch tools, code review tools, code analysis\ntools, etc. to reliably parse any given diff and gather useful information,\nother than the changed lines, particularly if they want to support multiple\ntypes of source control systems. And there’s a lot of good stuff in diff files\nthat some tools, like code review tools or patchers, want.\nYou should see what GNU Patch has to deal with.\nUnified Diffs have not kept up with where the world is going. For instance:\nA single diff can’t represent a list of commits\nThere’s no standard way to represent binary patches\nDiffs don’t know about text encodings (which is more of a problem than you\nmight think)\nDiffs don’t have any standard format for arbitrary metadata, so everyone\nimplements it their own way.\nWe’re long past the point where diffs should be able to do all this. Tools\nshould be able to parse diffs in a standard way, and should be able to modify\nthem without worrying about breaking anything. It should be possible to load a\ndiff, any diff, using a Python module or Java package and pull information out\nof it.\nUnified Diffs aren’t going away, and they don’t need to. We just need to add\nsome extensibility to them. And that’s completely doable, today.\nHere’s the good news\n¶\nUnified Diffs, by nature, are\nvery\nforgiving, and they’re everywhere, in\none form or another. As you’ve seen from the examples above, tools shove all\nkinds of data into them. Patchers basically skip anything they don’t\nrecognize. All they really lack is structure and standards.\nGit’s diffs are the closest things we have to a standard diff format (in that\nboth Git and Mercurial support it, and Subversion pretends to, but poorly),\nand the closest things we have to a modern diff format (as they optionally\nsupport binary diffs and have a general concept of metadata, though it’s\nlargely Git-specific).\nThey’re a good start, though still not formally defined. Still, we can build\nupon this, taking some of the best parts from Git diffs and from other\nstandards, and using the forgiving nature of Unified Diffs to define a new,\nstructured Unified Diff format.\nDiffX files\n¶\nWe propose a new format called Extensible Diffs, or DiffX files for short.\nThese are\nfully backwards-compatible with existing tools\n, while also being\nfuture-proof\nand remaining\nhuman-readable\n.\n#diffx:\nencoding=utf-8, version=1.0\n#.change:\n#..preamble:\nindent=4, length=319, mimetype=text/markdown\nConvert legacy header building code to Python 3.\n    \n    Header building for messages used old Python 2.6-era list comprehensions\n    with tuples rather than modern dictionary comprehensions in order to build\n    a message list. This change modernizes that, and swaps out six for a\n    3-friendly `.items()` call.\n#..meta:\nformat=json, length=270\n{\n\"author\"\n:\n\"Christian Hammond <christian@example.com>\"\n,\n\"committer\"\n:\n\"Christian Hammond <christian@example.com>\"\n,\n\"committer date\"\n:\n\"2021-06-02T13:12:06-07:00\"\n,\n\"date\"\n:\n\"2021-06-01T19:26:31-07:00\"\n,\n\"id\"\n:\n\"a25e7b28af5e3184946068f432122c68c1a30b23\"\n}\n#..file:\n#...meta:\nformat=json, length=176\n{\n\"path\"\n:\n\"/src/message.py\"\n,\n\"revision\"\n:\n{\n\"new\"\n:\n\"f814cf74766ba3e6d175254996072233ca18a690\"\n,\n\"old\"\n:\n\"9f6a412b3aee0a55808928b43f848202b4ee0f8d\"\n}\n}\n#...diff:\nlength=629\n--- /src/message.py\n+++ /src/message.py\n@@ -164,10 +164,10 @@\nnot isinstance(headers, MultiValueDict)):\n# Instantiating a MultiValueDict from a dict does not ensure that\n# values are lists, so we have to ensure that ourselves.\n-            headers = MultiValueDict(dict(\n-                (key, [value])\n-                for key, value in six.iteritems(headers)\n-            ))\n+            headers = MultiValueDict({\n+                key: [value]\n+                for key, value in headers.items()\n+            })\nif in_reply_to:\nheaders['In-Reply-To'] = in_reply_to\nDiffX files are built on top of Unified Diffs, providing structure and\nmetadata that tools can use. Any DiffX file is a complete Unified Diff, and\ncan even contain all the legacy data that Git, Subversion, CVS, etc. may want\nto store, while also structuring data in a way that any modern tool can easily\nread from or write to using\nstandard parsing rules.\nLet’s summarize. Here are some things DiffX offers:\nStandardized rules for parsing diffs\nFormalized storage and naming of metadata for the diff and for each commit\nand file within\nAbility to extend the format without breaking existing parsers\nMultiple commits can be represented in one diff file\nGit-compatible diffs of binary content\nKnowledge of text encodings for files and diff metadata\nCompatibility with all existing parsers and patchers (for all standard\ndiff features – new features will of course require support in tools, but\ncan still be parsed)\nMutability, allowing a tool to easily open a diff, record new data, and\nwrite it back out\nDiffX is\nnot\ndesigned to:\nForce all tools to support a brand new file format\nBreak existing diffs in new tools or require tools to be rewritten\nCreate any sort of vendor lock-in\nWant to learn more?\n¶\nIf you want to know more about what diffs are lacking, or how they differ from\neach other (get it?), then read\nThe Problems with Diffs\n.\nIf you want to get your hands dirty, check out the\nDiffX File Format Specification\n.\nSee\nexample DiffX files\nto see this in action.\nOther questions? We have a\nFAQ\nfor you.\nImplementations\n¶\nPython:\npydiffx\nWho’s using DiffX?\n¶\nReview Board\nfrom\nBeanbag\n. We built DiffX to solve long-standing\nproblems we’ve encountered with diffs, and are baking support into all our\nproducts.\nNext\nThe Problems with Diffs\nCopyright © 2016-2025, Beanbag, Inc.\nMade with\nSphinx\nand\n@pradyunsg\n's\nFuro"
  },
  {
    "title": "How We Reduced the Impact of Zombie Clients",
    "href": "https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/",
    "content": "Languages\nEnglish\nCatalà\nČeština\nDansk\nDeutsch\nGreek\nEspañol\nSuomi\nFrançais\nעברית\nHungarian\nBahasa Indonesia\nItaliano\n日本語\n한국어\nPolish\nPortuguês do Brasil\nРусский\nසිංහල\nSrpski\nSvenska\nTamil\nThai\nTürkçe\nУкраїнська\nTiếng Việt\n简体中文\n繁體中文\nSkip navigation links\nDocumentation\nGet Help\nBlog\nDonate\nBecome a Sponsor\nCurrent Sponsors & Funders\nGet Involved\nDonate\nAbout Us\nLet's Encrypt\nFrequently Asked Questions (FAQ)\nShortening the Let's Encrypt Chain of Trust\nPolicy and Legal Repository\nService Status\nStatistics\nContact\nCareers\nAnnual reports\nInternet Security Research Group (ISRG)\nDonate Now\nDonate Now\nBlog\nHow We Reduced the Impact of Zombie Clients\nBy Samantha Frank ·\nJune 4, 2025\nEvery night, right around midnight (mainly\nUTC\n), a horde of zombies wakes up and clamors for … digital certificates!\nThe zombies in question are abandoned or misconfigured Internet servers and ACME clients that have been set to request certificates from Let’s Encrypt. As our certificates\nlast for at most 90 days\n, these zombie clients’ software knows that their certificates are out-of-date and need to be replaced. What they don’t realize is that their quest for new certificates is doomed! These devices are cursed to seek certificates again and again, never receiving them.\nBut they do use up a lot of certificate authority resources in the process.\nThe Zombie Client Problem\nUnlike a human being, software doesn’t give up in frustration, or try to modify its approach, when it repeatedly fails at the same task. Our emphasis on automation means that the vast majority of Let’s Encrypt certificate renewals are performed by automated software. This is great when those renewals succeed, but it also means that forgotten clients and devices can continue requesting renewals unsuccessfully for months, or even years.\nHow might that happen? Most often, it happens when a device no longer has a domain name pointed to it. The device itself doesn’t know that this has changed, so it treats renewal failures as transient even though they are actually permanent. For instance:\nAn organization may have allowed a domain name registration to lapse because it is no longer needed, but its servers are still configured to request certs for it.\nOr, a home user stopped using a particular dynamic-DNS domain with a network-attached storage device, but is still using that device at home. The device doesn’t realize that the user no longer expects to use the name, so it keeps requesting certs for it.\nOr, a web hosting or CDN customer migrated to a different service provider, but never informed the old service provider. The old service provider’s servers keep requesting certs unsuccessfully. If the customer was in a free service tier, there might not be invoices or charges reminding the customer to cancel the service.\nOr any number of other, subtler changes in a subscriber’s infrastructure, such as changing a firewall rule or some webserver configuration.\nAt the scale of Let’s Encrypt, which now covers\nhundreds of millions of names\n, scenarios like these have become common, and their impact has become substantial. In 2024, we noticed that about half of all certificate requests to the Let’s Encrypt ACME API came from about a million accounts that never successfully complete any validations. Many of these had completed validations and issued certificates sometime in the past, but nowadays every single one of their validation attempts fails, and they show no signs that this will change anytime soon.\nUnfortunately, trying to validate those futile requests still uses resources. Our CA software has to generate challenges, reach out and attempt to validate them over the Internet, detect and report failures, and record all of the associated information in our databases and audit logs. And over time, we’ve seen more and more recurring failures: accounts that always fail their issuance requests have been growing at around 18% per year.\nIn January, we mentioned that we had been addressing the zombie client problem\nthrough our rate limit system\n. This post provides more detail on that progress.\nOur Rate Limit Philosophy\nIf you’ve used Let’s Encrypt as a subscriber, you may have run into one of our\nrate limits\nat some point, maybe during your initial setup process. We have eight different kinds of rate limits in place now; as our January post describes, they’ve become more algorithmically sophisticated and grown to address a wider range of problems. A key principle for Let’s Encrypt is that our rate limiting is not a punishment. We don’t think of rate limits as a way of retaliating against a client for misbehavior. Rate limits are simply a tool to maximize the efficient use of our limited resources and prevent people and programs from using up those resources for no constructive purpose.\nWe’ve consistently tried to design our rate limit mechanisms in line with that philosophy. So if a misconfiguration or misunderstanding has caused excessive requests in the past, we’re still happy to welcome the user in question back and start issuing them certificates again—once the problem has been addressed. We want the rate limits to put a brake on wasteful use of our systems, but not to frustrate users who are actively trying to make Let’s Encrypt work for them.\nIn addition, we’ve always implemented our rate limits to err on the side of permissiveness. For example, if the Redis instances where rate limits are tracked have an outage or lose data, the system is designed to permit more issuance rather than less issuance as a result.\nWe wanted to create additional limits that would target zombie clients, but in a correspondingly non-punitive way that would avoid any disruption to valid issuance, and welcome subscribers back quickly if they happened to notice and fix a long-time problem with their setups.\nOur Zombie-Related Rate Limits and Their Impact\nIn planning a new zombie-specific response, we decided on a “pausing” approach, which can temporarily limit an account’s ability to proceed with certificate requests. The core idea is that, if a particular account consistently fails to complete validation for a particular hostname, we’ll pause that account-hostname pair. The pause means that any new order requests from that account for that hostname will be rejected immediately, before we get to the resource-intensive validation phase.\nThis approach is more finely targeted than pausing an entire account. Pausing account-hostname pairs means that your ability to issue certs for a specific name could be paused due to repeated failures, but you can still get all of your other certs like normal. So a large hosting provider doesn’t have to fear that its certificate issuance on behalf of one customer will be affected by renewal failures related to a problem with a different customer’s domain name. The account-specificity of the pause, in turn, means that validation failures from one subscriber or device won’t prevent a different subscriber or device from attempting to validate the same name, as long as the devices in question don’t share a single Let’s Encrypt account.\nIn September 2024, we began applying our zombie rate limits manually by pausing about 21,000 of the most recurrently-failing account-hostname pairs, those which were consistently repeating the same failed requests many times per day, every day. After implementing that first round of pauses, we immediately saw a significant impact on our failed request rates. As we announced at that time, we also began\nusing a formula to automatically pause other zombie client account-hostname pairs from December 2024 onward\n. The associated new rate limit is called “\nConsecutive Authorization Failures per Hostname Per Account\n” (and is independent of the existing “Authorization Failures per Hostname Per Account” limit, which resets every hour).\nThis formula relates to the frequency of successive failed issuance requests for the same domain name by the same Let’s Encrypt account. It applies only to failures that happen again and again, with no successful issuances at all in between: a single successful validation immediately resets the rate limit all the way to zero. Like all of our rate limits, this is not a punitive measure but is simply intended to reduce the waste of resources. So, we decided to set the thresholds rather high in the expectation that we would catch only the most disruptive zombie clients, and ultimately only those clients that were extremely unlikely to succeed in the future based on their substantial history of failed requests. We don’t hurry to block requesters as zombies: according to our current formula, client software following the default established by EFF’s\nCertbot\n(two renewal attempts per day) would be paused as a zombie only after about ten years of constant failures. More aggressive failed issuance attempts will get a client paused sooner, but clients will generally have to fail hundreds or thousands of attempts in a row before they are paused.\nMost subscribers using mainstream client applications with default configurations will never encounter this rate limit, even if they forget to deactivate renewal attempts for domains that are no longer pointed at their servers. As described below, our current limit is already providing noticeable benefits with minimal disruption, and we’re likely to tighten it a bit in the near future, so it will trigger after somewhat fewer consecutive failures.\nSelf-Service Unpausing\nA key feature in our zombie issuance pausing mechanism is self-service unpausing. Whenever an account-hostname pair is paused, any new certificate requests for that hostname submitted by that account are immediately rejected. But this means that the “one successful validation immediately resets the rate limit counter” feature can no longer come into effect: once they’re paused, they can’t even attempt validation anymore.\nSo every rejection comes with an error message explaining what has happened and a custom link that can be used to immediately unpause that account-hostname pair and remove any other pauses on the same account at the same time. The point of this is that subscribers who notice at some point that issuance is failing and want to intervene to get it working again have a straightforward option to let Let’s Encrypt know that they’re aware of the recurring failures and are still planning to use a particular account. As soon as subscribers notify us via the self-service link, they’ll be able to issue certificates again.\nCurrently, the user interface for an affected subscriber looks like this:\nThis link would be provided via an ACME error message in response to any request that was blocked due to a pause account-hostname pair.\nAs it’s turned out, the unpause option shown above has only been used by about 3% of affected accounts! This goes to show that most of the zombies we’ve paused were, in fact, well and truly forgotten about.\nHowever, the unpause feature is there for whenever it’s needed, and there may be cases when it will become more important. A very large integration could trigger the zombie-related rate limits if a newly-introduced software bug causes what looks like a very high volume of zombie requests in a very short time. In that case, once that bug has been noticed and fixed, an integrator may need to unpause its issuance on behalf of lots of customers at once. Our unpause feature permits unpausing 50,000 domain names on a single account at a time, so even the largest integrators can get themselves unpaused expeditiously in this situation.\nConclusion\nWe’ve been very happy with the results of our zombie mitigation measures, and, as far as we can tell, there’s been almost no impact for subscribers! Our statistics indicate that we’ve managed to reduce the load on our infrastructure while causing no detectable harm or inconvenience to subscribers’ valid issuance requests.\nSince implementing the manual pauses in September and the automated pauses in December, we’ve seen:\nOver 100,000 account-hostname pairs have been paused for excessive failures.\nWe received zero (!) associated complaints or support requests.\nAbout 3,200 people manually unpaused issuance.\nFailed certificate orders fell by about 30% so far, and should continue to fall over time as we fine-tune the rate limit formula and catch more zombie clients.\nThe new rate limit and the self-service unpause system are also ready to deal with circumstances that might produce more zombie clients in the future. For instance, we’ve announced that\nwe’re going to be discontinuing renewal reminder emails\nsoon. If some subscribers overlook failed renewals in the future, we might see more paused clients that result from unintentional renewal failures. We think taking advantage of the existing self-service unpause feature will be straightforward in that case. But it’s much better to notice problems and get them fixed up front, so please remember to\nset up your own monitoring\nto avoid unnoticed renewal failures in the future.\nIf you’re a subscriber who’s had occasion to use the self-service unpause feature, we’d love your feedback on the\nCommunity Forum\nabout your experience using the feature and the circumstances that surrounded your account’s getting paused.\nAlso, if you’re a Let’s Encrypt client developer, please remember to make renewal requests at a random time (not precisely at midnight) so that the load on our infrastructure is smoothed out. You can also reduce the impact of zombie renewals by repeating failed requests somewhat less frequently over time (a “back-off” strategy), especially if the failure reason makes it look like a domain name may no longer be in use at all.\nLet's Encrypt is a free, automated, and open Certificate Authority brought to you by the nonprofit\nInternet Security Research Group (ISRG)\n. Read all about our nonprofit work this year in our\n2024 Annual Report\n.\nLegal Address\n548 Market St, PMB 77519\nSan Francisco\n,\nCA\n94104-5401\nUSA\nSend all mail or inquiries to:\nPO Box 18666\nMinneapolis\n,\nMN\n55418-0666\nUSA\nSubscribe for email updates about Let's Encrypt and other ISRG projects\n© 2025\nInternet Security Research Group\nGitHub\nLinkedIn\nTerms\nPrivacy Policy\nTrademark Policy"
  },
  {
    "title": "Cockatoos have learned to operate drinking fountains in Australia",
    "href": "https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia",
    "content": ""
  },
  {
    "title": "Giant planet discovered orbiting tiny star",
    "href": "https://www.ucl.ac.uk/news/2025/jun/giant-planet-discovered-orbiting-tiny-star",
    "content": "Close\nStudy\nResearch\nEngage\nAbout\nGive\nUCL News\nHome\nHome\nLatest news\nUCL in the media\nServices for media\nStudent news\nStaff news\nTell us your story\nContact us\nUCL Home\nUCL News\nGiant planet discovered orbiting tiny star\nHome\nLatest news\nUCL in the media\nServices for media\nStudent news\nStaff news\nTell us your story\nContact us\nHome\nLatest news\nUCL in the media\nServices for media\nStudent news\nStaff news\nTell us your story\nContact us\nUCL Home\nUCL News\nGiant planet discovered orbiting tiny star\nGiant planet discovered orbiting tiny star\n4 June 2025\nAstronomers at UCL and the University of Warwick, as part of a global collaboration including partners in Chile, USA and Europe, have discovered the smallest known star to host a transiting giant planet, which should not exist under leading planet formation theories.\nStar TOI-6894 is just like many in our galaxy, a small red dwarf, and only ~20% of the mass of our Sun. Like many small stars, it is not expected to provide suitable conditions for the formation and hosting of a large planet.\nHowever, as published today in\nNature Astronomy\n, an international team of astronomers have found the unmistakable signature of a giant planet, called TOI-6894b, orbiting this tiny star.\nThis system has been discovered as part of a large-scale investigation of TESS (Transiting Exoplanet Survey Satellite) data, looking for giant planets around low-mass stars, led by Dr Edward Bryant, who completed this work at UCL’s Mullard Space Science Laboratory and the University of Warwick.\nDr Bryant said: “I was very excited by this discovery. I originally searched through TESS observations of more than 91,000 low-mass red-dwarf stars looking for giant planets.\n“Then, using observations taken with one of the world’s largest telescopes, ESO’s VLT, I discovered TOI-6894b, a giant planet transiting the lowest mass star known to date to host such a planet. We did not expect planets like TOI-6894b to be able to form around stars this low-mass. This discovery will be a cornerstone for understanding the extremes of giant planet formation.”\nThe planet (TOI-6894b) is a low-density gas giant with a radius a little larger than Saturn’s but with only ~50% of Saturn’s mass. The star (TOI-6894) is the lowest mass star to have a transiting giant planet discovered to date and is just 60% the size of the next smallest star to host such a planet.\nDr Vincent Van Eylen, from UCL’s Mullard Space Science Laboratory, said: “It’s an intriguing discovery. We don’t really understand how a star with so little mass can form such a massive planet! This is one of the goals of the search for more exoplanets. By finding planetary systems different from our solar system, we can test our models and better understand how our own solar system formed.”\nDr Daniel Bayliss, based at the University of Warwick said: “Most stars in our galaxy are actually small stars exactly like this, with low masses and previously thought to not be able to host gas giant planets. So, the fact that this star hosts a giant planet has big implications for the total number of giant planets we estimate exist in our galaxy.”\nThe most widely held theory of planet formation is called the core accretion theory. A planetary core forms first through accretion (gradual accumulation of material) and as the core becomes more massive, it eventually attracts gases that form an atmosphere. It then gets massive enough to enter a runaway gas accretion process to become a gas giant.\nIn this theory, the formation of gas giants is harder around low-mass stars because the amount of gas and dust in a protoplanetary disc around the star (the raw material of planet formation) is too limited to allow a massive enough core to form, and the runaway process to occur.\nYet the existence of TOI-6894b (a giant planet orbiting an extremely low-mass star) suggests this model cannot be completely accurate and alternative theories are needed.\nDr Bryant added: “Given the mass of the planet, TOI-6894b could have formed through an intermediate core-accretion process, in which a protoplanet forms and steadily accretes gas without the core becoming massive enough for runaway gas accretion.\n“Alternatively, it could have formed because of a gravitationally unstable disc. In some cases, the disc surrounding the star will become unstable due to the gravitational force it exerts on itself. These discs can then fragment, with the gas and dust collapsing to form a planet.”\nBut the team found that neither theory could completely explain the formation of TOI-6894b from the available data, which leaves the origin of this giant planet as an open question for now.\nOne avenue to shed light on the mystery of TOI-6894b's formation is a detailed atmospheric analysis. By measuring the distribution of material within the planet, astronomers can determine the size and structure of the planet’s core, which can tell us whether TOI-6894b formed via accretion or via an unstable disc.\nThis is not the only interesting feature of TOI-6894b’s atmosphere; it is unusually cold for a gas giant. Most of the gas giants found by exoplanet hunters are hot Jupiters, massive gas giants with temperatures of ~1000-2000 Kelvin. TOI-6894b, by comparison, is just 420 Kelvin. The cool temperature alongside other features of this planet, such as the very deep transits, makes it one of the most promising giant planets for astronomers to characterise with a cool atmosphere.\nProfessor Amaury Triaud, University of Birmingham, co-author and member of the SPECULOOS collaboration, said: “Based on the stellar irradiation of TOI-6894b, we expect the atmosphere is dominated by methane chemistry, which is very rare to identify. Temperatures are low enough that atmospheric observations could even show us ammonia, which would be the first time it is found in an exoplanet atmosphere.\n“TOI-6894b likely presents a benchmark exoplanet for the study of methane-dominated atmospheres and the best ‘laboratory’ to study a planetary atmosphere containing carbon, nitrogen and oxygen outside the solar system.”\nThe atmosphere of TOI-6894b is already scheduled to be observed by the James Webb Space Telescope (JWST) within the next 12 months. This should allow astronomers to determine which, if either, of the possible theories can explain the formation of this unexpected planet.\nCo-author Dr. Andrés Jordán, researcher at the Millennium Institute of Astrophysics and professor at Adolfo Ibáñez University, said: “This system provides a new challenge for models of planet formation, and it offers a very interesting target for follow-up observations to characterize its atmosphere.\n“This discovery is the result of a systematic program we have been carrying out for several years from Chile and the UK. Our efforts have allowed us to contribute significantly to a better understanding of how often small stars can form giant planets, and we are providing prime targets for follow-up with space-based platforms.”\nLinks\nThe paper in\nNature Astronomy\nDr Vincent Van Eylen’s academic profile\nUCL’s Mullard Space Science Laboratory\nUCL Mathematical & Physical Sciences\nThe University of Warwick\nImage\nArtist’s illustration of planet TOI-6894b behind its host star. Credit: University of Warwick/Mark Garlick\nMedia contact\nMark Greaves\nm.greaves [at] ucl.ac.uk\nShare\nTweet\nFollow us\nInformation for\nCurrent students\nStaff\nAlumni\nBusiness\nDonors\nVisit\nMaps\nLibrary, museums and collections\nBloomsbury Theatre\nUCL East\nTours and visits\nConnect with UCL\nJobs\nMedia Relations\nEvents\nUCL and London\nUCL Shop\nUniversity College London, Gower Street, London, WC1E 6BT Tel: +44 (0) 20 7679 2000\nCopyright © 2025 UCL\nDisclaimer\nFreedom of Information\nAccessibility\nPrivacy and Cookies\nSlavery statement\nContact Us"
  },
  {
    "title": "Twain Dreams",
    "href": "https://harpers.org/archive/2025/06/twain-dreams-samuel-clemens-john-jeremiah-sullivan/",
    "content": "Signed In\nvia\nInstitutional Access\nSign In\nSubscribe\nAccount\nAccount\nSign in to access\nHarper’s Magazine\nWe've recently updated our website to make signing in easier and more secure\nSign in\nto\nHarper's\nLearn more about using our new sign-in system\nHi there\n.\n                            You have\n1\nfree\narticle\nthis month.\nConnect to your subscription or subscribe for full access\nYou've reached your free article limit for this\n                            month.\nConnect to your subscription or subscribe for full access\nConnect Subscription\nSubscribe\nUpdate login\nMake changes to your subscription\nSupport and FAQ\nLog out\nThanks for being a subscriber!\nUpdate login\nMake changes to your subscription\nSupport and FAQ\nLog out\nGet Access to Print and Digital for\n                                $23.99 per year.\nSubscribe for Full Access\nAbout\nArchive\nAuthors\nGala\nNewsletters\nShop\nPodcast\nEvents\nSubscribe\nSearch\n175 Years\nCurrent Issue\nSections\nThe Latest\nSearch\nAbout\nArchive\nAuthors\nGala\nNewsletters\nShop\nPodcast\nEvents\nSubscribe\nSearch\nJune 2025 Issue\n[Symposium]\nTwain Dreams\nDownload PDF\nAdjust\nShare\nThe enigma of Samuel Clemens\nby\nJohn Jeremiah Sullivan\n,\nMark Twain on the steps leading up to his study at Quarry Farm, New York, 1903, by T. E. Marr. Courtesy the Center for Mark Twain Studies, Elmira College, New York\nCould some kind of Mark Twain revival be afoot in this, the 175th-anniversary year of\nHarper’s Magazine,\na periodical that more consistently than any other provided a home for Twain’s writing during the half-century-long major phase of his career?\n1\nThe signs are come unto us. The writer Percival Everett’s 2024 novel\nJames,\nin which Everett reimagines the story of Twain’s\nAdventures of Huckleberry Finn\nfrom the perspective of Huck’s raft mate on the Mississippi, the self-emancipated Jim, took home last year’s National Book Award for Fiction. Only two months ago, we got a major new book by the Stanford professor Shelley Fisher Fishkin, who in the long history of scholarship on Mark Twain has written some of the best of it.\nJim\nis the title, and subject, of Fishkin’s latest. So, we have\nJames\nand\nJim,\nbarely a year apart. Meanwhile, the annual Mark Twain Prize for American Humor has taken on new significance as a sort of dissenter’s pulpit. This year’s winner, the comedian and talk-show host Conan O’Brien, seized the moment of his acceptance speech, delivered in March onstage at the Kennedy Center in Washington—where the previous month, the board of trustees had been ousted by President Trump and replaced with a shock brigade of his sort of people, among them the country singer Lee Greenwood, of “Proud to Be an American” fame, all in the interest of ushering in a new “Golden Age in Arts and Culture,” with Trump himself at the head, as chair, lobbing brain-damaged non sequiturs about this one time he saw\nCats\n—and used it, O’Brien did, to speak out not-so-subtly against the regime. “Twain hated bullies,” O’Brien told the crowd, a statement largely true (although Tom Sawyer was a bully at times, and a manipulative narcissist at all times). O’Brien said that Twain hated racism too, and it is true that Twain\ncame\nto hate racism, although he had been a racist earlier in his life and even farcically fought for the Confederacy for a couple of weeks. But this is pedantry on my part.\n2\nMost recently, a major new Twain biography has been published, by Ron Chernow, one of our great Biographers of the Great (on some future day he, Walter Isaacson, and Jon Meacham will write biographies of one another, and the faces on Mount Rushmore will simultaneously explode). The mere fact that Chernow has chosen Twain to stand in as the sole\nlittérateur\non his list of targeted cities, a lineup that otherwise comprises none but political or financial world-shakers, e.g., Washington, Hamilton, Grant, Morgan, and Rockefeller—says something about Twain’s place in our collective cultural memory, his “ongoing relevance,” maybe, although, in this case, not exactly:\nrelevance\nwould imply that he continues to play a loosely agreed-upon role in our self-understanding, or that his life and work imply some standard we fear to disappoint, whereas with Twain it seems more the case that we go on\nsuspecting he hides some meaning,\nsome message that we could probably use, behind the bushy eyebrows and mustache, but are less and less able to name or remember. He doesn’t quite haunt us. O’Brien had to perform some impressive textual excavation to make Twain serve the night’s theme. The author “populated his works with abusers such as Huck Finn’s alcoholic father and Tom Driscoll in\nPudd’nhead Wilson,\n” the comedian said, “and he made his readers passionately hate those characters.” Ah, yes, Tom Driscoll, born 1/32 black, swapped out in infancy by his mother and raised as a lazy white Southern aristocrat . . . or possibly the other, “real” Tom Driscoll, born white but raised enslaved? . . . Either way, God damn them\nand\nPap Finn! The comedian Dave Chappelle, who won the 2019 Mark Twain Prize, may have shown wisdom in keeping his own Twain references more oblique. He said nothing, in fact, about Twain. At one point he reportedly put his face down beside the bronze trophy bust of Twain that the organizers placed onstage for the ceremony, and grinned enigmatically. What did that mean? Something about Twain and race and America and now? Could he feel Everett’s novel coming, know that the script was about to be flipped?\nI grew up so hopelessly steeped in the cult of Twain that I have to perform a mental adjustment to understand how a Twain revival could be possible. How does one revive what is ever-present and oppressively urgent? My sportswriter father, who died when I was in my mid-twenties, worshipped Twain, to the extent of wearing, every year on specific occasions, a tailored white suit. With the shaggy hair and Twainish mustache that he maintained year-round, the object of the homage was unmistakable. I was raised in New Albany, Indiana, across the Ohio River from Louisville, Kentucky, and in the late Seventies, when I was a boy, some of the last of the old-time steamboat races were held there. One of my earliest memories is of being taken down to the riverfront at the age of four to watch that spectacle. Twain’s face was everywhere. It was on TV, in a disturbing Claymation film called\nThe Adventures of Mark Twain,\nwhich, I have since learned from the internet, gave bad dreams not just to me but to my whole microgeneration. Every Christmas until I was a teenager, I would find waiting under the tree a fine hardback copy of one or another Twain novel, sometimes one of the editions that had those marvelous N. C. Wyeth illustrations. These gifts would then stress me out for the rest of the year. They were given in love, but with a certain expectation or pressure, as well—they were a form of cultural proselytizing—and somehow I never felt that I read or loved them well enough. My father would quiz me on the stories. Hadn’t I loved the part when such and such happened? When Huck decided he’d rather go to hell than hand over Jim to be reenslaved? No, more than that, more than any “rather,” did I grasp the fact that Huck\nactually believed\nhe\nwould\ngo to hell for this loyalty to Jim, and chose it regardless? My answers, no matter how much forced enthusiasm I tried to pump in behind them, always left him a little crestfallen, a little chagrined. In his smoke-filled basement office, he would play his recordings of Hal Holbrook doing Twain. When I was cast in the role of Joe Harper, in our seventh-grade production of\nTom Sawyer,\nhe grew briefly delighted, and suggested I revisit the novel for character insights, but the show bombed. We had too little talent for too many parts. I remember our Injun Joe, a kid named Kevin. Bless him, he had one line, and the line consisted of a single word—“Bah!”—and somehow he kept fucking it up. It required a kind of genius to fuck up this line, but he did it every time, in a different way. The director would clutch the top of her head and scrunch up a fistful of hair and say, “Oh,\nKevin!\n” I may be tidying the timeline somewhat here, but I’m pretty sure that the school play marked the end of my father’s efforts to inspire me with his devotion. He had already inflicted on me, though, some guilty shadow of it.\nIt would be perverse or less than forthcoming to depart from the topic of one’s unusually fraught youthful encounters with Mark Twain and leave the following facts unmentioned.\nWhen I was twenty, my older brother had a terrible accident, involving near-electrocution by a guitar amplifier. This happened in Kentucky on April 21, 1995, on the eighty-fifth anniversary of Mark Twain’s death. My brother flatlined and was resuscitated multiple times. There was a period of about forty-eight hours when we didn’t know if he would survive or be a vegetable or what. He made a complete recovery, which his own doctors at the hospital did not hesitate to describe as a miracle. A popular reality-TV show made an episode about it. Today my brother lives in London and is married and still plays in a band.\nWhile he was in the other world, he witnessed a scene, or underwent an experience, and was briefly able to speak about it, in this world, before it faded from his mind. I wrote it down in a notebook I kept at the time, a record of the many strange remarks he made during those days. My father later worked it into a documentary poem that he sent around to the family. My brother had been sitting there in his hospital bed, his face (it seemed to me) very serious and emotional, and I asked him if he was okay.\n“I was thinking about the vision I had when I knew I was dead.”\nThere was silence.\n“I was on the banks of the River Styx,” he said. “The boat came to row me across, but . . . instead of Charon, it was Huck and Jim. Only, when Huck pulled back his hood, he was an old man—like, ninety years old.”\nI wrote about this once before, twenty-five years ago, but something that failed to occur to me then is that my brother said nothing about Jim’s age. It is not specified in the novel,\nAdventures of Huckleberry Finn,\nbut Jim is usually taken by scholars to be in his late twenties or early thirties. It seems that, in the vision, Jim was still that age, or not remarkably older. Yet Huck had grown ancient.\nAlso, they were in hell.\n“What is your name, my boy?”\n“Dey calls me Jimmy, sah, but my right name’s James, sah.”\n—Mark Twain, “Sociable Jimmy,”\nNew York Times\n, November 29, 1874\nLike Kafka when he went to see the aeroplanes at Brescia, did I not come to Percival Everett’s\nJames\nwith a kind of hostility? And if the answer is yes, what was the source of it? Certainly not any sense that a sacred cow of some kind had been violated, although the way I wrote “certainly” there makes me wonder if, in fact, it was that. Yes, there may have been some childish instinct to defend Twain.\nBut Mr. Everett, you must realize that\nTwain himself\nsaw Jim as fully human, and in the context of the time . . .\nHilarious. Everett knows this as well as anyone. Twain’s “humor and humanity,” he acknowledges in the acknowledgments, “affected me long before I became a writer.”\n3\nNo, this hostility was more an expectation that the “brilliance” of the concept—Jim becomes James, the runaway becomes the self-emancipated, the boy (in the racist sense) becomes a man, and the whole polarity of the narrative, in which Huck’s choices matter, while Jim’s are incidental, has been reversed—would prove greater than the novel could possibly prove good, and that the story, as a result, would amount more or less to an extended skit, throughout the interminable course of which you would have to keep reminding yourself how brilliant the idea was, to make your hand turn the actual pages. The worst kind of book, the kind we are assailed by in this era, the kind of book people tell you they “loved,” and you think to yourself, They cannot\npossibly\nhave read the book I tried to read. And often if you ask probing questions you find that they have not done so, or that they, like you, tried and failed, but came away\nloving\nthe book nonetheless, or feeling a need to say as much, and after a while, when you have been burned enough times, it can feel like this is what books have become, things not to read but to\nlove\n.\nYou know the story about Hemingway and Joyce’s\nUlysses\n? A “most goddamn wonderful book,” he called it, but when Hemingway died and they examined his copy, a third of the pages were “uncut”—they had never been read. Or even seen! Well, obviously there is a place for books like that in the world.\nHemingway also said, or had the character of himself, “Papa” (!), say, in one of his books,\nGreen Hills of Africa,\nwhich is classified as non-fiction but contains many scenes that read not quite plausibly as such, “All modern American literature comes from one book by Mark Twain called\nHuckleberry Finn.\n” But then Papa adds,\nIf you read it you must stop where Nigger Jim is stolen from the boys. That is the real end. The rest is just cheating. But it’s the best book we’ve had. All American writing comes from that. There was nothing before. There has been nothing as good since.\nSo many things about this oft-quoted statement are hard to understand. It’s the best American novel, but we should not read the\nlast\ntwelve chapters\n? That’s the final almost one hundred pages of the book. Ordinarily that would constitute a significant knock on a novel’s quality. Hemingway means that there is something deep and intrinsic in the good parts, qualities so important that they outweigh otherwise fatal formal defects. Hemingway is, by the way, technically correct to suggest the cut, in the sense that a good editor, the best editor, might have made the same suggestion to Twain. The chapter that Hemingway recommends be the last chapter is the aforementioned crucial chapter, the one in which Huck decides to go to hell rather than betray Jim. It is also the chapter in which Huck learns that Jim has been kidnapped and temporarily sold back into slavery by a confidence man. The novel would thus end in existential tragedy, with Huck making his moral choice and losing Jim anyway. Huck: bereft. Jim: reenslaved. Tom: who gives a shit. We are reminded (I am reminded, by a piece that Greil Marcus wrote for the\nLos Angeles Times\ntwenty-eight years ago, on the occasion of the\nlast\nTwain revival) that the critic Leslie Fiedler, in his 1960 book\nLove and Death in the American Novel,\ncalled Huck “the first Existentialist hero”:\nHe is the product of no metaphysics, but of a terrible breakthrough of the undermind of America itself. In him, the obsessive American theme of loneliness reaches an ultimate level of expression, being accepted at last not as a blessing to be sought or a curse to be flaunted or fled, but quite simply as a man’s fate. There are mythic qualities in Ahab and even Dimmesdale; but Huck is a myth: not invented but discovered by one close enough to the popular mind to let it; this once at least, speak through him. Twain sometimes merely pandered to that popular mind, played the buffoon for it, but he was unalienated from it; and when he let it possess him, instead of pretending to condescend to it, he and the American people dreamed Huck—dreamed, that is to say, the anti-American American dream.\nWe dreamed it together . . . how lovely.\nHemingway says that “Nigger Jim” (Twain never used that epithet) is “stolen from the boys.” That last part is wrong in two ways. Jim is not stolen from “the boys.” Tom is not there when it happens. Nor is Jim “stolen.” Huck does not own him. Huck\npretends\nto be Jim’s owner, when he finds out that Jim has been caught, in order to conceal their true relationship: Jim is his friend, and Huck is helping him escape. It is not possible for Jim to be “stolen” from the boys, or even from Huck. And don’t say, “Oh, you know what he means . . .” No, it was the wrongest possible word Papa could have used.\nThose details aside, what on earth does Hemingway mean when he says, “All American writing comes from that. There was nothing before. There has been nothing as good since”? People have wondered, psychoanalyzed. Hemingway could be so fantastically full of shit. There was nothing before? But there were Melville and Dickinson and Hawthorne and Whitman and Poe and . . . There had been “nothing as good since,” i.e., between 1885 and 1935, when Hemingway published those sentences? But there had been Henry James and Edith Wharton and Willa Cather and William Faulkner. Speaking of Faulkner, what did\nhe\nmean, twenty years later, when he called Twain “the father of American literature”?\nIf we could get under and behind this tradition of hyperbole, figure out what motivates it, we might learn something not only about Hemingway and Faulkner but about ourselves and this country. Why has it so often seemed necessary to\nclaim\nTwain in this fashion? Presumably the answer involves some variant of whatever instinct prompted Everett to subvert (and thereby affirm the power of) the very book that gave rise to this glorification.\nJames\nis a good novel and not just a clever idea. Everett accomplished the task that was necessary to make this so: not to criticize Twain and\nhis\nnovel (though he does that often enough and subtly enough) but to provide the element most sorely missing from those original\nAdventures,\nnamely, the interior life of Jim. The first sentence made me burst out laughing: “Those little bastards were hiding out there in the tall grass.” Or rather I burst out laughing ten sentences later when it is revealed that those little bastards are Huck and Tom. Much further into the story, the choice of insult will provide the matter for a deeper joke, one that may even transcend the status of a joke.\nThe place where Everett has left himself open to the most obvious criticism is in his decision to make James\n4\nan intellectual. The man is not merely intelligent, in other words, in the way that any healthy, alert person might be. He is instead a highly literate and systematic thinker, who, when he dreams, is visited by John Locke and Voltaire. They discuss such topics as the nature of civic equality and natural rights and the real-world responsibilities of philosophers. It’s absurd, in a way. Jim becomes not just James but a heroic scholar-in-exile, of the kind that one might occasionally have encountered not among the enslaved or formerly enslaved, as a rule, but in the free black communities of the South, the social context that produced a writer like David Walker (of\nWalker’s Appeal\n). By drafting James as a man born enslaved who rose to this level of cultural sophistication by reading books in the library of his owner, Everett has situated James’s backstory among an infinitesimally tiny group of historical destinies. One is meant to think, perhaps, of Toussaint Louverture in his Haitian cabin, reading the Abbé Raynal’s\nHistoire des deux Indes.\nAll but unique, in other words, and therefore—one could argue—a flawed lens through which to view James’s full humanity.\n5\nBut these passages, and the cluster of authorial decisions behind them, are redeemed by, of all things, laughter. At least, I’m pretty sure that I can hear Everett laughing behind them, or smiling, anyway. He hears the thing in me (and, I have to assume, in many other readers) that starts to rise up and protest, “Hey, come on, did you have to make him an\nintellectual\n?,” and the writer in him laughs.\nI see you little bastards hiding out there in the tall grass\n.\nJames and I will decide what he dreams about.\nTwain was interested all his life in what he called “mental telegraphy,” the capacity of human brains to transmit and receive information to and from other brains, at a distance. We smile at it now, but this was one of the ways in which Twain was very much of his time—“unalienated,” in Leslie Fiedler’s phrase, from the American popular mind. A famous instance of this phenomenon—a specific subspecies known as “simultaneous dreaming”—supposedly happened to a person he knew well, a woman whose family was intimately attached to his own, in Elmira, New York, where his wife, Olivia “Livy” Langdon, came from, and where the Clemenses spent almost every summer for decades. The name of the woman, the dreamer in this case—or one of the two dreamers who had the same dream—was Adele Amelia Gleason. She was also, I have discovered, the author of a reminiscence of Twain that is unknown.\nThe Gleasons of Elmira were a fascinating family, and I mean that not in a facile or phatic way. The mother, Rachel Brooks Gleason, was one of the first licensed female doctors in America—by most accounts the fourth. The father, Silas, was also a doctor. Their only daughter, the dreamer Adele, became a doctor, too. It would be truest to describe them as a family of healers. And they were Northern progressives of the classic late-nineteenth-century stamp: Feminists and advocates for the rights of the poor. Abolitionists and, later, ardent supporters of freedmen’s schools.\nThe Gleasons were best known, though, for operating, in Elmira, what was called a hydropathic sanitarium, or more colloquially a “water cure.” It was known up and down the eastern seaboard. People in Elmira referred to it as simply “the Cure.” It had been built on the site of a sulfurous spring, and you drank a lot of sulfur-infused water while you were there, and took a lot of baths. Mainly, though, you just chilled out. And this is why hydropathy was not like many of the other quack medical therapies of the not-quite-modern era. It did not involve any bad ideas. No snake oil, for the most part—no sketchy procedures, no unscrupulous claims. You drank a lot of water, ate healthy food, did gentle exercise, and avoided stress. Naturally, people\ndid\nget better.\nLivy, Twain’s wife, had done multiple stretches with the Gleasons as a young woman, including a long stay after an accident she suffered as a teenager, when she fell on the ice and lapsed into a sort of paralysis afterward. (People said it was neurasthenic, in her head.) It would be hard to overstate how much she, and Twain, trusted the Gleasons. Twain described Mother Rachel as “almost divine.” Her popular book on women’s health,\nTalks to My Patients,\nwas reviewed by this magazine in 1870, and praised for its “sound philosophy” and “sterling common-sense.” She was also a midwife who safely delivered (or “caught,” as the midwives say) all four of the Clemens children. When Livy fell ill, Twain arranged for Rachel to come and stay with the family. He himself spent plenty of time at the Cure. The Clemens cottage stood right across the road from the place, up on East Hill, overlooking Elmira. A woman named Hettie Devinny Wagner, who had spent time at the Cure as a sickly girl, later remembered,\nMany a time Mark Twain, or members of his family, came over for special baths for which the place was noted; their cottage had no modern plumbing in those days. If Doctor Gleason was not busy, he and Mark Twain would sit on one of the long porches and spend many minutes in happy talk.\nIt was during one of these visits, most likely, that Twain met the Gleason daughter, Adele.\nShe was described, when young, as “exceedingly beautiful”—the future President Garfield once stuck his head out the window of a moving train car to get a glimpse of her on the platform (he professed to have been concerned for her safety)—and she was possessed of an authentic literary streak that rendered Twain an object, in her eyes, of particular interest. She wrote poems and stories and a novel. The work is good enough, in places, to have remained compelling. In her short fiction, she paid close attention to the inner lives of black people—servants, mainly, who passed through her orbit—rendering their speech in the heavy dialect that, like Twain’s efforts in that line, offends our sensibilities now. Her best stuff is probably her poetry. Her collection of 1888 is strangely titled\nSongs and Verses for Christmas,\n“strangely” in that it consists entirely of hauntingly mournful elegies to lost or broken love, such as “The Storm”:\nAs bends the tree in the wind\nBefore the rain,\nSo do I bow, when thou comest,\nIn sudden pain.\nIt seems likely that Adele was a lesbian. There is nothing explicit on that, in the documents, but several aspects of her biography point to it, starting with her lifelong unmarried status and, for that matter, lack of any recorded attachment to a man. Many of the poems speak achingly of love for a woman. In “Pine Lake,” for instance:\nBeam of the golden sun, kiss her gold hair,\nKiss it as I would\nWere I but there.\nA sentence in an anonymous review of Adele Gleasons’s one published novel,\nThe Georgia Belle,\nfrom the\nElmira Telegram\nof October 13, 1895, suggests that her sexual and romantic inclinations were not a secret. “It is more than suspected,” wrote the reviewer, “that Miss Gleason has an ardent fancy toward and a deep affection for the beautiful young women of Dixie.”\nBorn on Christmas Eve, in 1850, she took her medical degree at the University of Michigan. “The daughter, M. Adele Gleason,” wrote an Elmira journalist, “promises to be to the generation of women to which she belongs what her mother was to those who preceded her.” She campaigned on behalf of prostitutes and unwed mothers. She traveled all over—to London and Scotland, Denmark and India, even to Iceland, and to California (where she bought a piece of land from the son of John Brown—\nthe\nJohn Brown, the one who led the raid on Harpers Ferry—and built her own sanitarium). During World War I, she joined the American ambulance corps in France, serving in the bandage room of the hospital in Neuilly-sur-Seine, treating “the wounded from Ypres and other devastated places.” In Buffalo, New York, she founded a “social settlement” called Mortimer House, giving embroidery and gardening classes to children, taking in strays. It was a beloved institution in that city for years. For part of that time, she was living with a woman named Margaret Taylor, a teacher who was her “assistant in social settlement work,” and in 1907 a newspaper item finds them on their way to Georgia, “for a spring vacation.” Could this be the Georgia Belle? Did they briefly have happiness?\nOne gets the sense that she suffered. Think of being who and what she was, in the America of that time, going everywhere and doing all that good, with a bulb so bright, and that passion. It seems that her mental health was not always strong. There are reports, along the path, of undisclosed illnesses that sound suspiciously like nervous breakdowns. In 1902, she appears in the diary of—of all people—Charlotte Perkins Gilman, the lesbian writer and feminist activist, famous even today for her story “The Yellow Wallpaper,” about a woman’s all but literal imprisonment in the strictures of the patriarchy. On December 6, 1902, Gilman writes,\nRise early for 8\nam\ntrain. Hour late. . . . Go to Buffalo. . . . Sleep two hours—walk two miles—dine lightly & lecture on Public Ethics again. Very well indeed. . . . Dr. Adele Gleason—a shocking wreck. Poor woman! & she was so glorious.\nShe died in an insane asylum, in 1930. She is buried in the Woodlawn Cemetery in Elmira, not far from Twain.\nHer dream, the simultaneous dream, had taken place in 1892, when she was forty-one and living with her parents at the Cure. Look online and you’ll find that the dream is not forgotten, in the world of folks who study the paranormal. It was first reported in June of 1895, in the\nJournal of the Society for Psychical Research\n(an issue of which, from that same year, mentions recent writing by Mark Twain, for\nHarper’s Magazine,\nabout his “experiences in what is termed Mental Telepathy or Mental Telegraphy”):\nThe night of Tuesday, January 26th, 1892, I dreamed between two and three o’clock that I stood in a lonesome place in dark woods. That great fear came on me; that a presence as of a man well-known to me came and shook a tree by me, and that its leaves began to turn to flame.\nThe dream was so vivid that I said to the man of whom I dreamed when I saw him four days later, “I had a very strange dream Tuesday night.” He said, “Do not tell it to me; let me describe it, for I\nknow\nI dreamed the same thing.”\nWhen Mark Twain died, on April 21, 1910, at the age of seventy-four, Adele was living in India, teaching physiology and gynecology at a mission medical school in Rajahmundry (today Rajamahendravaram), near the Bay of Bengal. She read that her beloved neighbor had died in bed of a heart attack at his Connecticut mansion, Stormfield, named in part for a character in the last book he published during his lifetime,\nExtract from Captain Stormfield’s Visit to Heaven,\nwhich was excerpted in this magazine, in two parts, in 1907 and ’08, and that somewhat uncannily—given Twain’s reported prediction that he would soon “go out” with Halley’s Comet, which had been in the sky when he was born—concerns the travels of a deceased outer-space captain (a proto–Hunter Gracchus of the stars) who half-unintentionally falls into a race with a giant comet that pulls him off course.\nTwain had been in bed for some days, fading. His heart was failing. He had always smoked too much—twenty cigars a day, supplemented by puffs on a pipe. “When he had passed the point of speech,” one of the news reports read, “and it was no longer certain that his ideas were lucid, he would make the motion of waving a cigar and, smiling, would expel empty air from under the moustache still stained with smoke.” His daughter Clara, his only surviving child, was nursing him. Livy had given birth to four altogether, three daughters and a son. The eldest, a boy, Langdon, was born weak and lived barely a year and a half. The first daughter, Susy, had died of spinal meningitis back in 1896, when she was twenty-four. The youngest, Jean, had died only four months before her father, there in that same house, Stormfield. She had an epileptic seizure in the bathtub. And Olivia, his Livy, she had already been gone six years. She was never well, and her heart had given out in 1904. Too many losses. The papers all mentioned that brokenheartedness had brought on his final decline.\nAdele sat down immediately and wrote an essay about Twain, a remembrance of the man, and sent it off to the\nCivil and Military Gazette,\nin Lahore (today Lahore is in Pakistan, but back then it was in the territory that the British called the Raj, and English-speaking people would have placed it in India). Twain himself had passed through Lahore, in the spring of 1896, on the world travels that a year later he described in\nFollowing the Equator.\nOn a Wednesday night in March, according to the\nCivil and Military Gazette,\nhe lectured in a “queer rambling style” before a “large and enthusiastic audience.” The crescendo came when he read aloud the passage involving that moment, the famous moment, in “the story of Huck Finn and Jim the slave,” those deathless sentences “with whose humour such a quaint strain of pathos is blended,” and which describe “the struggle of a sound heart against a deformed conscience.” At the end of the struggle, the reporter had Twain say, “My conscience was very sick, but I myself was powerful glad.” That language does not appear in the novel itself. Either Twain read a variant draft or the reporter mistranscribed his remarks. Most likely the latter. Huck is not being cute, at that moment in his and Jim’s story, after he has decided, “All right, then, I’ll\ngo\nto hell.” (Those italics on “\ngo\n”!—the full meaning of them, of the sound of them, could never be explained to a non-native speaker, and can hardly be parsed by a native one; they represent, without a doubt, a high-water mark in the history of idiomatic American prose; to have italicized “I’ll” or “hell” would have put the emphasis on Huck’s personal heroism, but to italicize “go” indicts, instead, an entire monstrous moral code: “Fine, you motherfuckers. I’ll\ngo\nto hell, before I send Jim back to the plantation.”)\nAdele sits there at her desk at the mission school in Rajahmundry and pours out her memories, then sends them off to the editors of the\nCivil and Military Gazette,\nwhich publishes them days later. Her essay appears on Friday, April 29, 1910. Twain has been dead hardly a week. What she writes is splendid, unforgettable. It has never been noticed or quoted or cited before, not even by the Twain scholars. I am not sure how that happened, how it slipped through the net. I suppose its being published in Lahore had much to do with it. All curiosities of syntax and typography are [\nsic\n].\nSAMUEL L. CLEMENS\n(specially contributed.)\nAs I knew him in the many summers he lived on estates joining ours in Elmira, N.Y., U.S.A., his wife’s birth place; the wonder of Mark Twain’s real personality increased. He reminded one of one of the many definitions of genius, a constant capacity for growth. The reproach of age is that it is “stale, flat and unprofitable.” Mr. Clemens’ age was always\nyouthful,\nnot in the way of amusement or of seeking amusement, for he seemed never to require them in an artificial sense. He found amusement and intense interest in everyday things about us, and in every-day people which those very people themselves were incapable of finding. I said once.\n“Mr. Clemens, you have been guest of Emperors and Princes and Presidents and Magnates and Artists and—\n“Yes,” he broke in with a quizzical smile “and of boarding house keepers!” “And” I resumed, “what is the best, the very best time you ever had or have, in your life?”\n“The best time I ever had or can have is when I feel a new idea, one I have never had before, coming into my mind. Then I want to share it with other people.”\nHe had the intense hatred of being interrupted that all authors and artists have, yet a gentle humility, touched with sadness or quaint jests, was to me most prominent in his character. His way of serving others was very characteristic. A hunch backed old negro by great courage had saved the lives of Mrs. Clemens’ two daughters when, as children with their aunt, they were driving down the steep hill from the place he summered in.\n6\nTo see Mr. Clemens sighting this old man, as he walked up the steep hill beside his team of poor old horses, and see him in his ever spotless white suit, run down the road to help the old negro block the wheel, and rest the horses, or walk beside him talking and telling him stories to make him laugh was a pleasant sight. He used to say “A man must be either a genius or a fool to be interesting; God spare me the average.” He often said a farmer with whom he talked one evening paid him the greatest compliment he ever received. As he was parting from his chance acquaintance he remarked, “I must go home now.” “Where do you live?” asked the farmer. “Over there,” said Mr. Clemens pointing to the house, “Lord! be you him?” was the man’s amazed response. . . .\nThe birth of each of his children was celebrated by a thank offering, in the form of a well cut stone trough placed at the roadside for the use of the horses as they toiled up the steep hillside. His helpful goodness, his shrewd insight, his peculiar way of helping people just in the nick of time, made the difference between success and failure; hope or despair, to many hundred people during his life time.\nHe would neither hide nor exhibit his charities. If he met an old doctor or clergyman, whom he knew and trusted, at dinner or in an afternoon of relaxation, he would sidle up to him or her and extend a few bills in a tight ball with the peculiar awkward twist of his left arm, and say, “Here take this. I feel about that much, but I don’t know who it ought to go to.” . . .\nIn his never failing devotion to his wife there was an element of anxious humility most touching to see. The fact that she wanted anything, or that he had displeased her in any way, was more absorbing to him than any honour or success in life. His wife’s last request was that he should stay with Jean, and he husbanded his failing strength in every way after her death to fulfil this request. The picture of their playing chess together shows her head as classic in its beauty as Portia’s might have been. It will be the treasure of some art gallery.\n7\nThe tragic—or shall we say, beautiful—death of this daughter must have left him with little motive for this life. . . .\nHis carelessness of practical things caused him to be cheated and defrauded frequently, to the indignation of his wife especially; but the gentleness of his forgiveness of a wrong was trying sometimes. But noble and generous he always was. His wife’s interest in India was very great, her letters from this country remarkable. He said to us: “India demands a new life,” then with a half given sigh “I want to live it but I can’t.”  . . .\nEvery country holds some of his personal friends who will never forget Samuel L. Clemens.\nadele a. gleason\nAdele Gleason’s profound reminiscence of the man she knew as Mr. Clemens is a striking example of the extent to which, even with a figure as archivally picked-clean as Mark Twain is (and has long been, for the most part), discoveries remain to be made. Granted, nothing in this one rewires any major critical or biographical assumptions, but neither are its revelations inconsequential. The handful of remarks attributed to or quoted by him are of such a choice nature—“Lord! be you him?”—that they now can do as they were meant to and enrich the corpus of his dicta and mots.\nThere is an intimate focal depth in Gleason’s tribute, which brings us close to Twain in a manner that, even after one has read through thousands of pages by and about him, seems no less rare. Ron Chernow’s new biography, titled simply (boldly?)\nMark Twain,\nobviously makes no mention of the text. There is no shame or negligence in that\n—\nit has been lost for more than a century. It is more of a problem that Chernow makes no mention of the Gleasons, and fails to discuss the Cure, where more than one member of the Clemens family convalesced. He does not even name Dr. Rachel Brooks Gleason. This is hard to understand in a book of more than a thousand pages (especially when Rachel is written about more than once in Ron Powers’s excellent twenty-year-old\nMark Twain: A Life,\nand the significance of the Gleason connection is clearly established in Laura E. Skandera-Trombley’s\nMark Twain in the Company of Women\n).\nIn Chernow’s defense, hard decisions—on what gets included and what left out—are the cost of doing business for a biographer who wants to produce a readable life, and his\nTwain\nis eminently readable. Anyone’s biography could run to a million pages, if everything were thrown in, and when the life in question is Twain’s, among the most multifarious and eventful and acquaintance-rich and correspondence-producing in the whole of the nineteenth century—“A many-sided man, this Mississippi pilot,” wrote the editor Charles Vale (\nSing, Muse, of this many-sided man!\n)—those decisions must be cold-blooded indeed, and will involve an unavoidable measure of arbitrariness. But the name\nGleason\nshould be in there.\nBefore I lodge a final grumble about stuff that Chernow’s biography leaves out, let me say that he gets a mighty amount in. He has wrestled the white-suited whale of Twain into a sturdy aquarium tank.\nThat being said,\nhow could he leave out the Sergeant Fathom and steamboat\nArago\nletters, the parodies of newspaper reports on river conditions that Twain wrote and published in 1859 and ’60 during his stint as a licensed pilot on the Mississippi? Chernow has this slight paragraph, which stops just short of alluding to their existence:\nEven though Sam Clemens published little during his four-year stint on the river, he squandered nothing from a literary standpoint. He harvested the anecdotal riches of serving on more than a dozen boats and was so content that he foresaw a peaceful, never-ending life afloat. “Time drifted smoothly and prosperously on, and I supposed—and hoped—that I was going to follow the river the rest of my days, and die at the wheel when my mission was ended.”\nThese sentences breezily pass over a crucial period in Twain’s development as a writer. The omission seems especially odd when we consider the great importance that Chernow places on Twain’s piloting days. He titles his prelude “The Pilot House”—the implication being that this provides a kind of guiding (as it were) metaphor for both Twain’s life and Chernow’s book—and he repeatedly quotes Twain’s fond memories of that short chapter in his career (“Verily, all is vanity and little worth—save piloting”). The pen name, Mark Twain, derives (as we used to learn in school) from a boatman’s call. “By the mark, twain!” meant that the water was two fathoms deep—or twelve feet, according to the leadsman’s weighted sounding rope—and by extension that it was safe to keep going: there was enough water for the boat to float through and not run aground. A hopeful cry, then.\nTwain consistently lied about where he’d got the name from—the idea of using it, that is. He claimed that he had essentially stolen it, albeit in an act of homage, from an older riverboat pilot—one of the original Mississippi steamboat men—Isaiah Sellers, who (according to Twain) used to generate occasional on-the-spot reports of river conditions and send them to the\nPicayune\nin New Orleans, signing himself “Mark Twain.” These reports were said to be amusingly all-knowing in tone. “Hoary” would be the word, I suppose. Twain writes about them, and Sellers, in\nLife on the Mississippi,\nand even quotes one of the alleged reports: “My opinion is that the water will be 4 feet deep in Canal Street before the first of next June,” etc.\nA fatal difficulty arises in that scholars have gone looking for these items, in the old newspapers, and they appear not to exist. Certainly there are none signed “Mark Twain.” Sellers existed—we can confirm that—but there is no evidence of his having published anything at all, much less under the famous pen name. An independent Twain scholar in Texas, named Kevin Mac Donnell, has recently discovered a far more likely source: a humor sketch, from a magazine published in 1861, that featured a character called Mark Twain. This would explain not only where the name came from but why Twain may have felt motivated to lie about it—he had basically plagiarized it, and not by way of honoring an obscure figure whom he felt bad about having lampooned, but from a popular source. Off the rack, as it were.\nThe reason that Chernow’s failure to take cognizance of the steamboat\nArago\nseems problematic enough to nitpick has nothing to do with the business of the pen name, which Chernow does discuss. He even deals with Isaiah Sellers, on one page, and alludes to the theory about the humor magazine,\nVanity Fair\n(neglecting to mention that this was not the modern-day publication). What matters is that Twain emerged as a stylist during this phase. He located the mask. We know that he himself saw it this way, because he writes, in\nLife on the Mississippi,\nthat “one of these [Sellers] paragraphs . . . became the text for my first newspaper article.”\nMind you, these river parodies were\nnot\nthe real debut of Twain in print. There had been earlier pieces, as early as 1851, written for his brother Orion’s newspaper in Missouri. “A Gallant Fireman” is one. But to read those is to know that you are reading juvenilia. Whereas, when we come to the essay that ran in the\nNew Orleans Daily Crescent\nin 1859, we hear Mark Twain. He did not sign himself that way—his adoption of the pseudonym was still a few years off—but as “Sergeant Fathom” (which is on its way to Mark Twain, thematically). Here he is, moving along the great American river, which he knows almost by muscle memory, mocking authority.\n“My opinion,” he writes, “is that if the rise continues at this rate\nthe water will be on the roof of the St. Charles Hotel\nbefore the middle of January. The point at Cairo, which has not even been moistened by the river since 1813, is now entirely under water.”\nI started laughing when I read the word “moistened.” It is no small thing to make a person physically laugh at the remove of 166 years. And how cryptically thrilling to know that the deckhands are laughing with you down the hallway of time. They hold their sides, tears in their eyes, as Twain impersonates the ancient mariner (one of Twain’s friends, Bart Bowen, had “insisted on showing it to others and finally upon printing it”). Nor is the piece merely silly. It involves a kind of time travel, offhandedly surreal, back into an older and weirder America. “In the summer of 1763,” writes Sergeant Fathom,\nI came down the river on the old\nfirst\n“Jubilee.” She was new, then, however; a singular sort of a single-engine boat, with a Chinese captain and a Choctaw crew, forecastle on her stern, wheels in the center, and the jackstaff “no where,” for I steered her with a window shutter, and when we wanted to land we sent a line ashore and “rounded her to” with a yoke of oxen.\nA scrap of textual evidence from a year later suggests that these early river satires were noticed and read with pleasure. They represented, in other words, not only some of Twain’s first appearances in print (where his brother was not the editor), but the dawn of his identity as a writer, in the public mind and in his own. In September of 1860, the River Intelligence column of the\nSunday Delta\nin New Orleans reports, “By far the best thing we have read since the days of the Sergeant Fathom letter, [is] a Pilots’ report, emanating from Sam. Clemens, of the\nArago\n.” Whether this writer realized that Sam Clemens had also been responsible for the Sergeant Fathom letter is not clear, but a reputation was growing.\nWhat had changed? Grief and trauma had deepened him and his voice, for one. His younger brother Henry, who followed him into the riverboat line, had been blown up in a boiler explosion in June of 1858, not even a full year before the first of these parodies was published. Twain raced to Memphis, where the bodies and survivors had been brought ashore, and got there in time to watch Henry die, horribly scalded, on a cot in an improvised hospital. He never stopped blaming himself.\nHe had dreamed Henry’s death, just a month before it happened. “In the dream I had seen Henry a corpse,” he writes in his\nAutobiography.\n“He lay in a metallic burial-case. He was dressed in a suit of my clothing . . . ”\nMy failure to rise more eagerly to Chernow’s biography might have more to do with genre than with the book itself—or rather it may be the genre that I mind in the book—the relentless march of linear chronology, so at odds with the way life and its endless circlings operate. The piling on of details is meant to bring us closer to an understanding of the subject, but the process somehow insulates us from the inner life. A flattening effect becomes a deadening effect. When Twain died, one of the reports included a marvelous anecdote from his early years, meant to illustrate his sometimes “leisurely” habits:\nAn old pressman who was printer’s devil in an office where Mark was an editorial writer tells this anecdote of his habits of work:\n“One of my duties was to sweep the room where editors worked. Every day Mark would give me a nickel to get away from him. He would rather die in the dust than uncross his legs.”\nThere is such skilled comedy in this vignette. The boy has been told to sweep the office. There sits Clemens, loafing. The boy approaches. He needs the man to move his legs, so he can get under with the broom.\nTell you what, youngster, I will PAY you to leave me alone.\nNo, better: “to get away from him.” Their absurd bargain becomes a ritual: “every day.” The rhythm, the language, everything.\nHere’s Chernow: “Sam stuck to his lazy, messy habits, as one old pressman learned when he tried to sweep around him. ‘He would rather die in the dust than uncross his legs.’ ” But the “old pressman” didn’t learn it. The boy he had once been did, the printer’s devil. And the boy didn’t learn it “when he tried to sweep around him”—sweeping around him is what he\ndid\ndo, what he was forced to do—he learned it when he tried to get the man to move, so he could sweep\nunder\nhim. The good but rhetorical sentence “He would rather die in the dust than uncross his legs” is preserved, but the best part, the gratuitous “nickel to get away from him,” is gone, and with it goes Clemens, the real presence.\nThe moment, the one in which Huck decides he’ll\ngo\nto hell: “there is no more significant passage in\nHuckleberry Finn\nthan that in which Huck struggles with his conscience over the knotty problem of his moral responsibility for compassing Jim’s emancipation.” That’s Archibald Henderson, initiating a tradition in 1911, a year after Twain’s death.\n8\nPercival Everett does not include the moment in\nJames.\nI had wondered, going in, how he would handle it. He doesn’t. That tracks with the original story. When the moment occurs, in Mark Twain’s novel, Jim is not present. He has been “stolen.” So he is not there to see anything. And what would he see? A young boy sitting in a tent, maybe wringing his hands, groping in the cave of his conscience for some thought that will allow him to do what his fear wants him to do, but his better angels won’t allow. (“I couldn’t seem to strike no places to harden me against him.”) Everett gives the agonizing either-or decision to James instead. At a dramatic moment on the river he must choose whom to save: his friend Norman, who is black but passing as white, or the boy, Huck, who is . . . well . . .\nSome pages later, Jim learns that his wife and child have been sold away.\n“Jim, they were sold.”\nI had heard his words clearly, but I said, “What?”\n“They were sold.”\nJust what happened next is blurry in my memory, but I remember being on my knees. I cried, really cried. I realized that Huck was hugging me. I could feel his concern through his hands.\nOf everything I have ever read about Sam Clemens, the pages that did most to clarify my sense of the relationship that existed between the man and his art are to be found in the introduction to a book titled\nMark Twain and the South,\nby the late American historian Arthur G. Pettit. “Having learned about the man’s protean personality,” Pettit writes, “we find that his feelings on race and region move in an intelligible direction . . . there is a clearly traceable movement away from the white South and toward the black race.”\nThen this remarkable paragraph:\nUnlike most Missourians or other Americans, the Clemenses sometimes owned a few slaves, and Clemens himself accepted the South’s peculiar institution well into his twenties. His early notebooks and journals are liberally sprinkled with jokes about black body odor, fried nigger steaks, black sexual promiscuity, and the evils of miscegenation. Yet he eventually married into an abolitionist family, befriended Frederick Douglass, financed a black artist’s apprenticeship in Paris, and supported several black students through Yale Law School. As Mark Twain he lectured in all-black churches, championed the cause of Booker T. Washington, wrote blistering essays about atrocities committed against blacks, and gave large doses of dignity and power to three of the outstanding black characters in nineteenth-century literature. He began his career as a segregationist, turned himself into a champion of interracial brotherhood, and ended his life as a prophet of racial war.\nCan there be any doubt that therein hunkers the secret of the enduring power of the moment? Huck’s redemption was Mark Twain’s. Or so Clemens hoped. And we seem to have always wished that this redemption might be ours someday, as a nation—the\nreal\nend of the novel, as old Papa had it (though to read Everett’s\nJames\nis to wonder if it wasn’t a beginning, instead, so generative has it proved). Everett has climbed into Twain’s dream and dreamed it with him. Dreamed the same dream differently. And now the dream is altered. Is that what a real myth is, a dream that runs along beneath the dreamers, like a river? Some voice goes on insisting that we are on this raft together.\nFrom the\nJune 2025 issue\nDownload PDF\nFrom the Archive\nTimeless stories from our 175-year archive handpicked to speak to the news of the day.\nEmail address\nSign Up\nGot it! Thanks for signing up!\nJohn Jeremiah Sullivan\nis a contributing writer for\nThe New York Times Magazine\n. He lives in Wilmington, North Carolina. This essay is part of a series supported by the John Templeton Foundation.\nTags\n175th anniversary\nAbolition\nAmerican literature\nHumor\nJohn Jeremiah Sullivan\nJohn Templeton Foundation\nMark Twain\nPercival Everett\nRachel Gleason\nRon Chernow\nSamuel Clemens\nShelley Fisher\nSymposium\nThe New Adventures of Huckleberry Finn\nAdjust\nShare\nI arrive, perhaps somewhat eccentrically, at that round fifty-year figure by countenancing the span between 1866, when this magazine published, and inadvertently attributed to “Mark Swain,” Twain’s first work of serious journalism, or at least the first to appear in a major national venue—a piece of experimental on-the-spot maritime disaster reporting titled “Forty-Three Days in an Open Boat,” which symphonically blends the voices of several survivors of the burning of the clipper ship Hornet in the Pacific (a thirty-year-old Twain, traveling in the archipelago that would become Hawaii as a special correspondent for the\nSacramento Union\n, had been able, via the help of his friend the diplomat Anson Burlingame, to gain access to the emaciated sailors in the hospital and collect their testimony; the story goes that Twain himself, suffering so badly from saddle sores that he could not walk, was smuggled in on a stretcher)—and, at the latter end, the year 1916, six years after Twain’s death, when\nHarper’s\nran, in serial installments, his unfinished novella\nThe Mysterious Stranger\n, a book with a tangled compositional history of false starts and competing drafts, but which is strangely easy to summarize, plotwise, insofar as it tells the story of Satan’s teenage nephew, Satan, and his adventures with some Austrian boys, versions of Huck and Tom, whom he helps to understand that the universe is a hideous and meaningless void.\nO’Brien neglected to quote a passage of Twain’s that may be particularly apropos, in regard to our present moment, some lines that his daughter Clara admired: “Here in our democracy,” he once said, “we are cheering a thing which of all things is most foreign to it and out of place—the delivery of our political conscience into somebody else’s keeping. This is patriotism on the Russian plan.”\nRalph Ellison once remarked that Twain had “made it possible for many [black writers] to find our own voices,” and long before that had made an even more suggestive comment, arguing that “the black man [was] a co-creator of the language that Mark Twain raised to the level of literary eloquence.” These statements made a deep impression on Shelley Fisher Fishkin and featured prominently in her book\nWas Huck Black?\n, which anyone interested in the commentary surrounding Everett’s novel (or in American culture, for that matter) would do well to consult.\nI paused there and wondered whether to write James or Jim, a sign that Everett has already won. He has already broken into the lab and messed with the DNA of one of our foundational literary archetypes.\nPerhaps the most remarkable part of Fishkin’s new book, and certainly the one that will prove most controversial, is the fourth chapter, “Jim’s Version: An Interpretive Exercise,” in which she attempts a fictional experiment that in many ways is the perfect opposite of Everett’s project. Like Everett, she writes the\nHuckleberry Finn\nstory in the voice of Jim or James, but not in an elevated diction, stripped of dialect. Instead she leans into the dialect, giving the narrative over to it entirely, on the counterintuitive grounds that, as she puts it, “Jim’s voice is not a ‘diminished voice.’ Rather, it is our continuing prejudice against Black English that leads us to devalue and fail to appreciate its richness and vitality.” Few readers, I imagine, will feel that the chapter rivals Everett’s performance as literature, but the mere fact that two brilliant writers, sympathetic to Twain but aware of his weaknesses, could come up with such markedly different stylistic strategies for approaching the Problem of Jim testifies to the ongoing cultural fertility of the source material.\nThis is John T. Lewis, acknowledged to have been one of the models on whom Twain based the character Jim. Lewis had saved the lives of Twain’s sister-in-law, Ida; her daughter, Julia; and the girl’s nurse, Nora, when, by an extraordinary act of keenness, courage, and strength, he had bounded into the road and stopped a runaway carriage, which had been dangerously accelerating down a steep hill. Lewis was literate and liked to read. Twain used to send him early copies of his works.\nThis photograph seems somehow not to have survived.\nHenderson was a professor of mathematics at the University of North Carolina, and a biographer of George Bernard Shaw. He met Twain once. We learn this in Chernow’s biography. Twain is on his way to England, to receive an honorary degree at Oxford. “By an extraordinary coincidence,” Chernow writes, “he had sailed on the same boat as Archibald Henderson, the biographer of George Bernard Shaw.” (“Extraordinary” seems strong, here—God, why can I not give Chernow a break? I must wish I had written this book or something…. Still, “extraordinary” is strong, no? It would make a kind of sense if Chernow were deploying it on the basis that this very same Archibald Henderson would soon write the first biography of Mark Twain, but this is not mentioned anywhere in Chernow’s book, nor is that earlier biography cited.)\nSubscribe for Full Access\nCurrent Issue\nAdvertising\nPermissions and Reprints\nInternships\nCustomer Care\nContact\nCareers\nClassifieds\nHelp\nArchive\nSubmissions\nFind a Newsstand\nAbout\nMedia\nStore\nTerms of Service\nPrivacy Policy\nCurrent Issue\nContact\nArchive\nAbout\nAdvertising\nPermissions and Reprints\nInternships\nCustomer Care\nCareers\nClassifieds\nHelp\nSubmissions\nFind a Newsstand\nMedia\nStore\nTerms of Service\nPrivacy Policy\n© Copyright 2025\nHarper's Magazine Foundation\nDo Not Sell My Personal Information\n< Previous Issue\n|\nView All Issues\n| Next Issue >\nJune 2025\n< Previous Issue\n|\nView All Issues\n| Next Issue >\nTable of Contents\nClose\nMust be logged in to access Microfiche Images.\n“An unexpectedly excellent magazine that stands out amid a homogenized media landscape.” —the\nNew York Times\nJoin us.\n“An unexpectedly excellent magazine that stands out amid a homogenized media landscape.” —the\nNew York Times\nJoin us.\nYou’ve read one of your two free articles this month. Subscribe for less than $2 an issue.\nYou’ve read your last free article this month. Subscribe now for only $21.97 per year to continue reading.\nDebug\nClose"
  },
  {
    "title": "Binary Wordle",
    "href": "https://wordle.chengeric.com/",
    "content": "Binary Wordle\n0\n1\nENTER\nUse keyboard (0, 1, Enter, Backspace) or buttons to play\nSponsor: Don't like waiting on hold? Try\naltodial.com\nMore cool stuff at\nchengeric.com"
  },
  {
    "title": "Machine Code Isn't Scary",
    "href": "https://jimmyhmiller.com/machine-code-isnt-scary",
    "content": "Jimmy Miller\nMachine Code Isn't Scary\nThe first programming language I ever learned was ActionScript. Writing code for Macromedia's Flash might be the furthest away from \"bare metal\" as you can possibly get. As I continued learning new languages, this starting heritage stuck with me. I was mostly interested in high-level, \"web languages\". Low-level languages felt impenetrable. Over time, I learned a bit more about them here and there, but for some reason, this notion stuck with me. Low-level things are scary, and machine code epitomized that most directly. When I Googled things asking about writing in \"straight machine code\", I was met with discouraging messages rather than learning.\nEventually, I decided I needed to overcome this barrier if I was going to achieve my goals. In doing so, I learned something I didn't expect.\nMachine code isn't scary. If you can make sure your JSON conforms to a JSON schema, you can write machine code.\nWhich Machine Code?\nOne problem with machine code is that there isn't simply one standard. There are many different \"instruction sets\" depending on the processor. Most modern PCs use x86-64 machine code, but newer Macs, Raspberry Pis, and most mobile devices use ARM. There are other architectures out there, especially as you go back in time. The goal of this article won't be to give you a deep understanding of any particular instruction set, but instead, to give you enough information about how machine code typically works so you cannot be afraid of machine code. So we will start by having our examples be in ARM 64-bit (also written as aarch64). Once we have a decent understanding of that, we will talk a bit about x86-64.\nMachine Code Basics\nTo understand the basics of machine code, you need three concepts:\nInstructions\nRegisters\nMemory\nInstructions are exactly what they sound like; they are the code that will run. Machine code instructions are just numbers. In fact, in AArch64, every instruction is a 32-bit number. Instructions encode what operation the machine should run (add, move, subtract, jump, etc.) and accept some arguments for what data to operate on. These arguments might be constants (meaning like the number 2; these constants are often called \"immediates\"), but they can also be registers or a memory address. For now, just think of a register as a variable and memory as a list.\nArm Instructions\nHere is an example of the instruction\nadd immediate\n.\n31\n30\n29\n28\n27\n26\n25\n24\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nsf\n0\n0\n1\n0\n0\n0\n1\n0\nsh\nimm12\nRn\nRd\nNow this might look a bit confusing, but once you've seen these tables long enough, they start to be fairly straightforward. Each column in this table represents a single bit in a 32-bit number. If the value is a 0 or 1, that just means it is already filled in. If it has a label, it is a variable that needs to be filled in.\nsf\ntells us whether the registers we are going to use are 64-bit or 32-bit registers.\nsh\nstands for shift.\nsh\ngoes in conjunction with imm12, which stands for a 12-bit immediate (constant). So if we want to add\n42\nto something, we would put\n000000101010\nin for\nimm12\nand set sh to 0 (meaning we aren't shifting the number). But what if we want to represent a number larger than 12 bits? Well, the add instruction doesn't let us represent all such numbers; but setting sh to 1 lets us shift our number by 12 bits. So for example we can represent\n172032172032\nby leaving our 42 alone and setting sh to 1. This is a clever technique for encoding larger numbers in a small space. Variables that start with R are registers, in this case, Rn is our argument to add, and Rd is our destination.\nSo the above instruction can be thought of like this:\nstruct\nAdd\n{\nis_sixty_four_bit\n:\nboolean\n,\nshift\n:\nboolean\n,\nimmediate\n:\nu12\n,\nn\n:\nRegister\n,\ndestination\n:\nRegister\n,\n}\nOur add instruction is really just a data structure where we put the right parts in the right places.\nRegisters\nRegisters are small places to store values. Every instruction set will have a different number of these registers, different sizes of registers, different kinds of registers, and different naming conventions for registers. For AArch64, there are 31 general-purpose registers numbered X0 through X30 for 64-bit registers. Let's say we want to add 42 to register X0 and store the result in X1; we use this binary number.\nsf\noperation\nsh\nimm12\nRn\nRd\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nTo encode our registers into our instruction, we just use their number. So register X0 would be 00000 and register X18 would be\n10010\n. Registers are simply places where we can store values. But by convention, registers can be used for different things. These are called calling conventions and they are how \"higher\" level languages like C encode function calls.\nWriting out all these binary numbers all the time (or even converting them to hex) can often be tedious. So instead, we usually talk about instructions in a simple text format called assembly.\nadd x1, x0, #0x2a\nIn order to feel cool, people usually write numbers in assembly as hex values. This is just the number 42. You can see that assembly hides some of the details of the encoding we just made. We don't think about sf, sh, what size our number is, that a register is Rn vs Rd. Instead, the destination comes first and the arguments after. Because of this lack of detail, a single assembly instruction\nadd\nmight actually map to many different machine code instructions depending on its arguments.\nMemory\nThe last piece we have to understand for machine code is memory. To understand what is going on with memory, we will look at an instruction that lets us store things in memory. This instruction is called\nSTR\nor not written in shorthand, store.\n31\n30\n29\n28\n27\n26\n25\n24\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\nx\n1\n1\n1\n0\n0\n1\n0\n0\nimm12\nRn\nRt\nUsing this instruction, we are going to store some value (RT) into the address (RN) + some offset (imm12). So if we think about memory as a big array, this instruction is like writing into that array.\narray[offset] = value\n. The x here is like our sf before, it controls whether we are using 64-bit values or not. If we want to make this concrete, let's say we have a value in X2, we have an address of memory in X1 and we want to store a value 2 bytes offset from that. We would get this structure:\nx\noperation\nimm12\nRn\nRt\n1\n1\n1\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\nSince writing that all is tedious, we often just write the assembly notation. We are storing the value in x2 based on the address stored in x1 + 2.\nstr x2, [x1, #0x2]\nX86-64\nX86 encoding is a bit different, but it more or less has the same parts. We are still working with instructions, registers, and memory. Some names are a bit different. Instead of the consistent 0-30 naming, we get the historical baggage of the following 64-bit registers: rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8-r15). However, the biggest difference is that x86 is not a fixed width instruction set. We can't simply give a nice little diagram of every instruction using 32 bits. Instead, instructions are assembled from parts. These parts are given different names; when you see an instruction encoding, it tells you how to put the parts together.\nREX\n7\n6\n5\n4\n3\n2\n1\n0\n0\n1\n0\n0\nW\nR\nX\nB\nThe first part is called the REX. This is a prefix that we can use to help us with 64-bit operations. Not sure if there is an official justification for the name REX, but my understanding is that it is the \"Register Extension Prefix\". Unfortunately, because the REX is a prefix, it will only make sense when we see what comes later. REX is there for backward compatibility. The W in REX lets us signal that we are using 64-bit or not for certain operations. The R and B will \"extend\" our registers in certain operations.  In other words, it allows more registers than you used to be able to (These are those r8-r15 registers with a different naming convention than the older registers). We need these because, before 64-bit x86, we had fewer registers and our instructions only had 3 bits per register. With 16 registers, we need an extra bit. (X is for the SIB structure, which we don't cover here).\nModR/M\n7\n6\n5\n4\n3\n2\n1\n0\nmod\nreg\nrm\nOur next part is ModR/M. ModR/M keeps up with the tradition of naming things incredibly short and confusing names.\nmod\nactually means Mode.\nmod\ntells us if\nrm\nis acting as a register or if it is a pointer to memory. If\nmod == 11\nthen rm is being used as a register, otherwise, it is being used as a pointer.\nreg\njust is a register.\nOpCode\nOpCode\nis simple, it is a number. It can be 1-3 bytes long.\nPutting It Together\nThere are other parts, but we won't cover them here. With just these parts, we can build up an instruction. Let's say we want to move a 32-bit signed immediate to a 64-bit register. We can consult\na table of instruction encodings\nand we will get this:\nREX.W + C7 /0 id\nSo now we can assemble our parts and make our instruction. Let's start with REX.W. This notation just means REX with W set to 1. Then there’s B8, which is just a number written in hex.\n/0\nis yet more shorthand for using the ModR/M but setting the reg to 0. Finally,\nid\nmeans \"immediate doubleword\", in other words, a constant number that is 32 bits long. So given all that, we can write our instruction. So let's move the number 42 to the rbx register.\nByte Index\nBits\nDescription\nByte 0\n55–48\n01001000      REX.W = 1\nByte 1\n47–40\n11000111      Opcode C7\nByte 2\n39–32\n11000011      ModR/M: reg=000, r/m=011 (RBX)\nByte 3\n31–24\n00101010      42\nByte 4\n23–16\n00000000      the rest of 42\nByte 5\n15–8\n00000000      ...\nByte 6\n7–0\n00000000      ...\nWhy is RBX 011? Well, because\nthe table\nsays so. Yeah, I did say that x86 is a bit weird.\nThe Rest of It\nI won't pretend that this is all you need. But I will say that starting here can get you further than you think. There are some other things to learn, like various flags for things like overflow, there’s also calling conventions, which are about which registers you use when for things like function calls. We haven't really talked about the stack here, but that's memory that you write to to keep track of things. Nor have we talked about jumps, or how to encode larger immediates in ARM, but you’ve gotten the basics. It’s easier than you would think to hop on\ncompiler explorer\nand learn how things are done.\nLearning machine code and writing things at this low level has unlocked so many things that were mental blocks for me before. Relying on libraries made by others to do these low-level things always left a gap in my knowledge that made me doubt my understanding. Even if I intellectually could explain things, actually doing them has made a huge difference for me. So if you, like me, find low-level things intimidating, I can't recommend enough starting from scratch, at the lowest possible level for your task. What I've found over and over again with low-level details, their not hard, their just poorly documented and poorly explained."
  },
  {
    "title": "Ask HN: Has anybody built search on top of Anna's Archive?",
    "href": "https://news.ycombinator.com/item?id=44176514",
    "content": "Hacker News\nnew\n|\npast\n|\ncomments\n|\nask\n|\nshow\n|\njobs\n|\nsubmit\nlogin\nAsk HN: Has anybody built search on top of Anna's Archive?\n239 points\nby\nneonate\n16 hours ago\n|\nhide\n|\npast\n|\nfavorite\n|\n107 comments\nWouldn't this basically give us Google Books and searchable Scihub at the same time?\nWhat would it cost?\nbhaney\n15 hours ago\n|\nnext\n[–]\nHonestly I don't think it would be that costly, but it would take a pretty long time to put together. I have a (few years old) copy of Library Genesis converted to plaintext and it's around 1TB. I think libgen proper was 50-100TB at the time, so we can probably assume that AA (~1PB) would be around 10-20TB when converted to plaintext. You'd probably spend several weeks torrenting a chunk of the archive, converting everything in it to plaintext, deleting the originals, then repeating with a new chunk until you have plaintext versions of everything in the archive. Then indexing all that for full text search would take even more storage and even more time, but still perfectly doable on commodity hardware.\nThe main barriers are going to be reliably extracting plaintext from the myriad of formats in the archive, cleaning up the data, and selecting a decent full text search database (god help you if you pick wrong and decide you want to switch and re-index everything later).\nreply\nserial_dev\n10 hours ago\n|\nparent\n|\nnext\n[–]\nThe main barriers for me would be:\n1. Why? Who would use that? What’s the problem with the other search engines? How will it be paid for?\n2. Potential legal issues.\nThe technical barriers are at least challenging and interesting.\nProviding a service with significant upfront investment needs with no product or service vision that I’ll likely to be sued for a couple of times a year, probably losing with who knows what kind of punishment… I’ll have to pass unfortunately.\nreply\n1vuio0pswjnm7\n40 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nBut he did not mention anything about creating a \"service\"\nIt could be his own copy for personal use\nWhat if computers continue to become faster and storage continues to become cheaper; what if \"large\" amounts data continue to become more manageable\nThe data might seem large today, but it might not seem large or unmanageable in the future\nreply\nnamlem\n10 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nIt would be incredible for LLMs. Searching it, using it as training data, etc. Would probably have to be done in Russia or some other country that doesn't respect international copyright though.\nreply\njxjnskkzxxhx\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nDo you have a reason to believe this ain't already being done? I would assume that the big guys like openai are already training on basically all text in existence.\nreply\nIlikeKitties\n9 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIn fact, facebook torrented annas archive and got busted for it, because of course they did:\nhttps://torrentfreak.com/meta-torrented-over-81-tb-of-data-t...\nreply\nHDThoreaun\n5 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nEvery LLM maker probably did the same. Facebook just has disgruntled employees who leaked it\nreply\ngpm\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nGoogle goes around legally scanning every book they can get their hands on with books.google.com. Legally scanning every paper they can get their hands on with scholar.google.com.\nI doubt they'd resort to piracy for what is basically the same information as what they've already legally acquired...\nreply\nlcnPylGDnU4H9OF\n55 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThat is a good reason to think they did not but it doesn't necessarily override reasons for them to do so. Perhaps it's dubious that the subset of data they could not legally get their hands on is an advantage for training but I really don't know, and maybe nobody does. Given that, Google's execs may have been in favor of similar operations as Facebook's and their lawyers may have been willing to approve them with similar justifications.\nreply\nar_lan\n1 hour ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nWasn't this confirmed what Meta does?\nhttps://www.forbes.com/sites/danpontefract/2025/03/25/author...\nreply\nexecutesorder66\n9 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> or some other country that doesn't respect international copyright though.\nLike the US? OpenAI et al. don't give a shit.\nreply\nTeMPOraL\n5 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThere's a difference between feeding massive amounts of copyrighted material to a training process that blends them thoroughly and irreversibly, and doing all that in-house, vs. offering people a service that indexes (and possibly partially rehosts) that material, enabling and encouraging users to engage directly in pirating concrete copyrighted works.\nreply\nr14c\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThat's Uber's Gambit. Nothing is illegal for large enough corporations with strong network effects and deep pockets.\nreply\ngosub100\n1 hour ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> that blends them thoroughly and irreversibly\nIt's okay, you can say 'laundering'\nreply\nfreedomben\n3 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> > or some other country that doesn't respect international copyright though.\n> Like the US? OpenAI et al. don't give a shit.\nOpenAI is not a country and therefore cannot make laws that don't respect international (or domestic) copyright.  Also the US is a lot bigger than OpenAI and the big tech corps, and the\nlaw\nis very much on the side of copyright holders in the US.\nreply\ndiggan\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> the law is very much on the side of copyright holders in the US.\nRemind me again what the status of the case is with Meta/Facebook using pirated material to train their proprietary LLMs, and even seeding the data back to the community while downloading it?\nreply\ngosub100\n1 hour ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nThe money is definitely in the side of big tech vs book publishers. There may be a nominal settlement to end the matter, perhaps after a decade of litigation\nreply\nandrepd\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> Would probably have to be done in Russia or some other country that doesn't respect international copyright though.\nIncredible, several years of major American AI companies showing that flaunting copyright only matters if it's college kids torrenting shows or enthusiasts archiving bootlegs on whatcd, but if it's big corpos doing it it's necessary for innovation.\nYet some people still believe \"it would have to be done in evil Russia\".\nreply\nDataDaoDe\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nOP does have an exaggerated statement - its not like there aren't laws in Russia or something and I largely agree with your sentiment. I think there are levels to this though and its pretty clear that Russia is much riskier than the USA when it comes to IP - just look up anything to do with insuring IP risk in Russia (here's one such example:\nhttps://baa.no/en/articles/i-have-ip-in-russia-is-my-ip-at-r...\n)\nAlso according to the office of US trade representative, Russia is on the priority watch list of countries that do not respect IP [1] and post 2022, largely due to the war, Russia implemented measures negatively effecting IP rights. [2,3]\nIf you think it isn't the case and Russia is just as risky as the US when it comes to copyright and IP, I would be interested to know why.\n1.\nhttps://ustr.gov/about/policy-offices/press-office/press-rel...\n2.\nhttps://www.papula-nevinpat.com/executive-summary-the-ip-sit...\n3.\nhttps://www.taftlaw.com/news-events/law-bulletins/russia-iss...\nreply\nmdp2021\n2 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n>\nevil\nIn this case and context, a label like \"evil\" is a twisted interpretation.\nreply\nsam_lowry_\n10 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nLLMs already use it, dude )\nreply\nexe34\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI think one use would be to search for information directly from a book, rather than get a garbled/half-hallucinated version of it.\nreply\nechollama\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\ngarbled/half-hallucinated is probably what you would've gotten 8-12mo ago but now adays im sure with good prompting you can pull value from any book.\nreply\njdironman\n5 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nYou don't need AI for that. I get the optimistic spirit of what you mean though.\nreply\nmdp2021\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nOptimized information retrieval of complex text\nis\nAI.\nreply\nbbor\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n1. It'd be for the scientific community (broadly-construed). Converting media that is currently completely un-indexed into plaintext and offering a suite of search features for finding content within it would be a game-changer, IMO! If you've ever done a lit review for any field other than ML, I'm guessing you know how reliant many fields are on relatively-old books and articles (read: PDFs at best, paper-only at worst) that you can basically only encounter via a) citation chains, b) following an author, or c) encyclopedias/textbooks.\n2. I really don't see how this could ever lead to any kind of legal issue. You're not hosting any of the content itself, just offering a search feature for it. GoodReads doesn't need legal permission to index popular books, for example.\nIn general I get the sense that your comment is written from the perspective of an entrepreneur/startup mindset. I'm sure that's brought you meaning and maybe even some wealth, but it's not a universal one! Some of us are more interested in making something to advance humanity than something likely to make a profit, even if we might look silly in the process.\nreply\nAachen\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> I really don't see how this could ever lead to any kind of legal issue. You're not hosting any of the content itself, just offering a search feature for it.\nYou don't need to host copyrighted material. It's all about\nintent\n. The Pirate Bay is (imo correctly, even if I disagree with other aspects about copyright law and its enforcement) seen as a place where people go to find ways to not pay authors for their content. They never hosted a copyrighted byte but they're banned in some form (DNS, IP, domain seizures) in many countries. Proxies of TPB also, so being like an ISP for such a site is already enough, whereas nobody is ordering blocks of Comcast's IP addresses for providing access to websites with copyrighted material because they didn't have a somewhat-provable intent to provide copyright infringement\nWhen I read the OP, I imagine this would link from the search results directly to Anna's archive and sci-hub, but I think you'd have to spin it as a general purpose search page and ideally not even mention AA was one of the sources, much less have links\n(Don't get me wrong: everyone wants this except the lobby of journals that presently own the rights)\nIt would be a real shame if an anonymous third party that's definitely not the website operator made a Firefox add-on that illegitimately inserts these links to search results page though\nreply\nDaSHacka\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> When I read the OP, I imagine this would link from the search results directly to Anna's archive and sci-hub\nYou could just give users ISBNs or link to the book's metadata on openlibrary[0], both of which AA's native search already does.\n[0]\nhttps://openlibrary.org/\nreply\ncarlosjobim\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> 1. Why? Who would use that?\nRather who would use a traditional search engine instead of a book search engine, when the quality of the results from the latter will be much superior?\nPeople who need or want the highest quality information available will pay for it. I'd easily pay for it.\nreply\nnotpushkin\n12 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nI think there’s a couple ways to improve it:\n1. There’s a lot of variants of the same book. We only need one for the index. Perhaps for each ISBN, select the format easiest to parse.\n2. We can download, convert and index top 100K books first, launch with these, and then continue indexing and adding other books.\nreply\nthrowup238\n8 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nHow are you going to download the top 100k? The only reasonable way to download that many books from AA or Libgen is to use the torrents, which are sorted sequentially by upload date.\nI tried to automate downloading just a thousand books and it was unbearably slow, from IPFS or the mirrors both. I ended up picking the individual files out of the torrents. Even just identifying or deduping the top 100k would be a significant task.\nreply\nWillAdams\n7 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nThe thing is, for an ISBN, that is one edition, by one publisher and one can easily have the same text under 3 different ISBNs from one publisher (hardcover, trade paperback, mass-market paperback).\nI count 80+ editions of J.R.R. Tolkien's _The Hobbit_ at:\nhttps://tolkienlibrary.com/booksbytolkien/hobbit/editions.ph...\ngranted some predate ISBNs, one is the 3D pop-up version, so not a traditional text, and so forth, but filtering by ISBN will _not_ filter out duplicates.\nThere is also the problem of the same work being published under multiple titles (and also ISBNs) --- Hal Clement's _Small Changes_ was re-published as _Space Lash_ and that short story collection is now collected in:\nhttps://www.goodreads.com/book/show/939760.Music_of_Many_Sph...\nalong with others.\nreply\npalmfacehn\n11 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nThere should be a way to leverage compression when storing multiple editions of the same book.\nreply\nbawolff\n9 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nFrom a good search perspective though you probably dont want 500 different versions of the same book popping up for a query\nreply\npalmfacehn\n6 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nAgreed. I would prefer to see a single result for a single title. The option of pursuing different editions should follow from there.\nreply\ntomthe\n10 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nI wonder if you could implement it with only static hosting?\nWe would need to split the index into a lot of smaller files that can be practically downloaded by browsers, maybe 20 MB each.\nThe user types in a search query, the browser hashes the query and downloads the corresponding index file which contains only results for that hashed query. Then the browser sifts quickly through that file and gives you the result.\nHosting this would be cheap, but the main barriers remain..\nreply\nThatPlayer\n8 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI've done something similar with a static hosted site I'm working on. I opted to not reinvent the wheel, and just use WASM Sqlite in the browser. Sqlite already splits the database into fixed-size pages, so the driver using HTTP Range Requests can download only the required pages. Just have to make good indexes.\nI can even use Sqlite's full-text search capabilities!\nreply\nAachen\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI wonder if you could take this one step further and have opaque queries using homomorphic encryption on the index and then somehow extracting ranges around the document(s) you're interested in\nInspired by: \"Show HN: Read Wikipedia privately using homomorphic encryption\"\nhttps://news.ycombinator.com/item?id=31668814\nreply\nshowerst\n5 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nHow would that scale to 10TB+ of plain text though? Presumably the indexes would be many gigabytes, especially with full text search.\nreply\nqcic\n3 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nSuper interesting.\nreply\ngreggsy\n10 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nIt's trivial to normalise the various formats, and there were a few libraries and ML models to help parse PDFs. I was tinkering around with something like this for academic papers in Zotero, and the main issue I ran into was words spilling over to the next page, and footnotes. I totally gave up on that endeavour several years ago, but the tooling has probably matured exponentially since then.\nAs an example, all the academic paper hubs have been using this technology for decades.\nI'd wager that\nall\nof the big Gen AI companies have planned to use this exact dataset, and many or them probably have already.\nreply\nfake-name\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> It's trivial to normalise the various formats,\nHa. Ha. ha ha ha.\nAs someone who as pretty broadly tried to normalize a pile of books and documents I have legitimate access to,\nno it is not\n.\nYou can get good results 80% of the time, usable but messy results 18% of the time, and complete garbage the remaining 2%. More effort seems to only result in marginal improvements.\nreply\nbawolff\n9 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n98% sounds good enough for the usecase suggested here.\nreply\npastage\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nWriting good validators for data is hard. You can be 100% sure that there will be bad data in those 98%. From my own experience I thought I had 50% of the books converted correctly and then I found I still had junk data and gave up, it is not an impossible problem I just was not motivated to fix it on my own. Working with your own copies is fine, but when you try to share that you get into legal issues that I just do not feel are that interesting to solve.\nEdit: my point is that I would like to share my work but that is hard to do in a legal way. That is the main reason I gave up.\nreply\nlandl0rd\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n2% garbage, if some of that garbage falls out the right way, is more than enough to seriously degrade search result quality.\nreply\ncarlosjobim\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIt's better than nothing, and nothing is what we currently have.\nreply\ntrollbridge\n4 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nDecent storage is $10/TB, so for $10,000 you could just keep the entire 1PB of data.\nA rather obvious question is if someone has trained an LLM on this archive yet.\nreply\nmoffkalast\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nA rather obvious answer is Meta is currently being sued for training Llama on Anna's archive.\nYou can be practically certain that every notable LLM has been trained on it.\nreply\nrthnbgrredf\n19 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> You can be practically certain that every notable LLM has been trained on it.\nBut only Meta was kind of not so smart to publicly admit it.\nreply\nnet01\n7 hours ago\n|\nprev\n|\nnext\n[–]\nThey did! They conducted a competition\nhttps://annas-archive.org/blog/all-isbns-winners.html\n, in which a few submissions exceeded the minimum requirements and implemented a good search tool & visualiser.\nreply\ncarlosjobim\n4 hours ago\n|\nparent\n|\nnext\n[–]\nHow is this a text search of the books?\nreply\noutside1234\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThe original question the poster made was not clear, so this is also an answer to it.  It depends on what they meant by \"search\"\nreply\nlaserstrahl\n10 hours ago\n|\nprev\n|\nnext\n[–]\nThere’s an android app called OpenLip. [1]\nDescription:\nOpenlib is an open source app to download and read books from shadow library (Anna’s Archive). The App Has Built In Reader to Read Books.\nAs Anna’s Archive doesn't have an API, the app works by sending requests to Anna’s Archive and parses the response to objects. The app extracts the mirrors from the responses, downloads the book and stores it in the application's document directory.\nNote :\nThe app requires VPN to function properly . Without VPN the might show the captcha required page even after completing the captcha\nMain Features:\nTrending Books\nDownload And Read Books With In-Built Viewer\nSupports Epub And Pdf Formats\nOpen Books With Your Favourite Ebooks Reader\nFilter Books\nSort Books\n[1]:\nhttps://f-droid.org/de/packages/com.app.openlib/\nreply\nggm\n16 hours ago\n|\nprev\n|\nnext\n[–]\nYou must mean free text search and page level return, because it already has full metadata indexing.\nThe thing is AA doesn't hold the texts. They're disputable IPR and even a derived work would be a legal target.\nreply\ncarlosjobim\n4 hours ago\n|\nparent\n|\nnext\n[–]\n> a derived work would be a legal target.\nWhy would it? Google isn't prosecuted for indexing the web.\nreply\n1970-01-01\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nOh it certainly is.\nhttps://www.reuters.com/sustainability/boards-policy-regulat...\nreply\ntrollbridge\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThat's not prosecution for indexing the web. Google is being treated the same way AT&T was for telephones.\nreply\n1970-01-01\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nhttps://harvardlawreview.org/print/vol-138/united-states-v-g...\nreply\npetra\n10 hours ago\n|\nprev\n|\nnext\n[–]\nZ-Library has a keyword search. Personally i didn't find it too useful, especially given Google Books exists. It's not easy to create a quality book search engine.\nreply\nimdavidsantiago\n12 hours ago\n|\nprev\n|\nnext\n[–]\nAs far as I know, no one has fully implemented full-text search directly over Anna's Archive. Technically it’s feasible with tools like Meilisearch, Elasticsearch, or Lucene, but the main challenges are:\nConverting all documents (PDFs, EPUBs, etc.) to clean plaintext.\n\n    Indexing at scale efficiently.\n\n    Managing potential legal issues.\nZ-Library does something similar, but it’s smaller in scope and doesn't integrate AA’s full catalog.\nreply\nbendangelo\n12 hours ago\n|\nparent\n|\nnext\n[–]\nI’ve done something like this before. Meilisearch will not be viable, because it indexes very slow and it takes up a lot of space.\nIn my experience only Tantivy can index this much data. Check out Lnx.\nreply\nsam_lowry_\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nLucene would fo fine as well, I guess. As much as I like the author of Tantivy, it is a toy compared to Lucene.\nreply\n_ache_\n6 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nTo manage the legal issues, you just have to put AI on the search. \"AI search\".\nreply\nwhimsicalism\n1 hour ago\n|\nprev\n|\nnext\n[–]\nsmall number of people willing to put in significant engineering hours for something that would be illegal and non-monetizable\nreply\nsimgt\n11 hours ago\n|\nprev\n|\nnext\n[–]\nRelated question, has Anna's archive been thoroughly filtered for non-copyright-related illegal material? Pedo, terrorism, etc. I've considered downloading a few chunks of it but I'm worried of ending up with content I really don't want to be anywhere near from.\nreply\nbilekas\n10 hours ago\n|\nparent\n|\nnext\n[–]\nThis is a really strange question to be honest you could ask this literally about any download let alone simply torrents of documents.\nreply\ngosub100\n40 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIt's the textbook example of the \"chilling effect\" created by mass surveillance.\nreply\ndns_snek\n7 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nDownload everything, we know that laws don't apply when you do it on a large enough scale. Not legal advice.\nreply\nlukan\n35 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI think you got that wrong. Laws only don't apply if\nyou\nare large enough.\n(Like Meta)\nreply\ngosub100\n42 minutes ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nThe team that curates it is very dedicated and wouldn't do such a thing. The least of reasons being they don't want the heat from it.\nI'm not sure what other forms of information is illegal beyond CP. In the US, bomb making instructions are not illegal. In other dictatorships or zealous religious regimes, information about democracy or works that insult Islam might be illegal\nreply\nniux\n10 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nHow might you inadvertently download illegal content while searching for legal content?\nreply\nlukan\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nHe said he wants to download lots of it in general, not specifical. Legit question, if you end up with dark material.\nI would assume pedo stuff is not really there, but the anarchist cookbook and alike likely will be.\nreply\nbilekas\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI'm still not sure the question makes much sense, if it's a general: \"I want to support the project and so I want to seed a large chunk\" Okay, I guess it's your due diligence to check, but there is a reporting feature built in, if something is found, report it.\nAside from that, if you're searching for specific content, the question is moot I guess.\nI guess my confusion is what distinguishes this apart from any other torrent ? That is, if the submitted content is submitted at all.\nreply\nlukan\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI understood it as he or she wants to download large chunks of potentially interesting books for offline use, or once Anna goes down. So a broad filter. Not for seeding.\nBut thanks for the explanation that there is a report build in.\nreply\nDocTomoe\n10 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nConsidering the anarchist cookbook is just a rebranded selection of freely-available US Army Field Manuals, ... I don't see the problem.\nreply\nlukan\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI don't either, but many states have laws regarding books on how to build bombs and they might get enforced more than copyright.\nreply\nsrum\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYou can get in trouble for having it in the UK (though not necessarily convicted)\nhttps://news.sky.com/story/anarchist-cookbook-case-student-j...\nhttps://www.bbc.co.uk/news/uk-england-oxfordshire-45841291\nreply\noguz-ismail\n10 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n>I would assume pedo stuff is not really there\nSearch for \"lolicon\"\nreply\nlukan\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nWell, I won't. But does it contain just text or real pictures? That would make a big legal difference I assume.\nreply\njxjnskkzxxhx\n10 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nI thought that was anime pictures...?\nreply\nareyourllySorry\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\na subset of that, yes. but that label implies more than just that\nreply\nbordercases\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nSeeding torrrent blocks.\nreply\nhbartab\n3 hours ago\n|\nprev\n|\nnext\n[–]\nSeeing as OpenAI & Co were trained on torrented books from similar places, I'm sure that ChatGPT provides an adequate search layer on top of Anna's Archive, though it is not as free from confabulations as one might hope for in a search engine.\nEdit: grammar\nreply\nnextos\n14 hours ago\n|\nprev\n|\nnext\n[–]\nAFAIK, Z-Library already does this, to some extent. Basic full-text queries do search inside the body of books and articles.\nIt's a bit smaller than Anna's Archive, as they do host their own collections. From some locations, it's only easy to access through Tor.\nreply\nteekert\n8 hours ago\n|\nprev\n|\nnext\n[–]\nProbably this was already done at Google, Meta, X\nand\nOpenAI, before training their LLMs.\nreply\nmaartin0\n3 hours ago\n|\nparent\n|\nnext\n[–]\nThere's actually section in the Wikipedia page that explicitly says DeepSeek was trained on it\nreply\npodgorniy\n9 hours ago\n|\nprev\n|\nnext\n[–]\nThere is a search solution for zipped fb2 files. Not exactly what you need, but it has potential.\nThe project has similar story to Anna's archive. There is 0.5 TB of archived books, and the project creates index of all the books with text, title and aruthor search capabilities, gives html UI for search and reading. On weak machine it takes about 2 hours to build that index.\nSo if you have zipped archives of fb2, you can use the project to create web UI with search for those files. Without need of enough space to unpack all the files.\nYou'll have to translate some russian though to get instructions on how to set it up.\nhttps://gitlab.com/opennota/fb2index/-/blob/master/README.ru...\nreply\ntangus\n9 hours ago\n|\nparent\n|\nnext\n[–]\nBut fb2 files are marked up text, which is (relatively) trivial to index. The bulk of Anna's Archive's books are made of from scanned images.\nreply\nallenleein\n10 hours ago\n|\nprev\n|\nnext\n[–]\nHas anyone explored a different angle — like mapping out the 1,000 most frequently mentioned or cited books (across HN, Substack, Twitter, etc.), then turning their raw content into clean, structured data optimized for LLMs? Imagine curating these into thematic shelves — say, “Bill Gates’ Bookshelf” or “HN Canon” — and building an indie portal where anyone can semantically search across these high-signal texts. Kind of like an AI-searchable personal library of the internet’s favorite books.\nreply\nDocTomoe\n10 hours ago\n|\nparent\n|\nnext\n[–]\nWell, there's this:\nhttps://hacker-recommended-books.vercel.app/category/0/all-t...\nreply\nunderlines\n3 hours ago\n|\nprev\n|\nnext\n[–]\nyes, every major llm company did it:\nillegally using annas archive, the pile, common crawl, their own crawl, books2, libgen etc. and embed it into high dimensional space and do next token prediction on it.\nreply\nrenegat0x0\n12 hours ago\n|\nprev\n|\nnext\n[–]\nI have found some searche engines, but I do not think they're for Anna's.\nhttps://searchthearxiv.com/\nhttps://refseek.com/\nhttps://arxivxplorer.com/\nreply\nxbmcuser\n9 hours ago\n|\nprev\n|\nnext\n[–]\nFacebook did it's ai is trained on it so you can use that.\nreply\ncarlosjobim\n4 hours ago\n|\nprev\n|\nnext\n[–]\nA functional full text search of the shadow libraries would be massive. It would have a comparable impact on humanity to the impact AI will have. And it's probably not difficult technically. Let's start a project to get this done!\nEdit: I have had this exact project as my dream for a couple of years, and even experimented a little bit. But I'm not a programmer, so I can only understand theoretically what would be needed for this to work.\nAnybody with the same dream, send me an e-mail to booksearch@fastmail.com and let's see what we can do to get the ball rolling!\nreply\nbravesoul2\n13 hours ago\n|\nprev\n|\nnext\n[–]\nThis works in various search engines\nsite:annas-archive.org avacado\nreply\nqingcharles\n10 hours ago\n|\nparent\n|\nnext\n[–]\nIt's not exactly clear, but OP is asking about indexing the content of all the documents, not the metadata (e.g. titles etc)\nreply\nQuin-tus\n9 hours ago\n|\nprev\n|\nnext\n[–]\nhttps://book-finder.tiiny.site/\nMore:\nhttps://rentry.co/StellaOctangulaIsCool\nreply\nHelloUsername\n9 hours ago\n|\nparent\n|\nnext\n[–]\n>\nhttps://book-finder.tiiny.site/\nThat just redirects to\nhttps://yandex.com/search\nreply\nQuin-tus\n40 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nWell yeah but with a specific query with which you can search multiple libraries\nreply\nDaSexiestAlive\n12 hours ago\n|\nprev\n|\nnext\n[–]\nMebbe easier to just search Amazon or Goodreads. Like site:amazon.ca <query words> as someone has mentioned below.\nEvery book has an ISBN 10 or 13 digit ISBN number to identify them. Unless it's some self-pub/amateur-hour situation by some paranoid prepper living in a faraday-cage-protected cage in Arkansas or Florida it's likely a publication with a title, an author and an ISBN number.\nreply\ntrollbridge\n4 hours ago\n|\nparent\n|\nnext\n[–]\nA  self-pub amateur-hour book printed by a paranoid prepper living in a faraday cage is exactly the type of book I'd probably enjoy reading, but I doubt these exist anymore.\nreply\npigeons\n11 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nWhat about pre-1970 books?\nreply\n1970-01-01\n4 hours ago\n|\nprev\n[–]\nDon't do it. Just because you can, doesn't mean you should. Do you know if they have anywhere near the legal muscle to push back the flood of legal notices if you did this? Assume it survives because it doesn't have a wide open barn door to the public.\nreply\nbethekidyouwant\n4 hours ago\n|\nparent\n[–]\nIt wouldn’t be called full text search of AA, It would be called full tech search of every book in the world.\nreply\n1970-01-01\n4 hours ago\n|\nroot\n|\nparent\n[–]\nYou are asking a judge to consider that a book is ok to scrape because it's part of a much larger collection of books, perhaps the biggest and best collection, and therefore it's all OK because at scale means good.\nreply\ncalibas\n3 hours ago\n|\nroot\n|\nparent\n[–]\nGoogle already successfully argued in court that creating an online search index of books constitutes fair use:\nhttps://en.wikipedia.org/wiki/Authors_Guild,_Inc._v._Google,...\n.\nreply\n1970-01-01\n2 hours ago\n|\nroot\n|\nparent\n[–]\nThe index isn't the full content. The OP search is about indexing the entire contents of the book for verbatim information retrieval.\nreply\nGuidelines\n|\nFAQ\n|\nLists\n|\nAPI\n|\nSecurity\n|\nLegal\n|\nApply to YC\n|\nContact\nSearch:"
  },
  {
    "title": "The Prompt Engineering Playbook for Programmers",
    "href": "https://addyo.substack.com/p/the-prompt-engineering-playbook-for",
    "content": "Subscribe\nSign in\nShare this post\nElevate\nThe Prompt Engineering Playbook for Programmers\nCopy link\nFacebook\nEmail\nNotes\nMore\nThe Prompt Engineering Playbook for Programmers\nTurn AI coding assistants into more reliable development partners\nAddy Osmani\nMay 27, 2025\n297\nShare this post\nElevate\nThe Prompt Engineering Playbook for Programmers\nCopy link\nFacebook\nEmail\nNotes\nMore\n1\n30\nShare\nDevelopers are increasingly relying on AI coding assistants to accelerate our daily workflows. These tools can autocomplete functions, suggest bug fixes, and even generate entire modules or MVPs. Yet, as many of us have learned, the\nquality\nof the AI’s output depends largely on the\nquality of the prompt\nyou provide. In other words,\nprompt engineering\nhas become an essential skill. A poorly phrased request can yield irrelevant or generic answers, while a well-crafted prompt can produce thoughtful, accurate, and even creative code solutions. This write-up takes a practical look at how to systematically craft effective prompts for common development tasks.\nAI pair programmers are powerful but not magical – they have no prior knowledge of your specific project or intent beyond what you tell them or include as context. The more information you provide, the better the output. We’ll distill key prompt patterns,\nrepeatable frameworks\n, and memorable examples that have resonated with developers. You’ll see side-by-side comparisons of\ngood vs. bad prompts\nwith actual AI responses, along with commentary to understand why one succeeds where the other falters.\nHere’s a cheat sheet to get started:\nFoundations of effective code prompting\nPrompting an AI coding tool is somewhat like communicating with a very literal,\nsometimes\nknowledgeable collaborator. To get useful results, you need to set the stage clearly and guide the AI on\nwhat\nyou want and\nhow\nyou want it.\nBelow are foundational principles that underpin all examples in this playbook:\nProvide rich context.\nAlways assume the AI knows nothing about your project beyond what you provide. Include relevant details such as the programming language, framework, and libraries, as well as the specific function or snippet in question. If there’s an error, provide the exact error message and describe what the code is\nsupposed\nto do.\nSpecificity\nand\ncontext\nmake the difference between vague suggestions and precise, actionable solutions . In practice, this means your prompt might include a brief setup like: “I have a Node.js function using Express and Mongoose that should fetch a user by ID, but it throws a TypeError. Here’s the code and error…”. The more setup you give, the less the AI has to guess.\nBe specific about your goal or question.\nVague queries lead to vague answers. Instead of asking something like “Why isn’t my code working?”, pinpoint what insight you need. For example: “This JavaScript function is returning undefined instead of the expected result. Given the code below, can you help identify why and how to fix it?” is far more likely to yield a helpful answer. One prompt formula for debugging is:\n“It’s expected to do [expected behavior] but instead it’s doing [current behavior] when given [example input]. Where is the bug?”\n. Similarly, if you want an optimization, ask for a\nspecific kind\nof optimization (e.g.\n“How can I improve the runtime performance of this sorting function for 10k items?”\n). Specificity guides the AI’s focus .\nBreak down complex tasks.\nWhen implementing a new feature or tackling a multi-step problem, don’t feed the entire problem in one gigantic prompt. It’s often more effective to split the work into smaller chunks and iterate. For instance,\n“First, generate a React component skeleton for a product list page. Next, we’ll add state management. Then, we’ll integrate the API call.”\nEach prompt builds on the previous. It’s often not advised to ask for a whole large feature in one go; instead, start with a high-level goal and then iteratively ask for each piece . This approach not only keeps the AI’s responses focused and manageable, but also mirrors how a human would incrementally build a solution.\nInclude examples of inputs/outputs or expected behavior.\nIf you can illustrate what you want with an example, do it. For example,\n“Given the array [3,1,4], this function should return [1,3,4].”\nProviding a concrete example in the prompt helps the AI understand your intent and reduces ambiguity . It’s akin to giving a junior developer a quick test case – it clarifies the requirements. In prompt engineering terms, this is sometimes called “\nfew-shot prompting\n,” where you show the AI a pattern to follow. Even one example of correct behavior can guide the model’s response significantly.\nLeverage roles or personas.\nA powerful technique popularized in many viral prompt examples is to ask the AI to “act as” a certain persona or role. This can influence the style and depth of the answer. For instance,\n“Act as a senior React developer and review my code for potential bugs”\nor\n“You are a JavaScript performance expert. Optimize the following function.”\nBy setting a role, you prime the assistant to adopt the relevant tone – whether it’s being a strict code reviewer, a helpful teacher for a junior dev, or a security analyst looking for vulnerabilities. Community-shared prompts have shown success with this method, such as\n“Act as a JavaScript error handler and debug this function for me. The data isn’t rendering properly from the API call.”\n. In our own usage, we must still provide the code and problem details, but the\nrole-play\nprompt can yield more structured and expert-level guidance.\nIterate and refine the conversation.\nPrompt engineering is an\ninteractive\nprocess, not a one-shot deal. Developers often need to review the AI’s first answer and then ask follow-up questions or make corrections. If the solution isn’t quite right, you might say,\n“That solution uses recursion, but I’d prefer an iterative approach – can you try again without recursion?”\nOr,\n“Great, now can you improve the variable names and add comments?”\nThe AI remembers the context in a chat session, so you can progressively steer it to the desired outcome. The key is to view the AI as a partner you can coach –\nprogress over perfection\non the first try .\nMaintain code clarity and consistency.\nThis last principle is a bit indirect but very important for tools that work on your code context. Write clean, well-structured code and comments, even before the AI comes into play. Meaningful function and variable names, consistent formatting, and docstrings not only make your code easier to understand for humans, but also give the AI stronger clues about what you’re doing. If you show a consistent pattern or style, the AI will continue it . Treat these tools as extremely attentive junior developers – they take every cue from your code and comments.\nWith these foundational principles in mind, let’s dive into specific scenarios. We’ll start with\ndebugging\n, perhaps the most immediate use-case: you have code that’s misbehaving, and you want the AI to help figure out why.\nPrompt patterns for debugging code\nDebugging is a natural fit for an AI assistant. It’s like having a rubber-duck that not only listens, but actually talks back with suggestions. However, success largely depends on how you present the problem to the AI. Here’s how to systematically prompt for help in finding and fixing bugs:\n1. Clearly describe the problem and symptoms.\nBegin your prompt by describing what is going wrong and what the code is supposed to do. Always include the exact error message or incorrect behavior. For example, instead of just saying “My code doesn’t work,” you might prompt:\n“I have a function in JavaScript that should calculate the sum of an array of numbers, but it’s returning NaN (Not a Number) instead of the actual sum. Here is the code: [include code]. It should output a number (the sum) for an array of numbers like [1,2,3], but I’m getting NaN. What could be the cause of this bug?”\nThis prompt specifies the language, the intended behavior, the observed wrong output, and provides the code context – all crucial information. Providing a structured context (code + error + expected outcome + what you’ve tried) gives the AI a solid starting point . By contrast, a generic question like “Why isn’t my function working?” yields meager results – the model can only offer the most general guesses without context.\n2. Use a step-by-step or line-by-line approach for tricky bugs.\nFor more complex logic bugs (where no obvious error message is thrown but the output is wrong), you can prompt the AI to walk through the code’s execution. For instance:\n“Walk through this function line by line and track the value of total at each step. It’s not accumulating correctly – where does the logic go wrong?”\nThis is an example of a\nrubber duck debugging prompt\n– you’re essentially asking the AI to simulate the debugging process a human might do with prints or a debugger. Such prompts often reveal subtle issues like variables not resetting or incorrect conditional logic, because the AI will spell out the state at each step. If you suspect a certain part of the code, you can zoom in:\n“Explain what the filter call is doing here, and if it might be excluding more items than it should.”\nEngaging the AI in an explanatory role can surface the bug in the process of explanation.\n3. Provide minimal reproducible examples when possible.\nSometimes your actual codebase is large, but the bug can be demonstrated in a small snippet. If you can extract or simplify the code that still reproduces the issue, do so and feed that to the AI. This not only makes it easier for the AI to focus, but also forces you to clarify the problem (often a useful exercise in itself). For example, if you’re getting a TypeError in a deeply nested function call, try to reproduce it with a few lines that you can share. Aim to isolate the bug with the minimum code, make an assumption about what’s wrong, test it, and iterate . You can involve the AI in this by saying:\n“Here’s a pared-down example that still triggers the error [include snippet]. Why does this error occur?”\nBy simplifying, you remove noise and help the AI pinpoint the issue. (This technique mirrors the advice of many senior engineers: if you can’t immediately find a bug, simplify the problem space. The AI can assist in that analysis if you present a smaller case to it.)\n4. Ask focused questions and follow-ups.\nAfter providing context, it’s often effective to directly ask what you need, for example:\n“What might be causing this issue, and how can I fix it?”\n. This invites the AI to both diagnose and propose a solution. If the AI’s first answer is unclear or partially helpful, don’t hesitate to ask a follow-up. You could say,\n“That explanation makes sense. Can you show me how to fix the code? Please provide the corrected code.”\nIn a chat setting, the AI has the conversation history, so it can directly output the modified code. If you’re using an inline tool like Copilot in VS Code or Cursor without a chat, you might instead write a comment above the code like // BUG: returns NaN, fix this function and see how it autocompletes – but in general, the interactive chat yields more thorough explanations. Another follow-up pattern: if the AI gives a fix but you don’t understand why, ask\n“Can you explain why that change solves the problem?”\nThis way you learn for next time, and you double-check that the AI’s reasoning is sound.\nNow, let’s illustrate these debugging prompt principles with a concrete example, showing a\npoor prompt vs. improved prompt\nand the difference in AI responses:\nDebugging example: poor vs. improved prompt\nImagine we have a simple Node.js function meant to convert a list of user objects to a lookup map by user ID. However, it’s throwing an error. Here’s the buggy code:\n// Buggy function: converts array of users to a map by ID\nfunction mapUsersById(users) {\n  const userMap = {};\n  for (let i = 0; i <= users.length; i++) {  \n    // Note: using <= will go out of bounds on the last iteration\n    const user = users[i];\n    userMap[user.id] = user;\n  }\n  return userMap;\n}\n\n// Example usage:\nconst result = mapUsersById([{ id: 1, name: \"Alice\" }]);\nThis code is supposed to iterate through the users array and build an object where keys are user IDs. However, there is a bug: the loop uses i <= users.length instead of i < users.length. This will cause users[i] to be undefined on the last iteration, likely leading to a runtime error (e.g., attempting to access property id of undefined). Let’s see how an AI assistant might respond to two different prompts about this issue.\n❌ Poor Prompt:\n“Why isn’t my mapUsersById function working?”\nAI’s response to the poor prompt (simulated):\nIt’s hard to say without more information. Are you sure the function is being called with a proper array of users? Perhaps the issue is that users is empty or not an array, causing the loop not to execute properly. It could also be that there’s an error when setting the property on userMap. Make sure that each user has an id property. If users is undefined or null, that would also cause it to not work.\nAnalysis:\nThis answer is quite vague. The prompt gave no details except the function name, so the AI is guessing at generic causes (“maybe the array is empty or not an array”, “make sure each user has an id”). It did not identify the actual bug. This is the kind of response you get when the question is underspecified – the model can only regurgitate common debugging tips. As a developer, this isn’t very helpful because it doesn’t address the real mistake. The\nlack of context\nand specificity in the prompt yields a generic answer (essentially the AI equivalent of “have you tried turning it off and on again?”). This aligns with the community advice that vague prompts lead to irrelevant or generic answers .\n✅ Improved Prompt:\n“I have a JavaScript function mapUsersById that should convert an array of user objects into a map (object) keyed by user ID. However, it throws an error when I run it. For example, when I pass [ {id: 1, name: \"Alice\"} ], I get TypeError: Cannot read property 'id' of undefined. Here is the function code:\nfunction mapUsersById(users) {\n  const userMap = {};\n  for (let i = 0; i <= users.length; i++) {  \n    const user = users[i];\n    userMap[user.id] = user;\n  }\n  return userMap;\n}\nIt should return { \"1\": {id: 1, name: \"Alice\"} }. What is the bug and how can I fix it?”\nAI’s response to the improved prompt (simulated):\n*The error Cannot read property 'id' of undefined suggests that at some point, user is undefined, and then you try to access user.id. Looking at your loop, the issue is with the loop bounds. You are using i <= users.length. If users.length is 1 (for one user in the array), the loop runs with i = 0 and i = 1. When i is 1, users[1] is undefined (since the only index in the array is 0). Thus user becomes undefined on that iteration, and user.id throws a TypeError. The fix is to change the loop condition to i < users.length so that i goes from 0 to users.length-1. For example:\nfor (let i = 0; i < users.length; i++) {\n  const user = users[i];\n  userMap[user.id] = user;\n}\nThis way, i will be 0 for the first (and only) element, and then the loop stops. With that change, your function will no longer hit an undefined user and should work as expected, returning {\"1\": {id: 1, name: \"Alice\"}} for the given input.*\nAnalysis:\nThis response directly pinpoints the bug (using <= in the loop) and explains why it causes user to be undefined. It then provides the correct code solution (i < users.length) and confirms the expected outcome. The improved prompt succeeded because it followed our principles: it specified the language (JavaScript), described the function’s purpose, included the exact error message and a sample input, and even provided the code snippet in question. Notice how the AI used the error message as a clue to focus on the loop bounds – a targeted prompt enabled the AI to engage in true problem-solving, effectively simulating how a human debugger would think: “where could undefined come from? likely from the loop indexing”. This is a concrete demonstration of the benefit of detailed prompts.\nAdditional Debugging Tactics:\nBeyond identifying obvious bugs, you can use prompt engineering for deeper debugging assistance:\nAsk for potential causes.\nIf you’re truly stumped, you can broaden the question slightly:\n“What are some possible reasons for a TypeError: cannot read property 'foo' of undefined in this code?”\nalong with the code. The model might list a few scenarios (e.g. the object wasn’t initialized, a race condition, wrong variable scoping, etc.). This can give you angles to investigate that you hadn’t considered. It’s like brainstorming with a colleague.\n“Ask the Rubber Duck”\n– i.e., explain your code to the AI. This may sound counterintuitive (why explain to the assistant?), but the act of writing an explanation can clarify your own understanding, and you can then have the AI verify or critique it. For example:\n“I will explain what this function is doing: [your explanation]. Given that, is my reasoning correct and does it reveal where the bug is?”\nThe AI might catch a flaw in your explanation that points to the actual bug. This technique leverages the AI as an active rubber duck that not only listens but responds.\nHave the AI create test cases.\nYou can ask:\n“Can you provide a couple of test cases (inputs) that might break this function?”\nThe assistant might come up with edge cases you didn’t think of (empty array, extremely large numbers, null values, etc.). This is useful both for debugging and for generating tests for future robustness.\nRole-play a code reviewer.\nAs an alternative to a direct “debug this” prompt, you can say:\n“Act as a code reviewer. Here’s a snippet that isn’t working as expected. Review it and point out any mistakes or bad practices that could be causing issues: [code]”.\nThis sets the AI into a critical mode. Many developers find that phrasing the request as a code review yields a very thorough analysis, because the model will comment on each part of the code (and often, in doing so, it spots the bug). In fact, one prompt engineering tip is to explicitly request the AI to behave like a meticulous reviewer . This can surface not only the bug at hand but also other issues (e.g. potential null checks missing) which might be useful.\nIn summary, when debugging with an AI assistant,\ndetail and direction are your friends\n. Provide the scenario, the symptoms, and then ask pointed questions. The difference between a flailing “it doesn’t work, help!” prompt and a surgical debugging prompt is night and day, as we saw above. Next, we’ll move on to another major use case: refactoring and improving existing code.\nPrompt patterns for refactoring and optimization\nRefactoring code – making it cleaner, faster, or more idiomatic without changing its functionality – is an area where AI assistants can shine. They’ve been trained on vast amounts of code, which includes many examples of well-structured, optimized solutions. However, to tap into that knowledge effectively,\nyour prompt must clarify what “better” means for your situation\n. Here’s how to prompt for refactoring tasks:\n1. State your refactoring goals explicitly.\n“Refactor this code” on its own is too open-ended. Do you want to improve readability? Reduce complexity? Optimize performance? Use a different paradigm or library? The AI needs a target. A good prompt frames the task, for example:\n“Refactor the following function to improve its readability and maintainability (reduce repetition, use clearer variable names).”\nOr\n“Optimize this algorithm for speed – it’s too slow on large inputs.”\nBy stating\nspecific goals\n, you help the model decide which transformations to apply . For instance, telling it you care about performance might lead it to use a more efficient sorting algorithm or caching, whereas focusing on readability might lead it to break a function into smaller ones or add comments. If you have multiple goals, list them out. A prompt template from the Strapi guide suggests even enumerating issues:\n“Issues I’d like to address: 1) [performance issue], 2) [code duplication], 3) [outdated API usage].”\n. This way, the AI knows exactly what to fix. Remember, it will not inherently know\nwhat you consider a problem\nin the code – you must tell it.\n2. Provide the necessary code context.\nWhen refactoring, you’ll typically include the code snippet that needs improvement in the prompt. It’s important to include the full function or section that you want to be refactored, and sometimes a bit of surrounding context if relevant (like the function’s usage or related code, which could affect how you refactor). Also mention the language and framework, because “idiomatic” code varies between, say, idiomatic Node.js vs. idiomatic Deno, or React class components vs. functional components. For example:\n“I have a React component written as a class. Please refactor it to a functional component using Hooks.”\nThe AI will then apply the typical steps (using useState, useEffect, etc.). If you just said “refactor this React component” without clarifying the style, the AI might not know you specifically wanted Hooks.\nInclude version or environment details if relevant.\nFor instance,\n“This is a Node.js v14 codebase”\nor\n“We’re using ES6 modules”\n. This can influence whether the AI uses certain syntax (like import/export vs. require), which is part of a correct refactoring. If you want to ensure it doesn’t introduce something incompatible, mention your constraints.\n3. Encourage explanations along with the code.\nA great way to learn from an AI-led refactor (and to verify its correctness) is to ask for an explanation of the changes. For example:\n“Please suggest a refactored version of the code, and explain the improvements you made.”\nThis was even built into the prompt template we referenced:\n“…suggest refactored code with explanations for your changes.”\n. When the AI provides an explanation, you can assess if it understood the code and met your objectives. The explanation might say: “I combined two similar loops into one to reduce duplication, and I used a dictionary for faster lookups,” etc. If something sounds off in the explanation, that’s a red flag to examine the code carefully. In short,\nuse the AI’s ability to explain as a safeguard\n– it’s like having the AI perform a code review on its own refactor.\n4. Use role-play to set a high standard.\nAs mentioned earlier, asking the AI to act as a code reviewer or senior engineer can be very effective. For refactoring, you might say:\n“Act as a seasoned TypeScript expert and refactor this code to align with best practices and modern standards.”\nThis often yields not just superficial changes, but more insightful improvements because the AI tries to live up to the “expert” persona. A popular example from a prompt guide is having the AI role-play a mentor:\n“Act like an experienced Python developer mentoring a junior. Provide explanations and write docstrings. Rewrite the code to optimize it.”\n. The result in that case was that the AI used a more efficient data structure (set to remove duplicates) and provided a one-line solution for a function that originally used a loop . The role-play helped it not only refactor but also explain\nwhy\nthe new approach is better (in that case, using a set is a well-known optimization for uniqueness).\nNow, let’s walk through an example of refactoring to see how a prompt can influence the outcome. We will use a scenario in JavaScript (Node.js) where we have some less-than-ideal code and we want it improved.\nRefactoring example: poor vs. improved prompt\nSuppose we have a function that makes two database calls and does some processing. It works, but it’s not pretty – there’s duplicated code and it’s hard to read. We’d like to refactor it for clarity and efficiency. Here’s the original code:\n// Original function: Fetches two lists and processes them (needs refactoring)\nasync function getCombinedData(apiClient) {\n  // Fetch list of users\n  const usersResponse = await apiClient.fetch('/users');\n  if (!usersResponse.ok) {\n    throw new Error('Failed to fetch users');\n  }\n  const users = await usersResponse.json();\n  // Fetch list of orders\n  const ordersResponse = await apiClient.fetch('/orders');\n  if (!ordersResponse.ok) {\n    throw new Error('Failed to fetch orders');\n  }\n  const orders = await ordersResponse.json();\n  // Combine data (match user to orders)\n  const result = [];\n  for (let user of users) {\n    const userOrders = orders.filter(o => o.userId === user.id);\n    result.push({ user, orders: userOrders });\n  }\n  return result;\n}\nIssues in this code: We see duplicated fetch logic for users and orders (could be refactored into a helper or loop), lack of error detail (just a generic error message), and it could perhaps run the fetches in parallel to improve speed. Let’s see two prompting approaches:\n❌ Prompt 1 (Underspecified):\n“Refactor the above getCombinedData function.”\nAI’s response to Prompt 1:\n*Sure. Here’s a refactored version of getCombinedData:\nasync function getCombinedData(apiClient) {\n  // Fetch users and orders in parallel\n  const [usersResponse, ordersResponse] = await Promise.all([\n    apiClient.fetch('/users'),\n    apiClient.fetch('/orders')\n  ]);\n  if (!usersResponse.ok || !ordersResponse.ok) {\n    throw new Error('Failed to fetch data');\n  }\n  const [users, orders] = await Promise.all([\n    usersResponse.json(),\n    ordersResponse.json()\n  ]);\n  // Map orders by userId for efficient lookup\n  const ordersByUser = {};\n  for (const order of orders) {\n    if (!ordersByUser[order.userId]) {\n      ordersByUser[order.userId] = [];\n    }\n    ordersByUser[order.userId].push(order);\n  }\n  // Combine data\n  return users.map(user => ({\n    user,\n    orders: ordersByUser[user.id] || []\n  }));\n}\nI fetched users and orders in parallel to improve performance. I also consolidated error handling and used a map (ordersByUser) to group orders by user, which makes the combination step cleaner and potentially faster.*\nAnalysis:\nThis is actually a pretty good refactor, hitting several improvements: parallel fetching, grouping orders by user for efficiency, cleaner combination using .map and defaulting to an empty array. The AI also explained what it did. So why do we label this prompt as underspecified or “poor”? In this case, the AI happened to make good choices, but it\nassumed\na lot because the prompt was open-ended. It assumed that performance should be improved (parallel requests, which is a smart change but what if parallel wasn’t safe here?), and it changed error handling to a generic message “Failed to fetch data,” which in combining the errors lost the specificity of which call failed.\nThe risk with a generic “Refactor this” prompt is that the AI might make changes that alter behavior in unintended ways. Here, combining error handling might be acceptable or not, depending on requirements. The assistant also made a structural change (using a dictionary to group orders) – that’s fine and probably for the better, but if we had specific constraints (like wanting to stream results or keep memory usage low), we didn’t inform it. So while the result looks good, it was somewhat lucky that the AI’s own interpretation of “refactor” aligned with what we wanted. This illustrates that an underspecified prompt can sometimes get good results on simple cases, but for complex code, it might go off in an undesired direction or miss certain improvements you care about.\n✅ Prompt 2 (Goal-Oriented):\n“Refactor the above getCombinedData function to eliminate duplicate code and improve performance. Specifically: (1) Avoid repeating the fetch logic for users and orders – maybe use a helper or fetch them together. (2) Fetch both lists in parallel if possible. (3) Keep the error handling for each fetch (we want to know which call failed). (4) Improve the combination of data, possibly by using a more efficient structure for lookup instead of a nested loop. Provide the refactored code with comments explaining the changes.”\nAI’s response to Prompt 2:\n*Sure! Here’s an improved version of getCombinedData with the requested refactoring:\nasync function getCombinedData(apiClient) {\n  // 1. Fetch users and orders in parallel to improve performance\n  const [usersResponse, ordersResponse] = await Promise.all([\n    apiClient.fetch('/users'),\n    apiClient.fetch('/orders')\n  ]);\n\n  // 2. Handle errors separately to know which call failed\n  if (!usersResponse.ok) {\n    throw new Error('Failed to fetch users');\n  }\n  if (!ordersResponse.ok) {\n    throw new Error('Failed to fetch orders');\n  }\n\n  // Parse JSON responses in parallel\n  const [users, orders] = await Promise.all([\n    usersResponse.json(),\n    ordersResponse.json()\n  ]);\n\n  // 3. Use a lookup map for orders by userId for efficient matching\n  const ordersByUser = orders.reduce((map, order) => {\n    if (!map[order.userId]) {\n      map[order.userId] = [];\n    }\n    map[order.userId].push(order);\n    return map;\n  }, {});\n\n  // 4. Combine data using the lookup map\n  return users.map(user => ({\n    user,\n    orders: ordersByUser[user.id] || []  // empty array if no orders for user\n  }));\n}\nExplanation of changes:\nParallel fetching:\nI used Promise.all to fetch users and orders at the same time (point 1).\nPreserved individual error handling:\nI kept separate checks for usersResponse.ok and ordersResponse.ok so that any error clearly indicates which fetch failed (point 2).\nParallel JSON parsing:\nAlso used Promise.all for response.json() calls, which is a minor improvement.\nEfficient combination:\nInstead of a nested loop, I created an ordersByUser map (point 3). This groups orders by userId in a single pass. Then I map over users to build the result, pulling each user’s orders from the map in O(1) time (point 4). This is more efficient and concise.*\nAnalysis:\nThis response directly addressed all the specified goals. The code is refactored to be cleaner and faster, and it maintained separate error messages as requested. The AI’s explanation confirms each point we listed, which shows it carefully followed the prompt instructions. This is a great outcome because we, as the prompter, defined what “refactor” meant in this context. By doing so, we guided the AI to produce a solution that matches our needs with minimal back-and-forth. If the AI had overlooked one of the points (say it still merged the error handling), we could easily prompt again:\n“Looks good, but please ensure the error messages remain distinct for users vs orders.”\n– however, in this case it wasn’t needed because our prompt was thorough.\nThis example demonstrates a key lesson:\nwhen you know what you want improved, spell it out.\nAI is good at following instructions, but it won’t read your mind. A broad “make this better” might work for simple things, but for non-trivial code, you’ll get the best results by enumerating what “better” means to you. This aligns with community insights that clear, structured prompts yield significantly improved results .\nAdditional Refactoring Tips:\nRefactor in steps:\nIf the code is very large or you have a long list of changes, you can tackle them one at a time. For example, first ask the AI to “refactor for readability” (focus on renaming, splitting functions), then later “optimize the algorithm in this function.” This prevents overwhelming the model with too many instructions at once and lets you verify each change stepwise.\nAsk for alternative approaches:\nMaybe the AI’s first refactor works but you’re curious about a different angle. You can ask,\n“Can you refactor it in another way, perhaps using functional programming style (e.g. array methods instead of loops)?”\nor\n“How about using recursion here instead of iterative approach, just to compare?”\nThis way, you can evaluate different solutions. It’s like brainstorming multiple refactoring options with a colleague.\nCombine refactoring with explanation to learn patterns:\nWe touched on this, but it’s worth emphasizing – use the AI as a learning tool. If it refactors code in a clever way, study the output and explanation. You might discover a new API or technique (like using reduce to build a map) that you hadn’t used before. This is one reason to ask for explanations: it turns an answer into a mini-tutorial, reinforcing your understanding of best practices.\nValidation and testing:\nAfter any AI-generated refactor, always run your tests or try the code with sample inputs. AI might inadvertently introduce subtle bugs, especially if the prompt didn’t specify an important constraint. For example, in our refactor, if the original code intentionally separated fetch errors for logging but we didn’t mention logging, the combined error might be less useful. It’s our job to catch that in review. The AI can help by writing tests too – you could ask\n“Generate a few unit tests for the refactored function”\nto ensure it behaves the same as before on expected inputs.\nAt this point, we’ve covered debugging and refactoring – improving existing code. The next logical step is to use AI assistance for\nimplementing new features\nor generating new code. We’ll explore how to prompt for that scenario effectively.\nModern debugging scenarios\nReact Hook dependency issues\n❌ Poor Prompt:\n\"My useEffect isn't working right\"\n✅ Enhanced Prompt:\nI have a React component that fetches user data, but it's causing infinite re-renders. Here's my code:\n\n\nconst UserProfile = ({ userId }) => {\n  const [user, setUser] = useState(null);\n  const [loading, setLoading] = useState(true);\n  \n  useEffect(() => {\n    fetchUser(userId).then(setUser).finally(() => setLoading(false));\n  }, [userId, setUser, setLoading]); // Problem is here\n  \n  return loading ? <div>Loading...</div> : <div>{user?.name}</div>;\n};\nExpected behavior:\nShould fetch user data once when userId changes Actual behavior: Component re-renders infinitely Error in console: \"Warning: Maximum update depth exceeded\"\nWhat's causing this infinite loop and how do I fix the dependency array?\nWhy this works:\nProvides exact code, error message, expected vs actual behavior, and focuses on a specific React pattern that's commonly misunderstood.\nModern state architecture\n❌ Poor Prompt:\n\"Build the state management for my Next.js ecommerce app”\n✅ Enhanced Prompt:\nI'm building a Next.js 14 e-commerce app and need to design the state management architecture. Here are my requirements:\nComponents:\nProduct listing page (needs: products[], filters, pagination)\nShopping cart (needs: cart items, totals, shipping info)\nUser auth (needs: user profile, auth status, preferences)\nReal-time notifications (needs: toast messages, error states)\nTechnical constraints:\nNext.js 14 with App Router and Server Components\nTypeScript strict mode\nServer-side data fetching for SEO\nClient-side interactivity for cart/user actions\nState should persist across navigation\nShould I use:\nZustand stores for each domain (cart, auth, notifications)\nReact Query/TanStack Query for server state + Zustand for client state\nA single Zustand store with slices\nPlease provide a recommended architecture with code examples showing how to structure stores and integrate with Next.js App Router patterns.\nWhy this works:\nReal-world scenario with specific tech stack, clear requirements, and asks for architectural guidance with implementation details.\nPrompt patterns for implementing new features\nOne of the most exciting uses of AI code assistants is to help you write new code from scratch or integrate a new feature into an existing codebase. This could range from generating a boilerplate for a React component to writing a new API endpoint in an Express app. The challenge here is often that these tasks are open-ended – there are many ways to implement a feature. Prompt engineering for code generation is about guiding the AI to produce code that fits your needs and style. Here are strategies to do that:\n1. Start with high-level instructions, then drill down.\nBegin by outlining what you want to build in plain language, possibly breaking it into smaller tasks (similar to our advice on breaking down complex tasks earlier). For example, say you want to add a\nsearch bar feature\nto an existing web app. You might first prompt:\n“Outline a plan to add a search feature that filters a list of products by name in my React app. The products are fetched from an API.”\nThe AI might give you a step-by-step plan: “1. Add an input field for the search query. 2. Add state to hold the query. 3. Filter the products list based on the query. 4. Ensure it’s case-insensitive, etc.” Once you have this plan (which you can refine with the AI’s help), you can tackle each bullet with focused prompts.\nFor instance:\n“Okay, implement step 1: create a SearchBar component with an input that updates a searchQuery state.”\nAfter that,\n“Implement step 3: given the searchQuery and an array of products, filter the products (case-insensitive match on name).”\nBy dividing the feature, you ensure each prompt is specific and the responses are manageable. This also mirrors iterative development – you can test each piece as it’s built.\n2. Provide relevant context or reference code.\nIf you’re adding a feature to an existing project, it helps tremendously to show the AI how similar things are done in that project. For example, if you already have a component that is similar to what you want, you can say:\n“Here is an existing UserList component (code…). Now create a ProductList component that is similar but includes a search bar.”\nThe AI will see the patterns (maybe you use certain libraries or style conventions) and apply them. Having relevant files open or referencing them in your prompt provides context that leads to more project-specific and consistent code suggestions . Another trick: if your project uses a particular coding style or architecture (say Redux for state or a certain CSS framework), mention that.\n“We use Redux for state management – integrate the search state into Redux store.”\nA well-trained model will then generate code consistent with Redux patterns, etc. Essentially, you are\nteaching the AI about your project’s environment\nso it can tailor the output. Some assistants can even use your entire repository as context to draw from; if using those, ensure you point it to similar modules or documentation in your repo.\nIf starting something new but you have a preferred approach, you can also mention that:\n“I’d like to implement this using functional programming style (no external state, using array methods).”\nOr,\n“Ensure to follow the MVC pattern and put logic in the controller, not the view.”\nThese are the kind of details a senior engineer might remind a junior about, and here\nyou are the senior telling the AI\n.\n3. Use comments and TODOs as inline prompts.\nWhen working directly in an IDE with Copilot, one effective workflow is writing a comment that describes the next chunk of code you need, then letting the AI autocomplete it. For example, in a Node.js backend, you might write: // TODO: Validate the request payload (ensure name and email are provided) and then start the next line. Copilot often picks up on the intent and generates a block of code performing that validation. This works because your comment is effectively a natural language prompt. However, be prepared to edit the generated code if the AI misinterprets – as always, verify its correctness.\n4. Provide examples of expected input/output or usage.\nSimilar to what we discussed before, if you’re asking the AI to implement a new function, include a quick example of how it will be used or a simple test case. For instance:\n“Implement a function formatPrice(amount) in JavaScript that takes a number (like 2.5) and returns a string formatted in USD (like $2.50). For example, formatPrice(2.5) should return '$2.50'.”\nBy giving that example, you constrain the AI to produce a function consistent with it. Without the example, the AI might assume some other formatting or currency. The difference could be subtle but important. Another example in a web context:\n“Implement an Express middleware that logs requests. For instance, a GET request to /users should log ‘GET /users’ to the console.”\nThis makes it clear what the output should look like. Including expected behavior in the prompt acts as a test the AI will try to satisfy.\n5. When the result isn’t what you want, rewrite the prompt with more detail or constraints.\nIt’s common that the first attempt at generating a new feature doesn’t nail it. Maybe the code runs but is not idiomatic, or it missed a requirement. Instead of getting frustrated, treat the AI like a junior dev who gave a first draft – now you need to give feedback. For example,\n“The solution works but I’d prefer if you used the built-in array filter method instead of a for loop.”\nOr,\n“Can you refactor the generated component to use React Hooks for state instead of a class component? Our codebase is all functional components.”\nYou can also add new constraints:\n“Also, ensure the function runs in O(n) time or better, because n could be large.”\nThis iterative prompting is powerful. A real-world scenario: one developer asked an LLM to generate code to draw an ice cream cone using a JS canvas library, but it kept giving irrelevant output until they refined the prompt with more specifics and context . The lesson is,\ndon’t give up after one try\n. Figure out what was lacking or misunderstood in the prompt and clarify it. This is the essence of prompt engineering – each tweak can guide the model closer to what you envision.\nLet’s illustrate feature implementation prompting with an example scenario:\nFeature implementation example: building a React component with AI assistance\nSuppose we have a React application and we want to add a new component: a product list with a search filter and a sort dropdown. This is a fairly typical feature in a web app. We’ll focus on prompting for the search functionality within this component for brevity.\nInitial Prompt (Feature request):\n“Create a React functional component called ProductList that displays a list of products and includes a text input to filter the products by name. The component should: fetch an array of products from /api/products (assume it returns JSON array of {id, name, ...}), store them in state, and allow the user to type in a search box to filter the displayed products by name (case-insensitive match). Display the filtered list in a simple <ul> with product names. Include basic error handling (e.g. show an error message if the API call fails) and a loading state while fetching.”\nThat’s a long prompt but it’s specific. Let’s assume we give this to an LLM in a chat.\nAI’s response (summary of what it might produce):\nThe AI would likely output a ProductList component something like:\nimport React, { useState, useEffect } from 'react';\n\nfunction ProductList() {\n  const [products, setProducts] = useState([]);\n  const [search, setSearch] = useState('');\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState(null);\n\n  useEffect(() => {\n    async function fetchProducts() {\n      try {\n        const res = await fetch('/api/products');\n        if (!res.ok) throw new Error('Network response was not ok');\n        const data = await res.json();\n        setProducts(data);\n      } catch (err) {\n        console.error('Failed to fetch products:', err);\n        setError('Failed to load products.');\n      } finally {\n        setLoading(false);\n      }\n    }\n    fetchProducts();\n  }, []);\n\n  const handleSearchChange = (e) => {\n    setSearch(e.target.value);\n  };\n\n  // Filter products by search query (case-insensitive)\n  const filteredProducts = products.filter(product =>\n    product.name.toLowerCase().includes(search.toLowerCase())\n  );\n\n  if (loading) return <div>Loading products...</div>;\n  if (error) return <div>{error}</div>;\n\n  return (\n    <div className=\"product-list\">\n      <input \n        type=\"text\" \n        placeholder=\"Search products\" \n        value={search} \n        onChange={handleSearchChange} \n      />\n      <ul>\n        {filteredProducts.map(prod => (\n          <li key={prod.id}>{prod.name}</li>\n        ))}\n      </ul>\n    </div>\n  );\n}\n\nexport default ProductList;\n(Plus possibly some comments in the code explaining each part, if we asked for them.)\nNow, let’s analyze this result and consider prompt refinements:\nSuppose the above code came back, but our project uses a context or a global store for data fetching instead of fetching inside the component. This is something we didn’t specify. The AI made a choice to fetch in useEffect. If we prefer using a custom hook or context, we should have hinted at that. So we could refine our prompt:\n“Actually, in our app we use a custom hook useProducts() that already handles fetching. Please refactor the component to use useProducts hook instead of directly calling fetch.”\nThe AI would then adjust, perhaps assuming useProducts returns { products, loading, error } and simplify the component accordingly.\nAnother refinement: maybe we realize we also want a sort dropdown (which we didn’t mention initially). We can now extend the conversation:\n“Great, now add a dropdown to sort the products by name (A-Z or Z-A). The dropdown should let the user choose ascending or descending, and the list should sort accordingly in addition to the filtering.”\nBecause the AI has the context of the existing code, it can insert a sort state and adjust the rendering. We provided a clear new requirement, and it will attempt to fulfill it, likely by adding something like:\nconst [sortOrder, setSortOrder] = useState('asc');\n// ... a select input for sortOrder ...\n// and sort the filteredProducts before rendering:\nconst sortedProducts = [...filteredProducts].sort((a, b) => {\n  if (sortOrder === 'asc') return a.name.localeCompare(b.name);\n  else return b.name.localeCompare(a.name);\n});\n(plus the dropdown UI).\nBy iterating like this, feature by feature, we simulate a development cycle with the AI. This is far more effective than trying to prompt for the entire, complex component with all features in one go initially. It reduces mistakes and allows mid-course corrections as requirements become clearer.\nIf the AI makes a subtle mistake (say it forgot to make the search filter case-insensitive), we just point that out:\n“Make the search case-insensitive.”\nIt will adjust the filter to use lowercase comparison (which in our pseudo-output it already did, but if not it would fix it).\nThis example shows that implementing features with AI is all about\nincremental development and prompt refinement\n. A Twitter thread might exclaim how someone built a small app by continually prompting an LLM for each part – that’s essentially the approach: build, review, refine, extend. Each prompt is like a commit in your development process.\nAdditional tips for feature implementation:\nLet the AI scaffold, then you fill in specifics:\nSometimes it’s useful to have the AI generate a rough structure, then you tweak it. For example,\n“Generate the skeleton of a Node.js Express route for user registration with validation and error handling.”\nIt might produce a generic route with placeholders. You can then fill in the actual validation rules or database calls which are specific to your app. The AI saves you from writing boilerplate, and you handle the custom logic if it’s sensitive.\nAsk for edge case handling:\nWhen generating a feature, you might prompt the AI to think of edge cases:\n“What edge cases should we consider for this feature (and can you handle them in the code)?”\nFor instance, in the search example, an edge case might be “what if the products haven’t loaded yet when the user types?” (though our code handles that via loading state) or “what if two products have the same name” (not a big issue but maybe mention it). The AI could mention things like empty result handling, very large lists (maybe needing debounce for search input), etc. This is a way to leverage the AI’s training on common pitfalls.\nDocumentation-driven development:\nA nifty approach some have taken is writing a docstring or usage example first and having the AI implement the function to match. For example:\n/**\n * Returns the nth Fibonacci number.\n * @param {number} n - The position in Fibonacci sequence (0-indexed).\n * @returns {number} The nth Fibonacci number.\n * \n * Example: fibonacci(5) -> 5  (sequence: 0,1,1,2,3,5,…)\n */\nfunction fibonacci(n) {\n  // ... implementation\n}\nIf you write the above comment and function signature, an LLM might fill in the implementation correctly because the comment describes exactly what to do and even gives an example. This technique ensures you clarify the feature in words first (which is a good practice generally), and then the AI uses that as the spec to write the code.\nHaving covered prompting strategies for debugging, refactoring, and new code generation, let’s turn our attention to some\ncommon pitfalls and anti-patterns\nin prompt engineering for coding. Understanding these will help you avoid wasting time on unproductive interactions and quickly adjust when the AI isn’t giving you what you need.\nCommon prompt anti-Patterns and how to avoid them\nNot all prompts are created equal. By now, we’ve seen numerous examples of effective prompts, but it’s equally instructive to recognize\nanti-patterns\n– common mistakes that lead to poor AI responses.\nHere are some frequent prompt failures and how to fix them:\nAnti-Pattern: The Vague Prompt.\nThis is the classic\n“It doesn’t work, please fix it”\nor\n“Write something that does X”\nwithout enough detail. We saw an example of this when the question “Why isn’t my function working?” got a useless answer . Vague prompts force the AI to guess the context and often result in generic advice or irrelevant code. The fix is straightforward:\nadd context and specifics\n. If you find yourself asking a question and the answer feels like a Magic 8-ball response (“Have you tried checking X?”), stop and reframe your query with more details (error messages, code excerpt, expected vs actual outcome, etc.). A good practice is to read your prompt and ask,\n“Could this question apply to dozens of different scenarios?”\nIf yes, it’s too vague. Make it so specific that it could\nonly\napply to your scenario.\nAnti-Pattern: The Overloaded Prompt.\nThis is the opposite issue: asking the AI to do too many things at once. For instance,\n“Generate a complete Node.js app with authentication, a front-end in React, and deployment scripts.”\nOr even on a smaller scale,\n“Fix these 5 bugs and also add these 3 features in one go.”\nThe AI might attempt it, but you’ll likely get a jumbled or incomplete result, or it might ignore some parts of the request. Even if it addresses everything, the response will be long and harder to verify. The remedy is to\nsplit the tasks\n. Prioritize: do one thing at a time, as we emphasized earlier. This makes it easier to catch mistakes and ensures the model stays focused. If you catch yourself writing a paragraph with multiple “and” in the instructions, consider breaking it into separate prompts or sequential steps.\nAnti-Pattern: Missing the Question.\nSometimes users will present a lot of information but never clearly ask a question or specify what they need. For example, dumping a large code snippet and just saying “Here’s my code.” This can confuse the AI – it doesn’t know what you want. Always include a clear ask, such as\n“Identify any bugs in the above code”\n,\n“Explain what this code does”\n, or\n“Complete the TODOs in the code”\n. A prompt should have a\npurpose\n. If you just provide text without a question or instruction, the AI might make incorrect assumptions (like summarizing the code instead of fixing it, etc.). Make sure the AI knows\nwhy\nyou showed it some code. Even a simple addition like,\n“What’s wrong with this code?”\nor\n“Please continue implementing this function.”\ngives it direction.\nAnti-Pattern: Vague Success Criteria.\nThis is a subtle one – sometimes you might ask for an optimization or improvement, but you don’t define what success looks like. For example,\n“Make this function faster.”\nFaster by what metric? If the AI doesn’t know your performance constraints, it might micro-optimize something that doesn’t matter or use an approach that’s theoretically faster but practically negligible. Or\n“make this code cleaner”\n– “cleaner” is subjective. We dealt with this by explicitly stating goals like “reduce duplication” or “improve variable names” etc. The fix:\nquantify or qualify the improvement\n. E.g., “optimize this function to run in linear time (current version is quadratic)” or “refactor this to remove global variables and use a class instead.” Basically,\nbe explicit about what problem you’re solving with the refactor or feature\n. If you leave it too open, the AI might solve a different problem than the one you care about.\nAnti-Pattern: Ignoring AI’s Clarification or Output.\nSometimes the AI might respond with a clarifying question or an assumption. For instance:\n“Are you using React class components or functional components?”\nor\n“I assume the input is a string – please confirm.”\nIf you ignore these and just reiterate your request, you’re missing an opportunity to improve the prompt. The AI is signaling that it needs more info. Always answer its questions or refine your prompt to include those details. Additionally, if the AI’s output is clearly off (like it misunderstood the question),\ndon’t just retry the same prompt verbatim\n. Take a moment to adjust your wording. Maybe your prompt had an ambiguous phrase or omitted something essential. Treat it like a conversation – if a human misunderstood, you’d explain differently; do the same for the AI.\nAnti-Pattern: Varying Style or Inconsistency.\nIf you keep changing how you ask or mixing different formats in one go, the model can get confused. For example, switching between first-person and third-person in instructions, or mixing pseudocode with actual code in a confusing way. Try to maintain a consistent style within a single prompt. If you provide examples, ensure they are clearly delineated (use Markdown triple backticks for code, quotes for input/output examples, etc.). Consistency helps the model parse your intent correctly. Also, if you have a preferred style (say, ES6 vs ES5 syntax), consistently mention it, otherwise the model might suggest one way in one prompt and another way later.\nAnti-Pattern: Vague references like “above code”.\nWhen using chat, if you say “the above function” or “the previous output”, be sure the reference is clear. If the conversation is long and you say “refactor the above code”, the AI might lose track or pick the wrong code snippet to refactor. It’s safer to either quote the code again or specifically name the function you want refactored. Models have a limited attention window, and although many LLMs can refer to prior parts of the conversation, giving it explicit context again can help avoid confusion. This is especially true if some time (or several messages) passed since the code was shown.\nFinally, here’s a\ntactical approach to rewriting prompts\nwhen things go wrong:\nIdentify what was missing or incorrect in the AI’s response.\nDid it solve a different problem? Did it produce an error or a solution that doesn’t fit? For example, maybe you asked for a solution in TypeScript but it gave plain JavaScript. Or it wrote a recursive solution when you explicitly wanted iterative. Pinpoint the discrepancy.\nAdd or emphasize that requirement in a new prompt.\nYou might say,\n“The solution should be in TypeScript, not JavaScript. Please include type annotations.”\nOr,\n“I mentioned I wanted an iterative solution – please avoid recursion and use a loop instead.”\nSometimes it helps to literally use phrases like\n“Note:”\nor\n“Important:”\nin your prompt to highlight key constraints (the model doesn’t have emotions, but it does weigh certain phrasing as indicating importance). For instance:\n“\nImportant:\nDo not use any external libraries for this.”\nor\n“\nNote:\nThe code must run in the browser, so no Node-specific APIs.”\n.\nBreak down the request further if needed.\nIf the AI repeatedly fails on a complex request, try asking for a smaller piece first. Or ask a question that might enlighten the situation:\n“Do you understand what I mean by X?”\nThe model might then paraphrase what it thinks you mean, and you can correct it if it’s wrong. This is meta-prompting – discussing the prompt itself – and can sometimes resolve misunderstandings.\nConsider starting fresh if the thread is stuck.\nSometimes after multiple tries, the conversation may reach a confused state. It can help to start a new session (or clear the chat history for a moment) and prompt from scratch with a more refined ask that you’ve formulated based on previous failures. The model doesn’t mind repetition, and a fresh context can eliminate any accumulated confusion from prior messages.\nBy being aware of these anti-patterns and their solutions, you’ll become much faster at adjusting your prompts on the fly. Prompt engineering for developers is very much an iterative, feedback-driven process (as any programming task is!). The good news is, you now have a lot of patterns and examples in your toolkit to draw from.\nConclusion\nPrompt engineering is a bit of an art and a bit of a science – and as we’ve seen, it’s quickly becoming a must-have skill for developers working with AI code assistants. By crafting clear, context-rich prompts, you essentially\nteach\nthe AI what you need, just as you would onboard a human team member or explain a problem to a peer. Throughout this article, we explored how to systematically approach prompts for debugging, refactoring, and feature implementation:\nWe learned to feed the AI the same information you’d give a colleague when asking for help: what the code is supposed to do, how it’s misbehaving, relevant code snippets, and so on – thereby getting much more targeted help .\nWe saw the power of iterating with the AI, whether it’s stepping through a function’s logic line by line, or refining a solution through multiple prompts (like turning a recursive solution into an iterative one, then improving variable names) . Patience and iteration turn the AI into a true pair programmer rather than a one-shot code generator.\nWe utilized role-playing and personas to up-level the responses – treating the AI as a code reviewer, a mentor, or an expert in a certain stack . This often produces more rigorous and explanation-rich outputs, which not only solve the problem but educate us in the process.\nFor refactoring and optimization, we emphasized defining what “good” looks like (be it faster, cleaner, more idiomatic, etc.) , and the AI showed that it can apply known best practices when guided (like parallelizing calls, removing duplication, handling errors properly). It’s like having access to the collective wisdom of countless code reviewers – but you have to ask the right questions to tap into it.\nWe also demonstrated building new features step by step with AI assistance, showing that even complex tasks can be decomposed and tackled one prompt at a time. The AI can scaffold boilerplate, suggest implementations, and even highlight edge cases if prompted – acting as a knowledgeable co-developer who’s always available.\nAlong the way, we identified pitfalls to avoid: keeping prompts neither too vague nor too overloaded, always specifying our intent and constraints, and being ready to adjust when the AI’s output isn’t on target. We cited concrete examples of bad prompts and saw how minor changes (like including an error message or expected output) can dramatically improve the outcome.\nAs you incorporate these techniques into your workflow, you’ll likely find that working with AI becomes more intuitive. You’ll develop a feel for what phrasing gets the best results and how to guide the model when it goes off course. Remember that the AI is a product of its training data – it has seen many examples of code and problem-solving, but it’s\nyou\nwho provides direction on which of those examples are relevant now. In essence,\nyou set the context, and the AI follows through\n.\nIt’s also worth noting that prompt engineering is an evolving practice.\nThe community of developers is constantly discovering new tricks – a clever one-liner prompt or a structured template can suddenly go viral on social media because it unlocks a capability people didn’t realize was there. Stay tuned to those discussions (on Hacker News, Twitter, etc.) because they can inspire your own techniques. But also, don’t be afraid to experiment yourself. Treat the AI as a flexible tool – if you have an idea (“what if I ask it to draw an ASCII diagram of my architecture?”), just try it. You might be surprised at the results, and if it fails, no harm done – you’ve learned something about the model’s limits or needs.\nIn summary, prompt engineering empowers developers to get more out of AI assistants.\nIt’s the difference between a frustrating experience (“this tool is useless, it gave me nonsense”) and a productive one (“this feels like pair programming with an expert who writes boilerplate for me”). By applying the playbook of strategies we’ve covered – from providing exhaustive context to nudging the AI’s style and thinking – you can turn these code-focused AI tools into true extensions of your development workflow. The end result is not only that you code faster, but often you pick up new insights and patterns along the way (as the AI explains things or suggests alternatives), leveling up your own skillset.\nAs a final takeaway, remember that\nprompting is an iterative dialogue\n. Approach it with the same clarity, patience, and thoroughness you’d use when communicating with another engineer. Do that, and you’ll find that AI assistants can significantly amplify your abilities – helping you debug quicker, refactor smarter, and implement features with greater ease.\nHappy prompting, and happy coding!\nFurther reading:\nHow to write better prompts for GitHub Copilot\n. GitHub Blog\nChatGPT Prompt Engineering for Developers: 13 Best Examples\nUsing ChatGPT for Efficient Debugging\nPrompt Engineering for Lazy Programmers: Getting Exactly the Code You Want\nBest practices for prompting GitHub Copilot in VS Code\nChatGPT: A new-age Debugger, 10 Prompts\nChatGPT Prompts for Code Review and Debugging\n297\nShare this post\nElevate\nThe Prompt Engineering Playbook for Programmers\nCopy link\nFacebook\nEmail\nNotes\nMore\n1\n30\nShare\nDiscussion about this post\nComments\nRestacks\nVirul D\n7d\nLiked by Addy Osmani\nFor frontend testing and debugging, Browserbase MCP will be really helpful as it can show the coding agent issur with a screenshot.\nExpand full comment\nReply\nShare\nTop\nLatest\nDiscussions\nNo posts\nReady for more?\nSubscribe\n© 2025 Addy Osmani\nPrivacy\n∙\nTerms\n∙\nCollection notice\nStart writing\nGet the app\nSubstack\nis the home for great culture\nShare\nCopy link\nFacebook\nEmail\nNotes\nMore\nThis site requires JavaScript to run correctly. Please\nturn on JavaScript\nor unblock scripts"
  },
  {
    "title": "Consider Knitting",
    "href": "https://journal.stuffwithstuff.com/2025/05/30/consider-knitting/",
    "content": "Consider Knitting\n←\nMay 30, 2025\nart\nknitting\nLet’s say that, like me, you are a person who stares at a computer and writes\ncode for a living. As a straight male who grew up in a time where knitting was\nvery strongly female coded, it for the most part never occurred to me that\nknitting was a thing I could do and might enjoy. Regardless of your demographic\ncategories and background, it’s possible that you have also not really\nconsidered knitting.\nThis article exists to get you to do so. Specifically, I’ll try to convince you,\none software person to another, why it might be a good fit for your life and\nbrain. This is a pitch for knitting, but—for better or worse—an extremely\nnerdily argued one.\nBefore I start, note that when I say “knitting”, you can read that as any of the\nvarious\nfiber arts\n, including crochet, weaving, macramé, cross-stitch, etc.\nI talk about knitting here because that’s the one closest to my heart and I\nstrive to speak from the heart. You can make stuff out of string however you\nwant. We are all fiber friends.\nThe sense of touch\nI love the aesthetics of programming. Sitting in a cool quiet room, techno\nthumping in my headphones, coffee mug next to me, while a neatly arranged field\nof glowing monospace glyphs stream across my screen. But there’s one sense\nunmentioned in that sentence: touch. The sense we devote more neurons to than\nany other is curiously underutilized while pumping out code.\nIt’s no surprise that some programmers\nfetishize\nkeyboards\n. It’s just about the only part of programming that has any physical sensation\nat all\n.\nI got into knitting a few years after the pandemic. While I have a variety of\nhobbies, most are still staring at a screen and maybe\npushing some buttons and\nturning knobs\n. When I wasn’t doing those, I was staring at a screen\nfor work, or staring at a screen for\nnot-work\n.\nI don’t know if I have a good way to explain how much my body craved tactile\nexperience by the end of that. It’s like my fingers ached. A deep hunger, but\nnot for taste. I’d wander around the house, driving my wife insane, unable to\nsit down and get comfortable. My body was just screaming at me to\ndo\nsomething.\nMy youngest daughter had just picked up knitting, and she taught me. At first it\nwas frustrating and annoying. But once I had the basics down, it was like a a\ndeep sigh felt in my hands. Knitting is\nso\ntouch-centered. Skilled knitters\ncan knit without looking at their hands at all—touch alone is sufficient.\nThere are so many different kinds of yarn to work with and they all feel\ndifferent. Cotton is tough and firm, like twine. Wool is soft and springy,\nforgiving as you pull stitches open to work them. Superwash wool is smooth and\nglides off the needles. Non-superwash wool has this very slight stickiness to it\nthat makes the resulting fabric feel solid and whole. Thin fingering weight yarn\nwraps tightly around your finger like a reminder knot. Working it is like\nperforming delicate surgery. Squishy chunky wool spreads your fingers wide and\nworks so quickly it’s like fabric is spooling out of your hands on its own\naccord.\nEven needles each have their own personality. Stiff grippy bamboo with its dull\nclack. Less worrisome to use because stitches don’t slide off as easily, but\nharder work to push against the friction. Polished stainless steel where the\nstitches just fly off the needles—good when you are done with those stitches\nbut not so much otherwise.\nOnce you’ve worked a few thousand stitches into your muscle memory, you can\nwatch your fingers form stitches almost of their own accord, hypnotically. Right\nneedle opens the stitch and slides in. Left finger wraps the yarn around the\nneedle. Right finger grabs yarn. Right hand pulls the needle back out, a new\nstitch formed and transferred to the other needle. Over and over, like a\nmeditation in the body.\nKnitting\nfeels good\n. It is an intimate, constant reminder that we are a tool-using species with thousands of years of evolution giving us incredible dexterity and the emotional wiring to make us want to use it.\nAn open world game with optimized skill curve\nOf course, you could get much of that same tactile joy by driving to your\nnearest yarn store and wandering around the aisles jamming your fingers into\nevery ball and skein of yarn they have. (An activity I certainly\nalso\ndo and\nhighly recommend.) Knitting isn’t just about having a hedonistic tactile\nexperience. It is a skilled art with an unbelievably deep lore.\nI\nused to be a game programmer\n, and I tend to look at a lot of\nactivities through the lens of game design. Games are interesting because they\nare user experience distilled to its essence. When you are, say, using a banking\napp to transfer some money into savings, there is a user experience in play. But\nthere’s also a\nutility\n, a real effect you are trying to have in the material\nworld. You may use your banking app and be satisfied that it helps you save\nmoney even if the UX is trash.\nBut a game, almost by definition, doesn’t\ndo\nanything “out there”. Aside from\nleaderboards and stuff, the point of a game is to be a low-stakes sandbox where\nyou can play without, you know, accidentally deleting money out of your bank\naccount. Because of that, it’s a pure user experience. All experience and\nnothing else.\nTherefore, skilled game designers are possibly the best user experience\ndesigners in the world, and the tools they use to think about game design are\nuseful lenses to evaluate just about any kind of human endeavor.\nTwo aspects (at least) are relevant to knitting.\nLinearity\nA game is “linear” if there’s only one path from beginning to end, one way to\nplay. Don’t particularly enjoy the desert level? Tough shit, you gotta get\nthrough it to get to the end. “Nonlinear” games let players choose among\nmultiple paths to reach the end. “Open world” or “sandbox” games blow the\ngameplay wide open and let players go where they want when they want. The game\nmay not even\nhave\nan “end”.\nKnitting is an open world game. There are all sorts of objects you can make out\nof yarn, and all sorts of styles and techniques to make them. Don’t like socks?\nFine, you don’t ever have to go through a “sock making phase” to graduate into\nwhat you really want to make. Find stranded colorwork too fiddly? There’s\nintarsia or just buy a ball of self-striping yarn and let the yarn change\ncolors for you. Is a sweater too big of a commitment? You can make hats forever.\nOnce you get past the very basics of getting loops on the needles and making\nstitches, knitting very rarely forces you to slog through something you don’t\nwant to do in order to reach some other goal. Except weaving in ends, I guess,\nwhich is kind of a chore. But, honestly, it’s not that bad.\nSkill curve\nA skill curve is sort of like a learning curve. It’s an imaginary graph of how\nmuch effort it takes to reach greater and more rewarding levels of skill. Some\nskill curves are steep at the beginning and then flatten out once you’re over\nthe hump. When you first start playing guitar and don’t have the hand strength,\ncallouses, or dexterity to form chords, it’s\nreally\nhard. But after a few days\nyou can get the basics down. Then it’s pretty easy to learn more and more chords\nafter that.\nOther skill curves start shallow and get steeper. You can learn chess in a day\nand have fun playing it, but as you get more serious about, each incremental\nincrease in skill requires a greater commitment to studying the theory of the\ngame.\nKnitting has a marvellously smooth, user-controllable skill curve. There’s a\nsmall hump at the beginning. It does take a little while to figure out how to\nhold the needles, control the tension of the yarn with your fingers, and get\nyour hands to work in concert to make stitches. It feels like you’re making a\nshadow puppet of a sewing machine. Your initial experience will be frustrating.\nBut you can push through that in an hour or so. In a day, you can learn a basic\nway to cast stitches onto the needles, make knit and purl stitches, and then\ncast off to finish the work. With just those, you can make scarves and\ndishclothes. You are a knitter.\nThen the world is your oyster. There are\nthousands of patterns out\nthere\n, each listing the techniques required. You can pick ones well\ninside your comfort zone and grow your skills slowly. Or you can challenge\nyourself to learn a bunch of techniques at once. There are dozens of tiny little\nindependent tricks to learn, each it’s own little merit badge serotonin hit:\nlong-tail cast on, increases and decreases, cables, etc. So many fun different\nways to form stitches. Each is one bite-sized lesson and no matter how many or\nfew you want to chew on at a time, there is a project out there that will\nsatisfy your appetite.\nIt’s not just the steepness of the curve, but also its height. Some skill curves\ntop out early. I suspect the world’s greatest kazoo player is not profoundly\nbetter at kazoo than I am. Others seem to have no limit, like the world’s best\nviolinists or Go players.\nFriend, the knitting lore goes\ndeep\n. People have been developing this artform\nfor literally over a thousand years. Knitters have sat there, brain semi-idle,\nwhile their fingers worked yarn for millions of hours. They had\nplenty\nof time\nto invent all sorts of crazy ways to tangle yarn up. You could knit full-time\nfor the rest of your life and never run out of new things to learn. Knitting\nwill never stop rewarding you.\nStructured but not a game\nTo be clear, I’m talking about knitting and videogames\nas a metaphor\n. I don’t\nthink of knitting as a game to win. Like a lot of programmers, I am prone to\npointing my dumb optimizing brain at random activities and trying to min-max the\nshit out of it. Ask my wife how many ways to make coffee I have tried to find\nthe optimal effort/reward ratio.\nWhen I knit, I do try to knit efficiently. I mean, if I’ve gotta make 10,000\nstitches to finish a single scarf, it pays to put some thought into the process.\nAt the same time, knitting isn’t just a pointless exercise to scratch my\ngamification itch. It is a real artform, and I am making real objects.\nFor me, knitting strikes a good balance between structure and unstructure. I\nlike making music and when it goes well, it’s very rewarding. But sometimes I\nsit down and every melody that comes out sounds like a knock-off NES game (and\nnot in that cool chiptune way). Or I’ll spend an entire evening working on the\ndrum mixing and at the end I can’t tell if I made things better or worse. It’s\ntoo\nunstructured.\nWhen I sit down to knit, I might make a mistake that needs unwinding and fixing.\nBut for the most part, I can be confident that an hour spent knitting will get\nme closer to a beautiful finished object. It provides a reliable serotonin hit\nof “I’m making progress”. But it’s not so rigidly structured that it triggers my\noptimizing brain into sucking the joy out of it by turning it into math\nhomework.\nA time and a space\nA particularly nice property of knitting is that it is able to provide that\nreliable gratification while accommodating all of the other complexities in my\nlife. It takes very little time to make progress knitting. Unlike, say,\npainting, there’s almost no set up at the beginning of a session or clean up at\nthe end. When I pick up the kids from school, I can get ten minutes of knitting\nin while I wait for the bell.\nNor does it take up much space. A zip-loc bag with a ball of yarn and two\nneedles is basically all you need, which is always in my backpack when I get on\nan airplane now.\nConveniently, the TSA\nspecifically allows knitting needles on flights\n.\n(The fact that the TSA is explicitly fine with you bringing a satchel full of\nfoot-long sharpened metal spikes onto a plane as long as its accompanied by some\nstring really says something about it as security theatre.) Note that this only\napplies to domestic flights in the US. Other countries have their own rules,\nthough I haven’t had any trouble bringing knitting to Denmark or Costa Rica.\nKnitting expands and contracts around not just physical space, but headspace\ntoo. Had a long day and want something mindless to help you unwind? Slap\ntogether a\ngarter stitch scarf\nand just do the same stitch over and\nover again. Stressed out by work or grieving a loss and need something consuming\nto take your mind off it? Start a lacework or cable knit project and the chart\nand counting won’t leave room to think about anything else.\nWhatever logistical or mental capacities you have, there is a knitting project\nthat will tuck neatly into it.\nAnd then at the end\nSo far, I’ve been talking about knitting as an\nactivity\n. A personal hobby to\nkill time for your own joy. Kill time it does, but knitting isn’t just about\nwhiling away the hours. It’s not playing solitaire or binging a TV show for the\nfifth time.\nAs yarn spools through your fingers and the hands twirl around the clock, an\nactual physical, beautiful object emerges at the end. Well, your first couple of\nobjects may not be so beautiful. But even the lumpiest knitted scarf is imbued\nwith something increasingly elusive these days:\ncare\n.\nThe first real thing I knitted was a scarf for my mother-in-law. In retrospect,\nI can’t say it’s a great scarf. Kinda cheap acrylic yarn. Not really her color.\n4x4 rib was about all I could handle complexity-wise at the time, and it means\nthe scarf tends to bunch up on itself. But when she opened the package on\nChristmas and saw it, her eyes teared up. Mine are tearing up now writing this.\nBecause regardless of how good the object itself is, it is an inarguable\ntestament to the fact that I chose to spend dozens of quiet hours making stitch\nafter stitch, all the while thinking about her and how much she means to me. A\nfraction of my life’s wick that I burned for her and no one else.\nIn a world where so many seem to want to get more and more out of less and less,\nto automate and AI-ify everything until an infinite content firehose is blasting\ninto every orifice of every consumer, hand knitting to me is the antidote. An\nacknowledgement that all we really have is time and thus there is no gift more\nprecious than spending it on someone.\nAlso, once you finish a project, you get to buy more yarn. Because, if I’m\nhonest, a little consumption feels kinda nice too.\nOK, I’m sold\nAnyway, this is what knitting means to me. Which, now that I read all this, is a\nlot more than I realized. If that didn’t pique your interest, fine. It’s not for\neveryone. I do hope you find something out there to spend your time on that\nprovides as much joy as knitting does to me. You deserve that.\nIf this did make you want to give knitting a try, you’re probably wondering what\nnext. Fortunately, there are, like, a million “learn how to knit” tutorials out\nthere. One of the actually marvellous things about living in the world today is\ngood access to lots of videos, and knitting is an activity that’s\nreally\nhard\nto convey in book form. You kind of need to watch someone’s hands. Learning from\nsomeone in person is best, but if you don’t have that, YouTube is a pretty good\nsubstitute.\nKeep in mind that everyone’s hands are different! There are many ways (a\nhandful, heh heh) to hold the needles and form stitches because our anatomy and\nthe texture of our skin varies so much. Watch a few videos and don’t worry if\nwhat works for them doesn’t work for you. Eventually, you’ll find one that does.\nExpect to be challenged and frustrated at first. There’s a lot to going on all\nat once: controlling the tension of the working yarn, keeping the stitches from\nthe previous row on the left needle, keeping the new stitches on the right\nneedle, forming new stitches. This may be one of the first times you’ve used\nthis many of your fingers doing\ndifferent\nthings all at once.\nI promise that if you’re patient with yourself and give it a few tries, you will\nget over the hump. Once you can knit a swatch of garter stitch, everything else\nwill come naturally over time.\nSo go your local craft store, buy a cheap pair of size 7 needles, a ball of\nworsted (i.e. medium) weight acrylic or wool yarn in a color you think is\npretty, and give it a try. The worst that can happen is you’ll waste a few\nbucks. If you’re lucky, you might end up making your mother-in-law cry (in a\ngood way).\nPlease enable JavaScript to view the\ncomments powered by Disqus.\nHi! I'm\nBob Nystrom\n, the one on the left. I wrote\nGame Programming Patterns\nand\nCrafting Interpreters\n.\nI make electronic music under the name\nTiny Wires\n.\nYou can follow me on Mastodon at\n@munificent\nor email me at\nrobert\nat this site.\nTags\nblog\nbook\nc\nc-sharp\ncode\ncpp\ndart\ndesign\nf-sharp\nfinch\ngame-dev\ngame-patterns\ngo\njava\njavascript\njs\nlanguage\nlua\nmagpie\nmusic\nparsing\npersonal\npython\nroguelike\nruby\nvgs\nAll articles…\nThis blog is built using a bespoke static site generator. The source repo\nis\nhere\n.\n© 2008-2025 Bob Nystrom"
  },
  {
    "title": "Ship Carrying EVs Abandoned in Pacific After Catching Fire",
    "href": "https://www.bloomberg.com/news/articles/2025-06-04/ship-carrying-evs-abandoned-in-pacific-after-catching-fire",
    "content": ""
  },
  {
    "title": "Ask HN: Startup getting spammed with PayPal disputes, what should we do?",
    "href": "https://news.ycombinator.com/item?id=44176510",
    "content": "Hacker News\nnew\n|\npast\n|\ncomments\n|\nask\n|\nshow\n|\njobs\n|\nsubmit\nlogin\nAsk HN: Startup getting spammed with PayPal disputes, what should we do?\n186 points\nby\njune3739\n16 hours ago\n|\nhide\n|\npast\n|\nfavorite\n|\n141 comments\nLongtime user posting from a new account out of an abundance of caution.\nI founded an e-commerce marketplace startup. We use PayPal's Multiparty APIs (PayPal Commerce Platform) for checkout. For the 10 days, someone has been bombarding us with purchases that they later dispute. There's consistent pattern to it:\n* They use an email address that has no footprint online, always from the same two domains\n* They use an unverified PayPal account to pay\n* They pay a low amount, not always the same, in a narrow range for a digital item\n* All of the charges were disputed within a few hours\nThey're not doing this through our API. The purchase process requires a browser because of the way our payment form is configured. There's an amount of variation to each purchase that tells us they're automating a browser. Logs indicate that they're changing IP each time. The events come in bursts and seem to be spaced to avoid automated detection.\nWe added the typical mitigations to our network stack and code. A few are still slipping through. Logs indicate a high amount of bot traffic.\nPayPal does not seem equipped to deal with this. Their support is always extremely slow, relies on canned responses, and to date has a very limited understanding of how their own Multiparty APIs work. Their phone support people will not talk with me, they see no indication that my PayPal account is affiliated with these purchases in any way. They want each of our sellers to contact them independently, which we know will result in disparate cases that don't tell the complete story or offer any assistance.\nHas anyone encountered anything like this before? We're struggling to find the motive or intended outcome by the attacker(s). We're a small company with a niche audience, we've never had a conflict with anyone that got serious enough that we'd expect them to come after us like this.\nAny thoughts and recommendations would be greatly appreciated. We feel like we are on our own here and are unsure of how to handle it.\npatio11\n3 hours ago\n|\nnext\n[–]\n(I worked at a different processing company, which I am not speaking for.)\nWe're struggling to find the motive or intended outcome by the attacker(s).\nThe highest likelihood for me is that they're doing card/credential testing. They have either stolen or purchased a large number of stolen credentials. Those credentials are worth more individually if they are known to function. They can use any business on the Internet which sells anything and would tell someone \"Sorry, can't sell you that because I couldn't charge your account/card/etc. Do you have another one?\" to quickly winnow their set of credentials into a pile of ones which haven't been canceled yet and another pile. Another variation of this attack is their list is \"literally just enumerate all the cards possible in a range and try to sift down to the cards that actually exist.\"\nAfter sifting through to find the more valuable cards, they sell this onto another attacker at higher price of the mixed-working-and-not-working cards, or they pass it to their colleague who will attempt to hit the cards/creds for actual money.\nDigital items are useful because people selling them have high margins and have lower defenses against fraud as a result. Cheap things, especially cheap things where they can pick their price, are useful because it is less likely to trigger the attention of the card holder or their bank. (This is one reason charities get abused very frequently, because they will often happily accept a $1 or lower donation, even one which is worth less than their lowest possible payment processing cost.) The bad guys don't want to be noticed because the\nreal\ntheft is in the future, by them or (more likely) by someone they sell this newly-more-valuable card information onto.\nThis hit the company I used to run back in the day, also on Paypal, and was quite frustrating. I solved it by adding a few heuristics to catch and giving a user matching those heuristics the product for free, with the usual message they got in case of a successful sale. This quickly spoils your website for the purpose they're trying to use it for, and the professional engineering team employed to abuse you experiences thirty seconds of confusion and regret before moving to the next site on their list. Back in the day, the bad guys were extremely bad at causing their browser instance to even try to look like a normal user in terms of e.g. pattern of data access prior to attempting to buy a thing.\nHope some of that is useful. Best of luck and skill. You can eventually pierce through to Paypal's attention here and they may have options available contingent on you being under card/credential testing attack, or they might not. I was not successful in doing so back in the day prior to solving the problem for myself.\nWould also recommend building monitoring so you know this is happening in the future before the disputes roll in. Note that those disputes might be from them or from the legitimate users depending on exactly what credentials they have stolen, and in the case they are from legitimate users, you may not have caught all of the fraudulent charges yet. (Mentioning because you said \"all of the charges\" were disputed.) If I were you I'd try to cast a wider net and pre-emptively refund or review things in the wider net, both because the right thing to do and also because you may be able to head off more disputes later as e.g. people get their monthly statements.\nreply\nNicholas_C\n3 hours ago\n|\nparent\n|\nnext\n[–]\nWe had the same issue (people testing stolen credit card numbers) on Stripe that was close to getting us shut off for a certain credit card company. We implemented a captcha and a tool to validate email addresses (emaillistverify) and it solved the problem.\nreply\nvdfs\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nWe had the same issue because Marketing was using a stupid landing page SaaS tool to generate sales, it was connected directly to Stripe and we didn't have any control over it. We discovered the problem through Intercom, which notified us about a high volume of bounced emails (automatically sent after purchase). It was clear what was going on after discovering the same pattern.\nTo fix it, I had to proxy that unreliable SaaS software to implement CAPTCHAs and stronger bot detection. It was essentially a MITM-style proxy but for protection. It was fun to implement\nreply\nalex_suzuki\n1 hour ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nTIL about emaillistverify. Their website always talks about „bulk email checking“, but I assume they also support „live checks“ through an API?\nI assume you prevent users from signing up if the check fails?\nreply\njffry\n41 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nTop nav of their site has an \"API\" link which goes to a page that says \"ELV’s API keeps your email list clean. Notify website user about an invalid email address when they are filling out a form.\"\nSo presumably yes\nreply\ntreebeard901\n2 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nThis is probably the best way to stop it from being automated. As well as a verified form of 2FA like a phone or email code.\nreply\ncolechristensen\n1 hour ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nAgreed.  This is a situation where you need a dedicated security team to classify and mitigate this kind of attack while making sure the mitigations don't add too much friction to your real customers.  It's not easy.  It's also not really on your payment processor to be the first line of defense for this kind of fraud.\nYou'll need to find some way to fingerprint to classify users into risk buckets and then treat them differently based on the bucket: blackhole, high friction verification, and likely safe are three reasonable buckets.\nCloudflare has tools that can help identify bots, much of this can be offloaded onto them.\nreply\ncookiengineer\n9 hours ago\n|\nprev\n|\nnext\n[–]\nThis is a money laundering scheme where they are trying out how far they can go per domain.\nIt's also a bug in the paypal API that they're abusing, where the SDK doesn't differ between example.com and www.example.com. If webshops like yours get exploited and used for money laundering, they will mix transactions from those two subdomains, while leaving the www.example.com domain as it is. The support people at paypal are dumb enough to not take care about each case, and usually they mix transactions later also via other social media services that have microtransactions (e.g. tiktok or snapchat streams where you can gift away items).\nThe way paypal support's workflow works is that they have to nanually identify each and every transaction separately, meaning a human will be busy for weeks on end. Not kidding you. That's how the scammers keep winning with schemes like this. Usually there's also no way to escalate this, not even for business customers, at paypal, due to how their support offices are structured organizationally.\nAs a mitigation I'd recommend to block ASNs that are known hosters that do this, and double check your webshop version for known vulnerabilities and fixes.\nIf you don't use docker already, start to virtualize your webshop software now. I can't stress how important this is. Also double check any users and passwords you are using for the services, and the rest of the filesystem for indicators on the VPS. Disable SSH passwords and use only SSH key authentication on the VPS in case this hasn't been done already.\nI'm writing this because usually this kind of scheme starts to happen after the server got pwned already, and after e.g. the ssh password bruteforce scanner was successful or after the web exploit / persistence exploit was successful.\nIf you need a starting point to block those botnet affiliated networks, I started both a firewall and scam database project that does exactly this:\n[1]\nhttps://github.com/cookiengineer/antispam\n[2]\nhttps://github.com/tholian-network/firewall\nreply\nJamesAdir\n9 hours ago\n|\nparent\n|\nnext\n[–]\nSorry for the noob question, but how can Docker help remediate the situation? I'm currently learning about DevOps.\nreply\ndanbreuer\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIt can't easily, Docker should not be naively treated as a security solution. It's very easy to misconfigure it:\n- The Docker daemon runs as root: any user in the docker group effectively also has sudo (--privileged)\n- Ports exposed by Docker punch through the firewall\n- In general, you can break the security boundary towards root (not your user!) by mounting the wrong things, setting the wrong flags etc.\nWhat Docker primarily gives you is a stupid (good!) solution for having a reproducible, re-settable environment.\nBut containers (read: magic isolated box) are not really a good tool to reason about security in Linux imo.\nIf you are a beginner, instead make sure you don't run services as the sudo-capable/root user as a first step.\nThen, I would recommend you look into Systemd services: you can configure all the Linux sandboxing features Docker uses and more.\nThis composes well with Podman, which gives you a reproducible environment (drop-in replacement for Docker) but contained to an unprivileged user.\nreply\nfugue88\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI agree with what you wrote, and add that you should make sure that your service's executables and scripts also should not be owned by the user they run as.\nIt's unfortunately very common to install, for example, a project as the \"ubuntu\" user and also run it as the \"ubuntu\" user.  But this arrangement effectively turns any kind of file-overwrite vulnerability into a remote-execution vulnerability.\nOwning executables as root:root, perms 0755, and running as a separate unprivileged user, is a standard approach.\nreply\nsmnc\n3 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> - Ports exposed by Docker punch through the firewall\nI've been using ufw-docker [1] to force ufw and docker to cooperate. Without it, Docker ports do actually get exposed to to the Internet. As far as I can tell, it does its job correctly.\nIs there another problem I am not aware of?\n[1]\nhttps://github.com/chaifeng/ufw-docker\nreply\nmsgodel\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nDocker keeps well behaved programs well behaved. You can escape in one line of shell.\nreply\nedoceo\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nHow? Like if I have a Debian-Slim container running it's possible to \"break-out\" onto the host?\nreply\nmsgodel\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYup that's trivially easy if you have permissions to use mknod and mount. (and if the file system namespace looks like it normally does all you need is mount.)\nDocker is for organizing things for yourself, just like directories are. If you want actual isolation you have to take extra steps.\nEDIT: and I feel like I should add those extra steps are exactly what most server software does automatically when it chroots itself. Again docker is really just for organizing things.\nreply\ncookiengineer\n9 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nContainers allow separation of access rights, because you don't have to pwn only one program/service that is running on the host system to get physical access to it.\nContainers have essentially 3 advantages:\n- Restart the containers after they got pwned, takes less than a second to get your business up and running again.\n- Separation of concerns: database, reverse proxy, and web service run in separate containers to spread the risk, meaning that an attacker now has to successfully exploit X of the containers to have the same kind of capabilities.\n- Updates in containers are much easier to deploy than on host systems (or VPSes).\nreply\nguappa\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThere's already unix permissions and regular namespaces. Docker is very hard to secure.\nreply\nimglorp\n5 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> Separation of concerns\nSorta: yes the container is immutable and can be restarted, but when it does, it has the same privs and creds to phone up the same DB again or mount the same filesystem again. I'd argue touching the data is always the problem you're concerned about. If you can get an exec in that container you can own its data.\nreply\nneom\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nWhy do you think ISOs never really took off? I feel like they solve so many issues but only ever see folks reach for containers.\nreply\ndiggan\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nDo mean VMs? ISO is a file format, commonly used for VMs and other computers.\nFor VMs, they did take off and essentially the entire cloud ecosystem runs on mostly VMs behind the scenes for VPS and similar hosting.\nIt's true though at it seems more popular for developers to reach for containers when they need to think about deployments, particularly docker containers. But VMs are still widely in use and deployed today.\nreply\nneom\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nyyeaaah, i built a cloud. :) I love VMs. I'm a disciple of Alex Polvi. Lets call it an \"Immutable Application VM\" Stack. Each application service (or a logical group of application services) is packaged directly into an immutable VM image, and the orchestration manages these VMs directly. No separate container runtime or container orchestration layer on top of the VM. So you have an Immutable, Bootable System Image, but you would use kvm plus .iso plus orchestration tech. Basically, why does nobody built a cloud on the cloud lol??\n(I helped build digitalocean from zero the pre-IPO, so I'm verrry rusty, this all might be nonsense/wrong think, and happy to be told as much! :))\nreply\nmjburgess\n7 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nJust thinking about this from a proxmox pov -- applying this advice, do you see an issue with then saying: take a copy of all \"final\" VMs, delete the VM and clone the copy?\nAnd, either way, do you have a thought on whether you'd still prefer a docker approach?\nI have some on-prem \"private cloud\"-style severs with proxmox, and just curious about thinking through this advice.\nreply\ncalgoo\n9 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nNot OP, but Im assuming its because of immutability of the containers where you can redeploy from a prebuilt image very quickly. There is nothing that says you cant do the same with servers / VMs however the deployment methodology for docker is a lot quicker (in most cases).\nEdit: Im aware its not truly immutable (read only) but you can reset your environment very easy and patching also becomes easier.\nreply\nahoka\n8 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nIt can't. Also there's nothing inherently wrong with ssh password auth.\nreply\ndmos62\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYou might want to back those statements up.\nreply\ndanbreuer\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nNot parent, but see my sibling comment re: Docker. The issue is imo that Docker is very easy to misconfigure and gives you the wrong mental model of how security on Linux works.\nOn SSH password auth: its secure if you use a long, random, not reused elsewhere password for every user. But it is also very easy to not do these things. SSH certs are just more convenient imo.\nreply\nblueflow\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nUsing docker does not help in this specific case - if the attackers came via ssh, they will have root access as before, and if they come in through the application, they still control your application inside the container and can make it serve what they want.\nFor ssh, the problem does not lie within password auth itself, but with weak passwords. A good password is more secure than a keypair on a machine whose files you can't keep private.\nreply\nwhyever\n7 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nDocker is not really a security boundary (unless you use something like gVisor), so it's a bit of a red herring here.\nThe idea is to make your app immutable and store all state in the DB. Then, with every deployment, you throw away the VM running the old version of your app and replace it with a new VM running the new version. If the VM running the old app somehow got compromised, the new VM will (hopefully) not be compromised anymore. In this regard, this approach is less vulnerable than just reusing the old VM.\nreply\nm00x\n4 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nThis is not money laundering. Why would they dispute if it's ML?\nreply\nbaobabKoodaa\n7 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nPlease explain the money laundering part here?\nreply\nmiltava\n5 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIm not op and I’m not sure they are using it for money laundering.\nA money launderer can use a marketplace by creating a seller account and buying from himself. Since he’s the one buying he doesn’t need to deliver anything but he gets the money from a legit source. Usually he would use a payment method as close to money as possible so that it leaves less traces. But in OPs case, the amounts are low so he needs too many transactions to get something valuable. And because of the disputes, he’s (probably) not getting the money (?).\nIt could be card testing: the fraudster has a bunch of cards and doesn’t know which is valid or canceled. The best way to find out is to test in a real site. So he’ll test out each of them and the ones that go through are good to use elsewhere. The thing is that it would be better for him not to dispute the transactions so the OP would take much longer to find out about the scheme and shut it down. It’s better to use low amount transactions in this case so it doesn’t use too much of the credit available for him to defraud and probably doesn’t warn the card owner.\nAnother option is doing it just to hurt the OP marketplace. If you have too many disputes the brands can fine you and if you don’t solve the problem they can turn your account off. I’ve seen it happen when a competitor was trying to hurt the e-commerce. It’s a low move and rare but it happens.\nOne thing that might help is to analyze the sellers too. In a money laundering and even in the other settings, it could be part of the scheme. Are they new accounts? Are their volume exploding out of nowhere? Etc\nreply\naddandsubtract\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> Since he’s the one buying he doesn’t need to deliver anything\nThis only works (in my mental model), when you produce the product you're selling in-house – like a digital product. But lots of \"reselling\" type businesses try to use this scheme as well. Like a restaurant might ring up more meals than they served, or less to not pay taxes. But, is this not easily spotted when the food import(?) cost doesn't match the revenue?\nMaybe I just answered my own question, if the business is able to cook the books both ways, but it would also limit how much they're able to launder. Or is the import/export balance rarely/never checked?\nreply\ngruez\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThat's why popular businesses for money laundering are car washes and nail salons. They're mostly cash based, and have very little in the way of inventory, so it's easy inflate your sales.\nreply\naddandsubtract\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nBut a car wash uses water and a nail salon hires workers. Shouldn't take long to check that those numbers don't add up with what was sold at the end of a month.\nreply\nbluGill\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nMaybe.  If you calim to wash a million cars but only wash a thousand that will be obvious, but 10 washes different is lost in the noise. Nail salons are easier because you can have the expensive personalized service that no real person buys but if someone investigates you will give it to them.\nMore likly the above are selling something illegal though.  Pay for the expensive hand car wash but get drugs instead with a cheap automatic wash - nobody will know the difference.\nFor higher valued goods they use horses.  A saddle can go for $30,000, so you buy some $1000 saddles and sell them for $30,000 and $29,000 worth of something else.\nreply\ndatavirtue\n2 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nThey will gladly send water down the drain if it threatens their enterprise. Besides, you have to be on the burner for huge crimes if law enforcement is going to care enough to audit water usage. Again, minor piece of circumstantial evidence in any case.\nreply\ntiahura\n2 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nThe gentleman who owns the nail salon in my Midwest suburbia strip mall drives a Lamborghini. One wonders about the immigration status and compensation structure of the nail techs.\nreply\npbronez\n6 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nMoney only has meaning as a flow. Value moves from A to B. Forensic analysis can follow this chain quite a long way, which is a problem for people trying to hide illegal activity. They're always looking for ways to break that chain. If OP is correct and this attack allows you to covertly shift money around, that can break the chain and let the bad guys use the illegally obtained funds with legitimate services.\nIt might look something like:\n1) get funds via illegal activity (dirty funds)\n2) spends funds at an ecommerce site (dirty funds)\n3) secure a paypal refund WHICH GOES TO ANOTHER ACCOUNT (clean funds)\nThe PayPal vulnerability allows the money to move from a dirty chain to a clean one.\nreply\nKomoD\n5 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIt wouldn't go to another account if you do a dispute, what are you talking about?\nreply\nm00x\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYeah I work at a large US fintech and this isn't ML\nreply\nhigh_na_euv\n5 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n>2) spends funds at an ecommerce site (dirty funds) 3) secure a paypal refund WHICH GOES TO ANOTHER ACCOUNT (clean funds)\nHow it breaks the chain?\nAccount1 buys for 10k USD, requests refund, receives it?\nEven if it went for some reason to account2 then there is still the chain, but why would it go to other?\nreply\ntyingq\n7 hours ago\n|\nprev\n|\nnext\n[–]\nYou can configure your account to reject unverified buyers.\nhttps://www.paypal.com/us/cshelp/article/what-are-payment-re...\nreply\nbobbiechen\n1 hour ago\n|\nparent\n|\nnext\n[–]\n+1. As mentioned on the side, this will negatively impact your conversion rate. But you don't need to leave it on forever, either; you can use it to get some breathing room.\nThe attacker may lose interest or move on to more fruitful targets if they find themselves blocked even temporarily. This is the \"don't need to be faster than the bear\" dynamic of online fraud: there are infinite targets and you don't need to perfectly shut out an attacker to make the ROI unappealing for them.\nMy thoughts on the scenario:\n1. Chargebacks are not just a financial problem. There is no amount of money you can pay to regain the trust of your sellers (as it's a marketplace) or the\n2. If the emails come from the same domain, can you block the domain? There are lots of throwaway domains, but it's effort for the attacker to switch them, too.\n3. CAPTCHAs are increasingly ineffective between captcha solving services and multi-modal AI. I've heard in a few recent attacks that hCaptcha does a little better than Turnstile or reCAPTCHA.\n4. Shadowbanning is good for wasting your attacker's time, which is really important to kill their ROI. You'll need to get your false positive rate low though to not piss off your actual good customers.\n5. Your scenario (no API, browser required, no bot activity expected) is a really good fit for properly implemented device fingerprinting.\nI'm the PM for Fraud & Security at Stytch and we do have a Device Fingerprinting product. It's harder to trial than the open-source ones, but the advantage is that attackers can't inspect the implementation to evade it.\nWould you be interested in talking more? I'm happy to walk through your current controls and see if it makes sense to test Device Fingerprinting, shoot me an email at (first letter of my username) + (last four letters of my username) @ stytch.com .\nreply\n_alternator_\n2 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nWhy is this not the top answer?\nreply\ntrollbridge\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nSome people want a high conversion rate, and before I flipped this on, something like ⅓ of my customers were unverified buyers.\nreply\nazemetre\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nHow many fraudulent charges did you deal with?\nreply\nabxyz\n9 hours ago\n|\nprev\n|\nnext\n[–]\nAssume that the transactions are coming from humans, it is often cheaper to instruct humans than it is to automate when there’s an expectation that you will try to mitigate the malicious behavior.\nBe willing to temporarily suspend your services in order to prevent the malicious behavior. Do the manual work to allow genuine customers to keep using your service, e.g: require manual account approval. You need to treat every one of these chargeback transactions as a risk to your businesses ability to operate, each that you allow to happen increases the risk of permanent damage to your business.\nReach out to your account manager at PayPal, this is not something that should be going via frontline support. You need to be talking to a person who knows and is responsible for your account. If you don’t have one, get one. If you can’t get one, look for anti payment fraud businesses that work with PayPal, they may be able to get a direct line to PayPal on your behalf.\nFor the future, if you’re dependent upon a service provider you should always have someone you can reach out to directly. If a provider isn’t willing to offer that, find a different provider. Financial services especially are very risk averse and will jettison your account if they get even a whiff of something untoward, whether you tried to prevent it or not. The cost of recovering from that will dwarf the cost of any drastic mitigation you take now. Losing your PayPal account is worse than turning off purchases for a few days.\nreply\nFoofoobar12345\n12 hours ago\n|\nprev\n|\nnext\n[–]\nThey are probably testing stolen/hacked PayPal accounts. Probably doing a dispute to ensure the owners don’t suspect anything is going wrong, until they use it for bigger transactions. Unfortunately with PayPal there’s no way to ascertain ownership of an account (like 3DS).\nThis used to happen to us, eventually after haggling with PayPay support for over a year on who should bear the cost, we just shut down PayPal payments. Don’t have anything better to offer, sorry.\nreply\nmrweasel\n5 hours ago\n|\nparent\n|\nnext\n[–]\nI haven't worked with online payments for a few years, so take it for what it is, but I'd agree. PayPal is possibly the worst payment solution, for the stores. Their support sucks and is completely unhelpful, managing your account was at the time extremely complex, compared any other payment solution.\nOur rule taking PayPal: Transfer EVERYTHING out of your PayPal account on a daily basis, do not let them hold your funds, they will block you from accessing it at some point. Minimize what they can touch.\nAlso don't all smaller amounts to be paid with PayPal. This prevents you from being abused as a source for verifying stolen accounts.\nThe only company I dealt with that came close to the same level of incompetency was Klarna. Klarna didn't at the time understand the concept of fraud, because they're Swedish and their system in Sweden MOSTLY prevented fraud at the time. Once people found away around that and Klarna expanded beyond Sweden, they gave up and attempted to stick the bill on us, despite their contracts clearly stated that they where responsible for collecting payments.\nreply\ntrollbridge\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThe one reason I still use PayPal is because the 5% + 5¢ for micropayments is the best deal out there if you're billing $1 or $2 transactions.\nI transfer all funds out on a daily basis.\nreply\nbusterarm\n1 hour ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\n> Our rule taking PayPal: Transfer EVERYTHING out of your PayPal account on a daily basis, do not let them hold your funds, they will block you from accessing it at some point. Minimize what they can touch.\nThat only works until your business is successful.  Once you reach enough transaction volume/dollars they will require you to float millions of dollars in your PayPal balance and not let you touch anything for 30-45 days after transactions.\nreply\nmaxclark\n3 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nMy immediate reaction reading the post was “don’t use PayPal”\nOnline marketplaces, multiparty sellers, credit card transactions, etc… are hard enough as it is\nDon’t become dependent on a vendor who’s absolutely terrible to work with\nreply\niforgotpassword\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIf you can pass on a chunk of customers sure. I've canceled a purchase more than once at checkout when I saw there is no PayPal available, if the website was unknown or looked a little shady, and I didn't desperately need the item. There are people who don't buy at all if there's no PayPal just because it's less convenient.\nreply\nalgo_trader\n9 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nAre there services that \"guarantee\" (or block) transactions for a fee?\nIn any case, this should be the primary responsibility of the payment service !! The fact it can so casually off load it to the merchants is just bizarre\nreply\nabxyz\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nGuaranteeing transactions would incentivize the provider to block transactions. There are many companies in the space, like sift.com, but they don’t guarantee.\nreply\nhalpow\n9 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\n> there’s no way to ascertain ownership of an account (like 3DS)\n3DS is 2FA and PayPal most definitely has it, it's just that they protect the customer regardless of 2FA.\nreply\nsky2224\n10 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nWhat have you switched to that isn't PayPal and also doesn't have this issue?\nreply\nA_D_E_P_T\n9 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI'm not that commenter but my business also moved away from PayPal and is using Stripe + Sezzle for transaction processing.  It has been about five years now without any issues at all.\nreply\n_QrE\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nEasy example is Stripe. You can enable 3DS, and you can listen for 'early_fraud_warning' events on a webhook to refund users & close accounts to avoid chargebacks and all the associated fees and reputation penalties.\nreply\nmrweasel\n5 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nPart of the problem is that not all countries have the same solutions, but credit/debit cards are an easy solution. In some countries that requires 2FA using a government issued ID. It's not 100% secure, people being people and doing stupid things, but it's better. If you're in the US, I don't know, it might not be better. If you can, ask your credit card processor to block cards that's not in the area you serve. E.g. we had huge success in blocking UK and US credit cards from our Scandinavian stores.\nIn Scandinavia there's also MobilePay, which is much much better, as it is also closely linked to real identities.\nreply\ntyfon\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> In Scandinavia there's also MobilePay, which is much much better, as it is also closely linked to real identities.\nDon't forget vipps, I think it also works in Poland now in addition to various nordic countries.\nreply\nnoodlesUK\n10 hours ago\n|\nprev\n|\nnext\n[–]\nSome of the other commenters here have reasonable mitigations. One word of advice - PayPal is ruthless about banning merchant accounts that it deems risky. You’d best sort this out quickly or have plans to be able to rapidly switch to another PSP. Even if your business doesn’t get banned, the multiparty vendors (or whatever the appropriate term is) might get hit.\nreply\ndabinat\n10 hours ago\n|\nprev\n|\nnext\n[–]\nI had a similar thing happen recently. Some of the IP addresses were proxy / datacenters but many of them weren’t, which made me think it might be a botnet. And the UAs were generic, so there wasn’t anything easily-bannable.\nI added fingerprinting and rate-limiting and the problem seems to have gone away. They’re trying to test a large number of accounts / credit card numbers so the best strategy is to slow them down to the point where it’s no longer worth it for them at scale.\nreply\njpalomaki\n4 hours ago\n|\nparent\n|\nnext\n[–]\nThere are many companies selling access to ”residential proxies”.\nreply\nikekkdcjkfke\n1 hour ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nWouldnt a \"service fee\" resolve this? A non refundable amount to even transact\nreply\ndatavirtue\n1 hour ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nI'm hot off fighting one of these bot nets. They automatically adapted and spread the calls over a ridiculous number of IPs and all had good JA4 fingerprints at Cloudflare (compromised or nurtured \"users\"). Gave us nothing to block. We started targeting high count JA4s and blocking those temporarily. This would usually cause them to stop automatically.\nVery sophisticated LLM-enabled rented mafia bot net. They crafted attacks of various approaches as we turned up the heat.\nIn the end we refactored our entire authentication flow. We had a lot of Anon endpoints and ones that would validate card numbers etc from past misguided product and management decisions.\nIn the end we had to block a lot of legitimate traffic at times.\nReducing friction for users reduces friction for scaled bot attacks.\nreply\nBeijinger\n5 hours ago\n|\nprev\n|\nnext\n[–]\nThis sounded interesting, provided here in the thread: You can configure your account to reject unverified buyers.\nBesides this: You can not build a long term business that relies on PayPal [or Amazon.]\nI would also try to attack the domains. Some strongly worded emails from a lawyer, report fraud at ICANN for the two domains.\nreply\nimdavidsantiago\n12 hours ago\n|\nprev\n|\nnext\n[–]\nSounds like automated chargeback abuse, maybe for card testing or just to exploit your payment/dispute setup. We’ve dealt with similar stuff.\nA few things that helped us:\n– Browser fingerprinting (FingerprintJS or even basic user agent + behavior tracking)\n– Logging full headers + TLS fingerprints — IPs rotate, but some other patterns leak through\n– Introduce small friction in the payment flow (e.g. lightweight CAPTCHA or JS challenge)\n– Look at timing patterns — automation tends to work in strict intervals\nPayPal support is notoriously slow for anything that’s not cookie-cutter. Try emailing merchanttechsupport@paypal.com — they’ve been more useful in escalated cases.\nThis kind of thing is more common than you’d think, especially for platforms selling digital goods.\nreply\naga98mtl\n4 hours ago\n|\nprev\n|\nnext\n[–]\nYou could take these type of orders as \"pending\" then require a SMS code to access the final payment page. Adding an extra step like this might discourage the attacker if their goal is not attacking you specifically. They will move on to another easier target.\nreply\nparasec\n2 hours ago\n|\nprev\n|\nnext\n[–]\nBeside all the helpful comments: If this is a serious problem for your business, invest in Cloudflare or other professional bot-protection. They do fingerprinting and similar stuff.\nAlso, if you implement your own methods, do shadow-banning of bots that you identified. These attacks will stop if the time and effort the malicious actor has to invest outweigh the benefits, so the more time and effort you let them waste, the better. A good example are unsolvable and ridiculously captchas. That is obviously a double-edged sword - you need a good way of whitelisting known good actors, so the effect of false-positives on your customers is minimized.\nreply\nshswkna\n1 hour ago\n|\nprev\n|\nnext\n[–]\nIf there is any way to avoid Paypal, and still continue your business viably, this would be my recommendation.\nPaypal is not a company that exists for its customers.\nreply\ntruesign\n1 hour ago\n|\nprev\n|\nnext\n[–]\nI created\nhttps://truesign.ai\nspecifically for this use case.\nIt detects bots, fake emails and proxies -- analyzing the network in realtime, no blocklists or IP reputation.\nIt's free during beta.\nreply\ntrollbridge\n4 hours ago\n|\nprev\n|\nnext\n[–]\nI operate a small not-for-profit site that has a (very inexpensive) subscription. To avoid this, I do a few things:\n- We have a no-questions-asked unlimited refund policy.\n- I don't tolerate unverified PayPal buyer purchases. However, if someone tries to buy with one, I activate the subscription, and then contact the buyer via the e-mail/phone number they signed up with, confirm they're a real person, and then send them a PayPal invoice.\n- Only subscriptions can be purchased.\n- We've configured the flow when using PayPal to not tell the user if a transaction is declined to the maximum extent possible. I.e., the subscription still gets activated and then we call the user to arrange other payment options.\nreply\nNkharrl\n2 hours ago\n|\nprev\n|\nnext\n[–]\nMy startup defends companies from exactly this. (www.specprotected.com)\nHappy to give guidance to a fellow startup - I know you're unlikely in a position to be able to pay for a solution.\nDigital goods, donations, ticketing, any sort of marketplace -- it doesn't matter your size, just having a merchant account they can transact against is enough motive for them.\nreply\njune3739\n1 hour ago\n|\nparent\n|\nnext\n[–]\nThanks, Nate. What's the best way to reach you?\nreply\nNkharrl\n1 minute ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYou can reach me at nate@specprotected.com - happy to be helpful\nreply\npaxys\n5 hours ago\n|\nprev\n|\nnext\n[–]\nAccept payments from verified accounts only. That's like PayPal 101.\nreply\nmlinhares\n1 hour ago\n|\nprev\n|\nnext\n[–]\nNot sure how paypal works but can you enable 3DS for this?\nreply\ngtech1\n15 hours ago\n|\nprev\n|\nnext\n[–]\nWhy not block those 2 domains from signing up ?\nreply\njune3739\n14 hours ago\n|\nparent\n|\nnext\n[–]\nThey're popular enough that we'd penalize a substantial number of users.\nreply\nmiyuru\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nDoes the email address has a pattern? I faced similar registration attack, but the email address had pattern, I blocked them in code but gave a success response and the attack went away.\nreply\nImustaskforhelp\n11 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nLet me guess, protonmail and tutanota?\nreply\nmschuster91\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nGoogle Mail is also a very popular source of spam these days...\nreply\nmjburgess\n7 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nCould you add some additional check if that domain is used? (Possibly with browser fingerprinting, or other req fingerprinting)\nPossibly something even that just wastes a little time and makes them know you're aware of the behaviour.\nreply\ncultofmetatron\n3 hours ago\n|\nprev\n|\nnext\n[–]\nI wish people would stop using paypal. they work ok (in my case, 20 years) until one day they dont and you find yourself locked out of your account and no one can help you.\nreply\ncryptonym\n9 hours ago\n|\nprev\n|\nnext\n[–]\nSolutions include TLS fingerprinting, browser fingerprinting, behavioral scoring, IP reputation, captcha/crypto challenge...\nIf you are on a premium CDN, they are probably equipped and can provide security consulting. If not, you may want to switch vendor or buy a separated bot detection solution.\nreply\nkolp\n9 hours ago\n|\nprev\n|\nnext\n[–]\nThese Cloudflare WAF rules (not my creation) should help mitigate some of the threat by blocking TOR traffic, blocking bots and blocking datacenter IPs (eg bots running on a VPS). The rules are granular so you can tweak them when you start to identify the traffic sources of the bad actors.\nYou'll probably need to block entire ASNs. I assume most of your legitimate customers aren't using VPNs or eg DigitalOcean droplets to access your site.\nhttps://webagencyhero.com/cloudflare-waf-rules-v3/\nIn addition, you should start looking for alternatives to PayPal in case they decide to drop you.\nreply\naitchnyu\n5 hours ago\n|\nparent\n|\nnext\n[–]\nDo western services ever offer two payment gateway options to the customer? Its common in India.\nreply\ntoast0\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYes. I buy things all the time from smaller vendors that support PayPal, Amazon Pay, maybe google/apple, as well as a direct credit card entry.  Actually, even many large vendors offer a selection of payment options; I pay Walmart and I think BestBuy with PayPal because it's got my card saved and I don't want to get up and grab my wallet. PayPal has big issues if you're a small seller, but as a purchaser it's convenient.\nI don't think I've seen vendors offer a choice of two different merchant accounts, but some do have multiple merchant accounts and select one or the other at time of billing; sometimes you can tell because it shows up a little different on the bill depending on the path, or more often because they send an announcement about trouble with billing and mention that it only affects some customers because they have two accounts.\nreply\nMagicMoonlight\n4 hours ago\n|\nprev\n|\nnext\n[–]\nDo you have any competitors in your area of business? I’ve heard of people doing this to wipe out a new entrant. Every chargeback costs you money and you eventually will get blocked from taking payments.\nreply\ntoomuchtodo\n15 hours ago\n|\nprev\n|\nnext\n[–]\nIs Turnstile an option to try to dissuade the bot traffic?\nhttps://www.cloudflare.com/application-services/products/tur...\n(no affiliation)\nreply\njune3739\n14 hours ago\n|\nparent\n|\nnext\n[–]\nYes, we added Turnstile to checkout and they were able to get past it. We assume it's either because Turnstile sometimes uses a pure-JS approach (no interaction) or they're using an AI to drive the browser and it was able to figure it out.\nreply\nwut42\n5 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nOr just a solver API like 2captcha.\nreply\nchristophilus\n8 hours ago\n|\nprev\n|\nnext\n[–]\nWe had a similar issue and rate limiting + IP blocking did the trick. You don’t have to solve the problem completely; just make yourself a less desirable target than your competitors.\nI’d love to hear what you end up doing.\nreply\ntrollbridge\n2 hours ago\n|\nparent\n|\nnext\n[–]\nI did the same thing with geo IP blocks + blocks of non-consumer IP ranges. I don't completely block the transaction - I just send them into a different workflow where we manually call them to run the transaction. This works fine for legitimate customers.\nreply\njune3739\n42 minutes ago\n|\nprev\n|\nnext\n[–]\nThank you, everyone. I want to start by saying how reassuring all of these comments, feedback, and support are. We've spent the past few days feeling very alone in this situation, unsure of why it was happening and whether our approach of adding friction really was the best option. We had a brief period where we wondered if we were being singled out by competitor! Knowing that this is just a thing that happens, especially with PayPal, is reassuring and helps ground us.\nI can't respond to every comment right now because we're actively dealing with it. There were more attempts this morning. Some quick replies to some of the frequent comments:\n* We're on a paid Cloudflare plan. We upgraded to the ~$2500 after this started and added a lot of filtering rules and interactive challenges to some key pages. Because purchases are either browser automation or humans, this has only been somewhat effective at filtering out bad traffic.\n* IP checks show a mix of proxy/VPN and not. Blocking at the IP or ASN level won't get us very far.\n* PayPal's Marketplace \"platform\" (it's a few APIs) processes orders through each of our sellers' accounts. As a result, we can't prevent purchases from unverified accounts because that has to be done by each seller.\n* Moving off of PayPal isn't possible. For a marketplace platform in the US, the only other real option is Stripe Connect, but our domain has a lot of micro-transactions and Stripe's $2 per month per active user is a nonstarter. We experimented with Stripe and users (esp casual sellers) found their onboarding so intimidating that we lost signups. We would love other options, we have great concerns about PayPal as a longterm partner.\n* Blocking the domains the purchases come from is not an option. They are recognizable names used by more legitimate users than illegitimate. We are adding extra scrutiny to these checkouts but we think it's possible they'll change tactics if they know we're onto that.\n* Thank you for the fingerprint suggestions. We are going to try Fingerprint Pro.\n* We've been gradually increasing friction via automated challenges and blocklists. We will increase this with more invasive Captchas, especially when aspects of the sale match criteria.\n* We built an \"Under Attack\" mode that we can enable to completely disable key areas. We are prepared to temporarily shut down all sales if need be.\n* We blocked prepaid credit cards from signing up for our subscriptions. This is a separate vector and we've had a few people try this over the past year. There was at least one person who did both the PayPal fraud and a signup scam + AI content. This should cut that off.\nAgain, thank you to everyone for the advice. We're monitoring this post closely.\nreply\nKennyBlanken\n2 hours ago\n|\nprev\n|\nnext\n[–]\nThis is not a technical matter. This is a legal matter. Sue the party as John Doe/Janes for business interference and fraud, and get records from PayPal, their ISP/phone provider, etc in discovery.\nAlso, have your attorney send a polite letter to Paypal's legal department.\nI'd place good money on this being a competitor trying to sink your merchant account by racking up a lot of fraudulent transactions.\nreply\nherbst\n10 hours ago\n|\nprev\n|\nnext\n[–]\nNot the easiest solution but I would suggest not using PayPal. Much more issues there than just using credit card, and as you noticed nobody there to care.\nWait until they ban your account and there again is nobody to talk to.\nreply\n0xEF\n9 hours ago\n|\nparent\n|\nnext\n[–]\nWhat's the best alternative?\nIt's easy to say \"don't use PayPal\" but if you're going to say it, you need to do the hard part of suggesting a viable alternative for eCommerce that has as broad a reach and acceptance as PayPal. Stripe? Almost none of the outlets I do business with use it. Venmo? Same company as PayPal. Back to using credit card numbers? The more we spread those around online, the higher the chance they get stolen and used, probably in refund scams like the one OP describes.\nPeople need an alternative with some degree of trust and most consumers, by my reckoning anyway, would prefer a single entity that is accepted everywhere. Right now, that's unfortunately PayPal.\nreply\nherbst\n8 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nPersonally I've had way less issues with stripe, especially in terms of fraud detection.\nAlso not sure what and where business is but in Europe it's common to just use a proxy provider where credit card is just one of many options and you use a central gateway (similar to stripe)\nYou'd have to check your local options. At least one of my local banks offers something more advanced than PayPal. And there are several of these proxy providers in my country.\nEdit:// if you just want low fee, fast and risk free transactions we all know there is only crypto\nreply\nK0balt\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nYeah. It’s ridiculous how crypto gets so much hate here, yet there are constant, often heart wrenching posts about the failures and even near-malfeasance of payment processing systems. The paradoxical claims that crypto has no use case except fraud and crime, juxtaposed with the lamentation and gnashing of teeth over the misery of traditional payment systems is enough to provoke an existential examination of the senses.\nI honestly think it must be mostly sour grapes, since by far, cash and other traditional payment methods facilitate the vast, vast majority of crime and fraud, and cryptocurrency is the only universally accessible, trustless, (nearly) costless, instant, global system for the transfer of value between two parties.\nIt is by far a better system, even with its flaws. Which is why, yes, many criminals use it, just as they use cash. Because it works.\nreply\nasterix_pano\n6 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nI am also amazed by the resistance to change here as there is a system clearly more efficient and transparent. It's just a matter of time in my opinion.\nBTW you can remove the (nearly) in \"(nearly) costless\", some solutions provide 0 fees and no inflation.\nreply\naxelthegerman\n6 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nBy more efficient you mean taking hours to settle? And no inflation as in Bitcoin level volatility and up?\nI'd love an efficient and cheap option to move funds online - especially for micro payments too. But so far I haven't heard of any crypto option that actually stayed around long enough to prove these things.\nHappy to be pointed in the right direction here.\nreply\nK0balt\n5 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nStablecoins on any number of low cost networks are fine for small payments (I haven’t lost anything, not even $0.01 in 10+years, vs tens of thousands in chargebacks, mostly fraudulent, for credit card processors).\nI factor in 2.5% total costs for transaction frictions, historically that is a bit over 3x our actual average cost from payer to bank account, but it would easily cover the occasional loss of a day or two of sales in a catastrophe.\nPick a top 5 stablecoin that has a good reputation and at least 3 years, on a network with at least that, and  settle your accounts daily, or whenever the accumulation represents a significant dent if lost.\nThe approximate aggregate risk-cost of major (top 10) stablecoins is somewhere south of .001% per day, and is better than the aggregate risk-cost of national fiat  currencies, which unremarkably collapse or suffer catastrophic inflation and rebasing on a regular basis. There are frequently several undergoing this process at any given time.\nreply\njcalvinowens\n4 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> The approximate aggregate risk-cost of major (top 10) stablecoins is somewhere south of .001% per day, and is better than the aggregate risk-cost of national fiat currencies\nThis thinking is dangerous and stupid. Learn from history:\nhttps://en.m.wikipedia.org/wiki/Black_Wednesday\nThis \"stablecoin\" garbage needs to die yesterday: a lot of people are going to lose their shirt when the first one blows up. Fixing exchange rates is folly, yet here we go again...\nreply\nK0balt\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n>>This \"government issued fiat currency\" garbage needs to die yesterday: a lot of people are going to lose their shirt when the first one blows up.\nWhat you are saying is a risk endemic to all fiat currencies, including stablecoins.\nAll symbolically represented forms of value quantization are subject to a failure of confidence. Cryptocurrencies are nothing new in this regard. All money is memetic in nature.\nreply\njcalvinowens\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThat's like saying \"base jumping isn't really more dangerous than flying commercial, after all we're all going to die anyway\".\nFiat currencies have militaries. Your stablecoin doesn't.\nreply\nK0balt\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThat’s one of the reason the stablecoins won’t be taking my assets? Idk what your point is but it doesn’t seem like you are debating from a point of rational examination.\nWeird, people on the internet spewing BS? Who’d have thought?\nreply\njcalvinowens\n57 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nWell, losing three months of revenue is going to really hurt when the stablecoin inevitably eats shit: hope you're prepared for that.\nThe risk is obviously lower because you aren't parking money there. I could certainly see how you might come out ahead in fees for certain international transactions.\nBut your original claim was that the aggregate risk-cost of dealing in stablecoins is lower than real currencies, and that is absolutely preposterous: you aren't accounting for all the risks.\nreply\nK0balt\n3 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nWhy would I care if a stablecoin blows up? My payment cost allocation more than compensates for that possibility and my losses in a worst case scenario would be eclipsed to oblivion by the cost savings I have already realized.\nreply\njcalvinowens\n3 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> my losses in a worst case scenario would be eclipsed to oblivion by the cost savings I have already realized.\nPlease elaborate :)\nreply\nK0balt\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nMy theoretical potential losses compared to the costs of the payment processsors I ditched, and the chargebacks we used to deal with.\nInternational payment processing is quite expensive, both on a teansaction and on an administrative basis.\nMy worst case total risk exposure is approximately the same as the cost of 3 months of payment processing overhead, without counting fraudulent chargebacks and “we are going to freeze your account because we can” risks.\nFWIW  in the last 60 years I have lost way more money to fraud and theft dealing with banks and cash then I ever will using cryptocurrency. On a total, or a percentage basis.  I see the risk profile, when properly managed, to be much, much lower using blockchain solutions.\nreply\njcalvinowens\n1 hour ago\n|\nroot\n|\nparent\n|\nnext\n[–]\n> My worst case total risk exposure is approximately the same as the cost of 3 months of payment processing\nOkay, yes: what you're describing is the actual utility of these things.\nI think you underestimate how many people dealing in them are using them much less intelligently than you are.\nThey are being marketed in an extremely dishonest way, as a safe long term store of value. I regularly overhear normal people at my local bars talking about how they're \"investing big in stablecoins\" and it terrifies me.\nreply\npessimizer\n6 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nNobody claims that crypto doesn't have a use case. They claim that it has failed at its use case. You can't use it to buy things easily or safely. Buying a bunch of crypto on the internet and looking at a graph every day hoping the line goes up isn't useful.\nIts use case is still fraud and crime: when laundering money or fencing stolen goods, you expect the process to be difficult, dangerous and to have to pay a large fee. Crypto is a clear improvement on older criminal methods. It's not an improvement on credit cards.\nreply\nK0balt\n6 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIdk, my experience is that it works fine.\nEven to the point that the cost of maintaining other payment systems was less cost effective than just dropping them and focusing on crypto only. FWIW we are not a “crypto” business, our focus is kinodynamics.\nOur market is global and we are small from a payment perspective in this context, so our case may be a bit of an outlier, but our lived experience does directly contradict your claim.\nI agree that speculation and crime is a problem, but the speculation market in global commerce and currency dwarfs crypto, así does the criminal usage of cash, so it’s disingenuously myopic to frame those as a crypto-centric issue.\nThe relative difficulties of regulation because of the decentralized nature of crypto does make it a hotbed for schemes that wouldn’t be practical under local regulations, but I have a hard time getting riled up about that when we have giant state sponsored gambling industries nearly worldwide.\nIf you were to compare the impact on criminal activity if you eliminated cash vs eliminating crypto, I think it’s easy to see that eliminating cash would be much more detrimental to criminal activity.\nIn all, the arguments being made are not at all based on a rational examination of verifiable ground truths. They are almost to a fault emotionally based arguments with a near hysterical pitch woven into them from the start.\nIt seems like some people fear crypto. I don’t know why, but they do. On some level, they fear the threat it poses to the devil they know, perhaps. Maybe that is why they react the way they do.\nreply\ncess11\n8 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nWhat's wrong with Trustly, Adyen and others among the ~40 alternatives we maintained integrations with at the casino operator I was working at almost ten years ago?\nreply\n0xEF\n7 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nHaven't heard of them at all, that's what's wrong.\nThey may be great services, of course, being that I, a single consumer, am not a barometer for the success of a payment platform. But whoever they are, they're not being used by major retailers, distributors or manufacturers that I shop with both personally and professionally.\nreply\ncess11\n9 minutes ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nIIRC Ebay uses Adyen. I think Paypal uses Trustly in some significant markets.\nI don't know why they didn't show up early in your research, as they are among the most well known and easy to find. Ingrid is lesser known and mostly active in e-commerce in the european markets.\nreply\ndisgruntledphd2\n4 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nAdyen are everywhere for in-person payments where I live (Ireland).\nWorldpay/Stripe seem to be the most common providers for ecommerce.\nMore generally, payments are painful so finding a good provider is very, very important.\nreply\nmrweasel\n5 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\n> Wait until they ban your account\nBan your account and prevent you from accessing any funds in that account.\nreply\npalmfacehn\n6 hours ago\n|\nparent\n|\nprev\n|\nnext\n[–]\nI don't think you can manage payments between multiple users with most card processors?\nreply\nrendall\n12 hours ago\n|\nprev\n|\nnext\n[–]\nWhat would happen if you ignored it? That's the basis for figuring out next steps.\nIt sounded from what you wrote that it will not affect your relationship with PayPal, because they are asking your sellers to contact them individually, and it's distributed across all of your sellers, so it won't affect their relationships either? Did I read that wrong?\nreply\ndatavirtue\n3 hours ago\n|\nprev\n|\nnext\n[–]\nCloudflare bot protections and enhanced WAF rate limiting rules. Go.\nreply\nxyst\n3 hours ago\n|\nprev\n|\nnext\n[–]\nShould be some settings on PayPal to refuse payments from new accounts. Or switch to Stripe which has much better \"anti fraud\" mechanisms in place.\nreply\nmattl\n15 hours ago\n|\nprev\n|\nnext\n[–]\ndo you have the user agent string of their browser?\ndid you look up the AS number of the IP addresses they're using?\nreply\njune3739\n14 hours ago\n|\nparent\n|\nnext\n[–]\nYeah, the UA is pretty consistent but very generic. It reads as a desktop browser.\nWe did not look up the AS number. Can you describe that we'd be looking for there? Based on how the address was changing, I assume they're using Tor or some kind of VPN that will obfuscate IP so I didn't spend much time looking at them.\nreply\nbruceallmighty\n10 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nTry running some of the IPs through a proxy detection API like\nhttps://ipinfo.io/products/proxy-vpn-detection-api\nor\nhttps://proxycheck.io/\nYou can't trust those services 100% but you can use them to turn up the level of turnstile/captcha/verification on those clients.\nI'm somewhat concerned that you don't know what you'd be looking for (or to verify Tor) if you're running an ecommerce platform, fraud is an almost certain outcome for any store and merchant providers (Paypal, Stripe, Adyen, etc) want zero to do with helping you solve that (even if you're only embedding their Javascript!)\nreply\nprotocolture\n12 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nIf the ips all belong to a single AS you could look at blocking just that traffic, or make a complaint to the AS.\nYou could also gather geolocation data from the ips and block commonalities.\nreply\njonasdegendt\n10 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nWhat do the IP addresses belong to? As in, are they data center IPs, or residential addresses?\nConsider blocking all of Tor IPs, known data center ranges and the likes.\nreply\ntallytarik\n9 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nYou can look up the AS and other info, like detected proxies or VPNs, using the form on\nhttps://iplocate.io/what-is-my-ip\n(disclaimer: I've run this service since 2017).\nIf they come from a consistent AS, you can block the AS. If they're using a proxy or VPN, you could try blocking those. If you don't expect to get traffic from hosting providers, you can block where `asn.type == 'hosting'`.\nreply\nmattl\n11 hours ago\n|\nroot\n|\nparent\n|\nprev\n|\nnext\n[–]\nYep or consider just blocking AS numbers of places people typically aren’t purchasing things from such as cheap VPS companies.\nThe user agents, can you post those?\nreply\nlun4r\n8 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nCheck if the client sends the \"Accept\" and/or \"Accept-Language\" header. Or check if the order of request headers matches what would be expected from that generic User Agent. You'd be surprised how often they fail to send \"Accept-Language\", while every \"normal\" browser does.\nreply\nwhalesalad\n4 hours ago\n|\nprev\n|\nnext\n[–]\nswitch to stripe? ditch paypal altogether?\nreply\njoering2\n2 hours ago\n|\nparent\n|\nnext\n[–]\nI can promise you that's not a better solution. I witnessed a card testers smarter than Stripe tech and Stripe shutting down account that had literally 0% chargeback ratio and doing over $10MM in sales monthly. Still to them that was peanuts. They will help you initially and customer support is pretty fast at figuring things out, but in no shape of form will they say \"gee, we see you are not the bad guy and someone else is doing that so your account is good to continue abusing our merchant accounts\". They. Will. Shut. You. Down. Because they care about their rates of CB more than about your business. Just 2c, based on experience.\nreply\ndatavirtue\n2 hours ago\n|\nroot\n|\nparent\n|\nnext\n[–]\nThis. Stripe is too big.\nreply\nnprateem\n10 hours ago\n|\nprev\n|\nnext\n[–]\nI just tried fingerprintjs. Got different IDs on the same browser in normal & incognito mode. Doesn't seem like any help at all.\nKnown issue apparently:\nhttps://github.com/fingerprintjs/fingerprintjs/issues/1088\nreply\ndatavirtue\n2 hours ago\n|\nparent\n|\nnext\n[–]\nYou are supposed to get a different ID in incognito.\nreply\nnodesocket\n10 hours ago\n|\nprev\n|\nnext\n[–]\nAs others have suggested, implement a captcha if you aren’t already. Implement Fingerprint.js and see if you can spot some patterns you can create firewall or application rules to block. Finally, not optimal but migrate off PayPal.\nreply\nregisteredcorn\n2 hours ago\n|\nprev\n|\nnext\n[–]\nI mean this sincerely:\nYour company should pivot into competing with PayPal. You've identified profound deficiencies in how they operate, know what type of services customers value, and have motive: someone is attacking your business and PayPal can't even comprehend that there is a problem, let alone protect you from it.\nMore than that, there are vast swaths of people that have horrendous horror stories about dealing with PayPal, having their accounts shutdown without explanation, being abused in the same or similar ways, and a wide variety of other concerns. There is a market for it. You just need to consider what made you go with PayPal Multiparty over whatever competition exists.\n>We're struggling to find the motive or intended outcome by the attacker(s).\nUnless you plan to sue, determining motive probably doesn't matter a whole lot. We could guess at different reasons, and even if we figured out a good one, it wouldn't change\nwhat\nis happening, just\nwhy\nit's happening. That's not much of a meaningful change.\nreply\nredcobra762\n4 hours ago\n|\nprev\n|\nnext\n[–]\nPayPal terrifies me; I would move off of their platform ASAP, or at least be ready to when they inevitably pull the \"lock your account pending an investigation\" move that kills so many new companies.\nreply\nfiresteelrain\n11 hours ago\n|\nprev\n[–]\nYou could implement FingerprintJS [1] or even implement email or phone verification before allowing purchases for unverified PayPal accounts or implement some transaction frequencies per IP address. With FingerprintJS, it can basically create a unique ID per user and mitigate the behavior you are seeing and block them or add in additional countermeasures like 2FA.\nreply\nGuidelines\n|\nFAQ\n|\nLists\n|\nAPI\n|\nSecurity\n|\nLegal\n|\nApply to YC\n|\nContact\nSearch:"
  },
  {
    "title": "Writing a postmortem: an interview exercise I like (2017)",
    "href": "https://www.danielputtick.com/writing/mapbox-postmortem-interview.html",
    "content": "Dan Puttick\nAbout\nWriting\nGithub\nWriting a postmortem: an interview exercise I really like\n2017-10-31\nFirst, some exciting news that’s relevant as context: starting in November, I’ll be joining\nMapbox\nin their Washington,\nDC\noffice. One thing that made me excited about joining Mapbox was how much I enjoyed their\ninterview process\n. No part of it felt like it required extra “preparation” and it was clear that some significant thought had been put into its design. In my opinion, this is a pretty useful signal about an engineering organization. If we accept that finding great engineers (by whatever definition of “great” you subscribe to) is one of the largest contributors to the success of any company, it’s possible that a company with an interview process that appears disorganized or nonsensical is otherwise well-functioning, but it’s probably at least somewhat less likely.\nOne aspect of the Mapbox interview process that I particularly liked was their request that I write a blameless postmortem as a take-home exercise. If the term is unfamiliar, postmortems are a tool designed to help facilitate a culture of building institutional knowledge and learning from the past. Whenever something goes wrong – an outage, bug in production, failure to meet an\nSLA\n, etc – anybody involved in the situation can call for a postmortem. The postmortem takes the form of a shared document where everybody can contribute their account of the incident to help identify its ultimate causes and propose changes to prevent it from happening again. The “blameless” aspect is crucial: a good postmortem avoids conclusions like “Dan wrote a bug and it brought down our service” and instead says “Dan wrote a bug and it brought down the service: we need to improve our testing and deployment processes to make sure that they catch this category of bugs in the future.” For many mistakes that initially look like they ought to be blamed on an individual, it’s possible to identify a deficiency in a process as the root cause. You can read more about blameless postmortems in the\nGoogle\nSRE\nbook\n.\nI thought this exercise was a great interview question because it lets the reader learn more about several different attributes that one might associate with good programmers:\nIt’s a sample of the type of written communication you’ll be doing frequently in a work environment. Between documentation, taking notes while working, design documents, commit messages, chat, and email, programmers can produce significantly more prose than  code. Being able to express technical concepts clearly, unambiguously, and succinctly is essential. I may be wrong, but I sometimes get the feeling that this is widely acknowledged, but without the accompanying recognition that writing is a skill that can be improved relatively easily. Producing excellent writing might take a lot of work, but learning to write\ndecently\nisn’t that different from improving as a programmer: it takes building up experience, practicing deliberately, taking the time to revise repeatedly, and getting constructive feedback.\nIt’s a great way for the writer to demonstrate their ability to assume the perspective of others. So much of being a good engineer is having empathy for your users and the other programmers who will read, modify, or interact with your code. Writing a useful postmortem necessarily requires empathy, because you have to put yourself in the shoes of everybody involved in the incident and understand what they were thinking and why they took the actions they did.\nIt helps assess the writer’s ability to think critically and logically about a complex chain of events. Imagining all possible contributing factors to an incident and identifying the various links of causality is the same type of thinking as is required when trying to think of edge cases or identify the tradeoffs and compromises in a system’s design. Finding ultimate rather than proximate causes, dismissing alternate explanations, and considering counterfactuals are all closely related to the type of critical thinking involved in debugging software or troubleshooting systems.\nWhen I sat down to write my postmortem, I wasn’t sure whether to analyze a programming-related incident or choose something else. The instructions said the postmortem could be about anything I liked, and I decided it might be fun to write about a certain eventful and unusual accident that happened to me a few years ago. What I wrote is definitely not perfect, it’s just one example, and there’s no one “right” way to format or organize a postmortem. Anyway, I’m excited about the new job, and feel free to reach out if you’re curious about the Mapbox hiring process or what it’s like to work there (you should probably give me some time after I’ve started for the latter). Here’s what I sent the hiring team, unedited, in full:\nBackground\nI chose this incident by thinking, “what’s the most memorable unfortunate thing that I’ve been involved in over the past few years?” For context, in 2014 I had purchased an old, smallish (28 foot) sailboat. At the time I had some extra savings, and decided that it would be a enjoyable way to pass the summertime in Boston. I stored the boat on a mooring near Boston Harbor and sailed it frequently in 2014 and 2015.\nOne weekend in October 2015, I decided to sail to nearby Cohasset Harbor by myself, with plans to anchor there and spend the night onboard. Sailing alone entails a higher workload, as well as having no assistance if something goes wrong. However, I knew the boat well, and I had been sailing on my own frequently that summer. In the morning, I checked the weather forecast carefully. Although the breeze was forecast to increase the following day, everything looked well within the limits of my comfort level and ability. The journey to Cohasset was peaceful and beautiful, with light wind and plenty of sunshine.\nThat night, I slept somewhat fitfully. Sleeping at anchor is generally nerve-wracking; there’s a constantly lingering worry of being woken by the boat bumping into something because the anchor has come loose. It also takes some adjustment to sleep with the motion of the boat when you’re used to a bed on firmer ground. I woke up to a gentle rain as the sun came up, which gave way to a gorgeously thick bank of fog. I spent the morning reading a book and dozing, waiting for the fog to lift. After eating lunch, it was time to head home. The wind had started to build a little, and a quick check of the forecast told me it was going to continue to strengthen. However, the journey back was only two hours, the wind direction was right, and most of it was in sheltered waters.\nThe incident\nAbout half of the way home, the wind had built significantly. It wasn’t strong enough to make me feel in danger, but the boat was going at its maximum speed and required constant attention. Suddenly, an especially strong gust hit, and I heard a clank near the mast. I saw that a shroud, one of the metal cables that connect the the mast to the deck and hold it upright, had detached and fallen. Thinking back, I remember feeling more surprised than afraid. After all, I thought, the mast was still secured in several other places. However, I was quite close to shore, and I knew I would soon need to turn to avoid an area of shallows. Then, the wind started to change direction, causing me to panic and make the turn. This was a critical mistake. The change of wind angle altered the forces on the mast, and, in what felt like slow motion, the mast snapped in two at its middle and fell overboard.\nAftermath and response\nAfter recovering, I realized that I needed to take action immediately. I ran to start the gasoline engine, hoping to move to somewhere more sheltered to anchor and buy myself more time to solve the problem. However, if any of the wreckage tangled in the propellor, I would be in deep trouble. Luckily, the engine worked. Fighting the wind and current, the boat moved along at barely one mile per hour. After a painfully long 45 minutes, I finally managed to put the anchor down. A group of good samaritans on another boat saw what had happened and offered their help. Together, we pulled the broken mast and sails out of the water and tied them to the deck. Eventually, I used the engine to travel the last half hour back to my mooring.\nUltimate causes\nThere were multiple root causes of this incident. Outside of extreme weather conditions, a shroud detaching from the mast is a very low-probability event. The cables themselves are designed to withstand forces far beyond their normal working load. They are secured at both ends by thick steel pins, which only break when severely corroded. Unfortunately, it is difficult to visually inspect the end that attaches to the mast. Most sailors check them when they can, often during the winter when the boat is on land for storage. The rigging on the boat had been completely redone by a reputable contractor in 2013, and visually appeared to be in perfect shape. As a result, I hadn’t had it inspected since I bought the boat. It is probable that, either due to bad luck or error on the part of the contractor, the steel pin managed to wiggle itself loose. Vibration from the gusty winds probably accelerated the process to completion. The entire incident could most likely have been prevented by having the rigging inspected when I purchased the boat.\nAlthough the shroud coming loose was a serious failure, it was not by itself enough to bring down the mast. Appropriate and rapid action on my part would have preserved it. If I had quickly dropped the sails, the pressure on the mast would have eased, and I could have used the engine to return home. Although I had a significant amount of sailing experience, much of it was in smaller boats, and relatively little of it was alone. I did not have enough practice dealing with serious equipment failures at sea on my own, or generally in making decisions under time pressure with significant consequences. My initial reaction to the incident was insufficient recognition of its severity, as well as a lack of awareness that I had more time to assess the situation than I thought. A contributing factor was a lack of sleep from the previous night, which impacted my ability to think clearly.\nAnalysis and prevention\nThe first question, and probably the factor I had the most control over: should I have been out there on my own in those conditions? Given that I was alone, should I have waited for the wind to abate? I’ve thought about it many times, and I think that my decision was ultimately the correct one. You don’t learn or improve without pushing yourself towards the edge of your comfort zone, and one reason I purchased the boat was to improve as a sailor. Also, there are always numerous low-probability things that can go wrong. Being cautious and preparing adequately only serves to lower those probabilities, not eliminate them entirely.\nI learned several valuable lessons from this incident. When facing situations where you depend on your equipment, it’s worth spending the time and whatever resources you have to ensure that all of it is in peak condition. A few hundred dollars spent on a rig inspection could have prevented the entire incident in the first place. Also, I’ve heard repeatedly that it’s often the second failure that gets you into trouble, not just the first one in isolation. For example, if I had run out of gasoline, or if my engine had failed, the situation would have gone from just a broken mast to putting the entire boat and my personal safety at risk. The compounding nature of failures increases the importance of maintenance for every critical component.\nI also learned that making good decisions under pressure is a skill that can be improved like any other. This was one of the few times in my life so far where I’ve had to make decisions in seconds with serious consequences. My inexperience in those situations led to a few sub-optimal choices. However, I’m proud of the fact that I successfully executed the correct series of actions under pressure after my initial mistake. I now recognize the importance of practicing making decisions in lower-stakes scenarios to better cope with higher-stakes situations when they occur.\n© Dan Puttick, license\nCC BY-NC 4.0\nunless otherwise noted."
  },
  {
    "title": "Interactive MissileMap",
    "href": "https://nuclearsecrecy.com/missilemap/",
    "content": "MISSILEMAP\nby\nAlex Wellerstein\n, 2017-2019\nHosted by the\nCollege of Arts and Letters, Stevens Institute of Technology\nLaunch site\n:\n0,0\nTarget\n:\nnone\nDistance\n:\nN/A\nSSPK\n:\nN/A\nRange\n:\nkm\nYield\n:\nkt\nAnimate CEP\nshadow\nCEP\n:\nm\nBlast damage display\n:\nLaunch preset:\nMissile preset:\nAbout\n|\nFAQ\n|\nTwitter\nToggle high contrast map\nPermalink to settings\nExport to NUKEMAP\nLocation lookup\n×\nLatitude and longitude (decimal coordinates, separated by a comma):\nUse coordinates\nOr type in the name of a place, and a GeoCoding service will be used to look it up:\nLook up location\nSingle Shot Probability of Kill calculator\n×\nThe\nSSPK\n(Single Shot Probability of Kill) is the probability that a single launch will completely destroy a given target.\nCircular Error Probable*:\nm\nAccuracy of weapon\nWeapon yield*:\nkt\nExplosive power of weapon\nBlast pressure required*:\nBlast pressure required to destroy target\nWeapon reliability:\n%\nTotal system reliability\nTarget diameter:\nm\nDiameter of a circle encompassing 95% of the target\n* Required\nClose window\nAbout MISSILEMAP\n×\nMISSILEMAP is an interactive data visualization by\nAlex Wellerstein\n, an associate professor of Science and Technology Studies in the\nCollege of Arts and Letters\nat the\nStevens Institute of Technology\n.\nMISSILEMAP is designed to make it easy to see the relationship between missile range, accuracy, and warhead size. It is especially developed for assistance in understanding the power of nuclear warheads and long-range missiles.\nFor more information about using this application, the making of this application, the limitations of the underlying mathematical models, and various simplifying assumptions applied to this visualization, please read the\nFAQ\n.\nFor more details about the explosive power of nuclear weapons, see\nNUKEMAP\n.\nThe hosting services that serve this application are provided by the\nCollege of Arts and Letters, Stevens Institute of Technology\n, in Hoboken, New Jersey, United States.\nOK\nMISSILEMAP\nβ0.2\nby\nAlex Wellerstein\n, 2017-2018"
  },
  {
    "title": "A critical look at NetBSD’s installer",
    "href": "https://eerielinux.wordpress.com/2025/05/31/installing-bsd-in-2025-part-3-a-critical-look-at-netbsds-installer/",
    "content": "Skip to content\neerielinux\nAll the litte things *nix\nMenu\nAbout Eerie Linux\nData protection\nFreeBSD: Tutorials, Howtos and Reviews\nGemini FAQ\nOverview of recent posts\n2012 posts\n2013 posts\n2014 posts\n2015 posts\n2016 posts\n2017 posts\n2018 posts\n2019 posts\n2020 posts\n2021 posts\n2022 posts\n2023 posts\nInstalling *BSD in 2025 part 3 – A critical look at NetBSD’s installer\nMay 31, 2025\nMay 31, 2025\n~\nkraileth\n[New to Gemini? Have a look at my\nGemini FAQ\n.]\nThis article was bi-posted to Gemini and the Web\n; Gemini version is here:\ngemini://gemini.circumlunar.space/users/kraileth/neunix/2025/installing_bsd_pt3.gmi\nThe\nfirst part\nof this series was about FreeBSD’s installer and the\nsecond\ncovered OpenBSD’s installation program.\nThis is a longer article and it has the most images that I ever included in one. While writing the other two articles didn’t take me too long, this one has has occupied several evenings / nights over the course of about three weeks.\nNetBSD is an OS that I installed only a couple of times over the years, so I’m not very familiar with its installer,\nsysinst\n. This fact was actually what led to this article (or the whole series rather): Talking to a NetBSD developer at EuroBSDcon 2023, I mentioned my impression that NetBSD was harder to install than it needed to be. He was interested in my perspective as a relative newcomer, and so I promised to take a closer look and write about it. While it certainly took me long enough, I finally get to do this. So let’s take a look at NetBSD’s installer, shall we? The version explored here is NetBSD\n10.1\non\namd64\n.\nInstalling a simple graphical NetBSD system\nSysinst is menu-driven like FreeBSD’s installer, but the two are quite different. This is not a direct comparison, but I’m going to mention other installers where appropriate. Like in the previous two articles, I’m going to install the OS twice: One time using the standard VGA console and once to assess how well it works over serial. Both installations will be in VMs just for the sake of convenience.\nLanguage settings\nThe first screen that the installer presents is the language selection. If you’re used to more mainstream operating systems, you may be surprised that there’s only four languages in addition to English. But hey: Many other installers are available in English only, and if you’re somebody who’s more comfortable with German, (Castilian) Spanish, French, or Polish than with English, it’s nice to have the option!\nI like that there’s a short introduction text at the top which briefly explains how to navigate the installer. This is very nice for newcomers! Assigning each item an alphabetic hotkey is also a great mechanic that makes navigating the installer a breeze even if there’s a lot of options (FreeBSD’s choice of numeric hotkeys is obviously inferior as soon as there are more than ten items).\nLanguage selection\nAfter selecting the language, the installer presents a list of supported keyboard layouts. Again, NetBSD’s installer does not offer the most comprehensive list ever, but there are some surprises. Given the relatively short list, your preferred keymap may not be available.\nYou probably wouldn’t expect that somebody invested the effort to make Icelandic available, would you? And there’s also things like “German (NEO2)”, a relatively obscure ergonomic variant (that I’ve actually used in the past before adopting BONE whith is basically NEO3). Not even FreeBSD offers NEO2, BTW. Those unusual cases are strong indicators of the NetBSD project being happy to include contributions. So if you consider making a keymap for your layout, another option in the list could probably be yours!\nKeyboard layout selection\nNext is the main menu that allows the user to either install a fresh system, upgrade an existing one, reboot and so on. Like in the menu before, the text at the top did not change. But it didn’t have to since once again the menu options are self-explanatory.\nInstaller options\nThe next screen presents some information on the steps of the installation process and asks if that is what the user wants to do. It’s concise and to the point – I like it. It means an additional key press, but it’s helpful for newcomers.\nInstallation process overview\nPreparing the hard disk\nNow sysinst needs to know the installation target. I believe a more detailed explanation at the top would be beneficial. It’s probably nitpicking to point out that “Extended partitioning” is not exactly an answer to the question “On which disk do you want to install NetBSD?”. That aside, I genuinely believe this screen could be improved. Something like “Select a single disk as the installation target or choose ‘extended partitioning’ for softraid, a logical volume manager or device encryption.” would be better. Probably mentioning that ‘wd*’ means IDE or SATA drives, ‘sd*’ is for SCSI or external USB drives and ‘ld*’ are logical drives like VirtIO drives and NVME could be helpful as well.\nInstallation target selection\nThe next screen does not have any explanatory text at all – another missed opportunity. While people who install NetBSD probably know it, would it hurt to mention that MBR is the scheme that older BIOS systems expect while GPT is required for EFI systems? I guess it wouldn’t.\nPartitioning scheme selection\nThe partition options and the description text for GPT are fine.\nPartitioning options\nThe default partition layout makes sense: One big partition for /, another for swap and a tmpfs-backed one for /tmp – and of course the FAT partition that is required for EFI booting. Since splitting out /usr and /var is common, both are suggested but have a size of 0, so they won’t be used unless the proposed layout is edited.\nIn general the top text is fine. While the explanation of the plus sign is helpful, I would appreciate a hint on how to select the partition that receives the plus. It took me quite a while to find out that when defining a partition size it can simply be appended! This was not immediately obvious to me.\nPartition sizes and file systems\nEditing partition sizes is easy enough. However the fact that giving /usr a size that is not 0 by default moves the plus sign to it even if *not* added to the size entered is quite confusing. It makes the plus mechanic appear magic even though it really isn’t. To be honest, before taking a closer look at sysinst, I assumed that it was not possible to add the plus e. g. to /home like I would have liked to do! Up until recently this was one of the things that bugged me about NetBSD’s installer. I read\nThe NetBSD Guide\n‘s chapter on the installation and that info is not in there, either.\nEditing a partition size\nWhile it’s quite common for installers to allow specifying the unit as a suffix to the size, NetBSD does this differently. Sysinst allows for changing the unit via an option in the menu. It’s definitely useful to be able to switch to GB when partitioning large drives (and I guess due to the portability aspect of NetBSD it may make sense for some architectures to allow entering sectors or cylinders for more control).\nChanging input units\nThe drawback with NetBSD’s method is that this makes the EFI partition appear as 0 GB as it’s only 128 MB. Allowing for input sizes like ’20g’ and ‘128m’ would be more flexible. Then again setting the EFI partition’s size (or just leaving it alone altogether) before switching units of course also works. Maybe displaying the size as 0.1 or something instead of 0 in that case would still make sense?\nPartition sizes now in GB\nAdding more partitions is easy to do; the first menu option below the table will let you do that. Sysinst asks for a mountpoint in that case and adds the new partition to the table. Not being able to cancel the operation with ESC feels a little weird. If the box said “Mount point (leave empty to cancel)?: ” or something like that it would be clear how to abort adding another partition.\nAdding a custom partition\nI simulated fat-fingering the input here by “accidentally” creating a “/s” partition. Much to my frustration I did not find out how to delete partitions from the table! Yes, since /usr and /var are initially of size 0, one can deduce that allocating no space to it will probably make the installer discard them. It’s details like these that make sysinst feel somewhat unfriendly and leave a negative impression.\nI also don’t like that it’s not possible (or I was at least unable to find out how to do it) to change the mount point once the table entry is there. These two problems are also likely much worse with an MBR slice as BSD disklabels have a maximum number of partitions. If you’re doing a complex layout, better get it right the first time! It’s probably less of a problem for regular NetBSD users, but people who are new to an OS and are giving it a first try are not *that* unlikely to change their mind over the partitioning. The feeling of being unable to undo a mistake is discouraging.\nTwo new partitions added – one as a deliberate mistake\nAfter finishing the partitioning, an overview of the layout is presented that the user can either accept – or cancel the installation. Again as a user I can’t say that I feel in control here. This screen basically asks: “Is this ok?”. And the only option besides “yes, go ahead” is “no, let me start over completely”. How about “let me go back to the partition editor and fix one small mistake”? Why isn’t there an option to modify this layout, and can I only accept or discard it? Again, this is probably not so much a problem for people who regularly install NetBSD. As someone who does so infrequently, I’m not entirely satisfied with this.\nPartition layout overview\nNext is another screen that gives the user the chance of “yes” and “no”. While it feels a bit redundant, I think it makes sense to make sure people are aware that at this point the changes will be written to the drive and data previously on it will be destroyed. It could probably still be another window over the last screen rather than a separate one. That’s a relatively minor point, though.\nMake changes to the drive?\nWriting the system to disk\nAcknowledging the previous question, sysinst creates the partitions and filesystem(s). Now the user gets to choose which components of the OS to install. I like the options as well as the text.\nSelecting the distribution sets\nNow the user has to select the source for the distribution sets. The text only mentions FTP and NFS as requiring a network while this is of course the case for HTTP as well. I assume that this is because the other two are traditional sources while HTTP was added later but the text was not updated. This is of course another nit-picky one.\nSelecting the installation source\nWith the source selected, the actual installation (unpacking of the distribution sets) begins. What I find a little strange is that while there is a progress bar, it’s for every single distribution tarball\nindividually\n. There’s no global progress displayed anywhere! And since NetBSD consists of a respectable number of sets, I actually kind of miss this. Fortunately the system is fairly slim (especially compared to many other operating systems) and so the installation doesn’t take too long (even on machines with old, slow drives).\nExtracting a distribution set\nWhen all the sets were extracted, there’s an information screen letting the user know that the new system can now be configured. While in theory the process could be streamlined and this screen be dropped, the completion of the data transfer is probably significant enough to warrant it.\nSet extraction complete\nSystem configuration\nThe first thing that the installer lets you do to configure the system is setting a root password. While this is not technically necessary, basically everybody will want to do that, so it’s a logical choice to present it automatically.\nSetting the root password\nAfterwards, sysinst presents a very nice configuration menu with various options that allow for convenient configuration of some basics. The “Configure additional items as needed.” would be fine – if all options were self-explanatory. For newcomers however, they aren’t. I would suggest changing “Enable xdm” to “Enable xdm (X11 display manager)”, “Enable cgd” to “Enable cgd (cryptographic disk driver)” and “Enable lvm” to “Enable lvm (logical volume manager)”.\nConfiguration menu\nYou usually want to configure the network on your system. If you select the appropriate menu item, you get a list of detected interfaces to pick from. I’m going to make my usual suggestion here: Consider adding the MAC address for each interface!\nWhile it’s often perfectly obvious which one you want to select, this is not always the case. I don’t know how often NetBSD is being deployed in datacenters these days, but servers often have several interfaces and it can be very beneficial for the admins to decide based on the MAC address which one is which (especially since you cannot expect penguins to be familiar with BSD interface naming!). ­¤śē\nNetwork interface selection\nNext is selecting the network media type. Not setting it means attempting autoconfiguration. This is another case where sysinst gives me a weird feeling. From the presented information alone I have no clue if leaving it empty will cause the installer to send DHCP requests or if it’s just going to assume 1000baseT full duplex or something for the media type. Also making this a text input rather than a list to pick from automatically (pun intended) makes people wonder what the supported values are. This is not ideal from a UX perspective.\nSetting the network media type\nEven when leaving the field empty to enable autoconfiguration, a menu pops up asking you if that’s really what you wanted to do. That feels very redundant and is another strong indication that this section of the installer could use an overhaul.\nAutoconfiguration?\nAfter that, sysinst asks for the hostname of the machine. Since there’s no standard way of handling this, I always appreciate it when the installer provides a clue as to whether it expects an FQDN or the short form. As I have the habit of using the former, the first time I installed NetBSD I of course noticed my mistake in the next screen.\nHostname configuration\nNetBSD expects the short form for the hostname, so in the next screen it asks for the domain name. If DHCP was used, it proposes the domain name it got from it. My suggestion would be to either indicate on the previous screen that only the short form is acceptable, or to indicate that either form is and to skip this screen if an FQDN was entered as the hostname.\nDomain name configuration\nIn the next screen, sysinst will provide a summary of the network parameters and ask the user whether they are ok.\nNetwork parameter summary\nIf the user accepts them, the installer offers to write the configuration to disk. This is super helpful for people new to NetBSD who will certainly enjoy a working network connection before having to dig into the documentation just for that! However there might be use cases where people use an installation network to access the distfiles while the actual production network settings are different. So I guess presenting the choice to the user is the right thing to do.\nMake network configuration persistent?\nNext is configuring the time zone. NetBSD defaults to UTC which is a sensible choice. Users who prefer localtime can select items from a list.\nTime zone selection step 1\nIf a continent was chosen in step 1 (like Europe in my case), another list of zones in that area are now presented.\nTime zone selection step 2\nChoosing an item selects it and makes sysinst jump to the “Exit” menu item which is a nice convenience.\nTime zone selection step 3\nMost people will not want to use only the base system, so making the\npkgin\npackage manager available is something that is commonly done. Luckily sysinst makes doing this really easy. In this screen, the installer allows you to configure the remote repository, protocol to use and so on.\nConfiguring pkgin repository\nOnce repository access has been configured, sysinst installs pkgin and uses it to update the package summary.\nFetching the package summary\nWhen that’s done, the installer lets you know that pkgin has been successfully installed.\nPkgin ready\nSome people prefer to (or have to) build packages from source. Sysinst assists in installing the Pkgsrc tree on the system, too. Doing this is quite similar to the previous action: The user first needs to configure a remote source …\nPreparing to fetch pkgsrc\n… and then the compressed tarball is getting downloaded and extracted. Very convenient!\nFetching and installing pkgsrc\nAnother very common thing that a lot of people will want to do is creating an unprivileged user. This works without any fancy dialog windows by just asking for input of the name.\nAdding a user\nIt then asks whether to add the user to the wheel group, which is excellent. For newcomers (especially from Linux) I’d love to see a little explanation that this means allowing the user to use the ‘su’ command to become root. Otherwise this is fine.\nAdd user to the wheel group?\nThen you get to select which of the three shells that the base system provides to use. This works effectively.\nWhich shell for the new user?\nFinally you are asked to set the password for the user. Altogether the process is pretty bare bones as you can neither customize the home directory nor can you pick the UID or assign the user to additional groups. But you can always create (or modify) users manually after booting into the installed system, right?\nPassword for user\nThere are other options here, but they are simply on/off switches. The last item is for finishing the configuration.\nFinishing the configuration\nThe last screen after the installation gives new users some valuable advice like reading afterboot(8) (you installed the\nman\nset, right?) and customizing /etc/rc.conf.\nInstallation finished\nAfter acknowledging this, the installer returns to the main menu, where you can choose to reboot into the newly installed system.\nBack on the main menu for rebooting\nInstallation over serial\nTo explore more of the installer, this time I’m aiming for a more customized installation fit for a server.\nTo be able to install via serial, it’s necessary to escape to the loader prompt and then issue the command:\nconsdev=com0,115200\nSetting the console to serial in the loader\nChanging the console to serial resets the terminal screen. Now it’s possible to just ask the loader to boot the kernel regularly:\nboot\nBooting the installation system\nJust before the installer starts, it asks for the type of terminal that is connected. It needs to know this to make the best use of available features. I’m going with\nxterm\nhere.\nTerminal type selection\nThe first screen of the installer is the same as in the previous installation which used the video console.\nLanguage selection\nThe same is true for the main menu …\nSysinst main menu\n… and the information screen about the installation procedure.\nInstallation overview screen\nI’ve given this machine two virtual drives, so this time I could choose which one to install to.\nSelecting the installation target\nWhile for the first installation I’ve taken the straight-forward path, this time I went with the extended partitioning. This portion of the installer allows for\ncryptographic volumes\n,\nvirtual disk images\n,\nlogical volume management\nand\nsoftware raids\nin addition to more complex partitioning in general. The text contains the instructions to follow and works well for that matter.\nExtended partitioning menu\nSelecting one of the drives opens a menu with various configuration options. People who only ever go the standard path through the installer miss out on a lot of its capabilities! At least I was surprised by the many options when I first explored extended partitioning. Beforehand I considered sysinst to be quite limited, but that is obviously not the case.\nConfiguration options for a drive\nI went with “Format as RAID”, but since the drive is empty, sysinst first asks about which partitioning schema to use. Your options here are GPT, MBR or BSD disklabels.\nSelecting the partitioning schema\nThe installer went ahead and proposed a partition of type RAID for me. It would span the entire available space.\nProposed RAID partition\nAccepting the the partition (going with the proposal or changing the size) takes you back to the partitioning menu. A new partition\ndk0\nhas appeared in the list and ld0’s status has changed to USED. (As a newcomer I ask myself what\ndk\nis, though. A search in the manpages does not solve the mystery.)\nRAID partition created on the first drive\nI did the same thing for the second drive and as expected,\ndk1\nappeared.\nSecond RAID partition created\nThe next step is to select “Create Software RAID”, which opens a window with configuration options.\nSoftware RAID configuration\nSelecting “Disks:” lets the user add or remove disks from the RAID array.\nSoftware RAID disk selection\nChoosing “Add” allows to add members to the array. I think this is a little cumbersome, especially if you want to assemble a large RAID with several members. I would suggest getting rid of the window that’s used to add or remove disks and instead provide one where available disks can be marked as member or unused. That would streamline RAID creation.\nAdding a disk\nAfter adding disks dk0 and dk1 as RAID members and not selecting any spares, the next thing to do is to choose the RAID level. Sysinst obviously does not take disk requirements into account: It offers RAID 5 even though that requires three or more disks while only two were added. So that’s another thing that could be improved.\nSelecting the RAID level\nI went with creating a mirror. Besides selecting the disks to use and the RAID level I left everything else alone.\nRAID configuration complete\nThe RAID configuration was done, but the list does not reflect that. The text said to select “Save Changes” after configuring a RAID, so that’s what needs to be done next.\nRAID configured but not created, yet\nWhat then happened again surprised me: The RAID is being built by supposedly writing parity information! This is very weird. I selected mirroring and RAID 1\ndoes not use\nparity information – mirroring means having the same data on both drives after all! But this is probably raidctl(8)’s fault which simply calls mirrored data “parity” as well.\nBuilding RAID: Syncing…\nAfter the sync is complete, the new\nraid0\ndevice appears in the list. According to the information it is a RAID 1 with two disks and 0 spares. It appears that what I intended to do has worked out.\nRAID synced\nNow the RAID can be partitioned like the disks beforehand. The same menu window is used for that. This time I go with editing the partitions.\nPartitioning the RAID\nThe RAID array doesn’t contain any data, yet, so it needs a partitioning schema, first. It’s possible to use GPT or disklabels, here.\nPartition schema selection for the array\nGoing with GPT, the user is then presented with an empty partition list and the option to add partitions.\nEmpty partition table\nAdding a new partition presents the user with various options like partition type, size, mount point and so on.\nAdding a new partition\nOther than the default FFSv2, there are various other types that you can pick from. There’s also “other type” which opens another menu with less common options like vinum, FreeBSD’s old volume manager.\nPartition types\nI changed everything required to make this new partition be the EFI system partition required to boot from on EFI systems.\nValues for an EFI partition\nWhen creating a new partition for a filesystem that is going to be mounted, mount options can be selected.\nFilesystem mount options\nAfter adding a swap partition I added a third one for the root filesystem with the values shown on the screenshot below.\nThe root partition\nWhile doing much more comprehensive partitioning is of course possible, I don’t think it’s necessary to do that here. A simple layout on a software RAID should be fine for the test system.\nIt appears that this section of the installer was re-used from the standard partitioning path. This is why the text says “This is your last chance to change” the partitions – which is not true. Accepting the partitioning returns you to the extended partition menu from where you could just choose to edit it again. It’s a small inaccuracy, though.\nPartition table for raid0\nThe new partitions appear in the list on the main extended partitioning menu now. So everything is ready to proceed to the actual installation.\nExtended partitioning done\nThe instructions didn’t say that the latest changes need to be saved again, but if you just try to go ahead, sysinst will simply ask you if you want to save or not.\nSave partitioning changes?\nLike in the previous installation, the next choice is the distribution sets to be installed. I’ll go for a custom installation this time.\nSelecting the distribution sets\nThe standard selection of sets is fairly minimal. I would suggest to at least include the manpages by default!\nCustom installation: Standard selection of sets\nI chose to install the compiler toolchain (so that Pkgsrc would be usable) as well as the manpages (of course!) and miscellaneous (simply because I have no idea what’s in it).\nCustom installation: A couple more sets enabled\nTo not simply repeat the first installation, I’m going with HTTP as the source for the sets instead of the local medium.\nInstalling over HTTP\nThis of course triggers network configuration which was only done after the installation in the previous attempt. I’m not providing screenshots of the other configuration steps as they are identical.\nSelecting the network interface\nAfter the network is configured, the mirror to fetch the sets from needs to be selected.\nSelecting the mirror to use\nAnd then the installation begins! Or not, since the filesystem for some reason turns out to be read-only …\nWhoops! Disk is ro?!\nPain points\nThe above section ending rather abruptly is not due to any malice on my side trying to break poor little sysinst. In fact I tried to do several dozen installations over the past three weeks, trying out various setups. Truth be told: I’ve failed time and time again.\nI didn’t mention it before, but after the first installation I wanted to show a screenshot of the graphical NetBSD system. The installation succeeded, but the system would panic during boot. Bhyve is more of a niche thing and not among the hypervisors supported by NetBSD, so I’m not complaining. But I haven’t been able to finish *any* installation after picking extended partitioning. Going with a software RAID makes the FS read-only. Trying to create a simple LVM lead to this:\nStatus: Command failed\r\n> Command: lvm pvcreate -ffy /dev/rdk2\r\n> Hit enter to continue\r\n> ---\r\n> Device /dev/rdk2 not found (or ignored by filtering)\nI read the first few chapters of\nThe NetBSD Guide\n, hoping to better understand what I was doing wrong. It’s a good document but unfortunately pretty outdated, too. When I read “The examples from this chapter were created with NetBSD 8.0.”, I got a feeling what to expect (that version was released 7 years ago, in 2018).\nI honestly don’t mind that the image explaining partitioning uses DOS as the other operating system in a dual-boot setup. But the guide does not even mention GPT at all! It’s all legacy MBR which is not terribly helpful for newer machines at this point. Also much to my surprise, the extended partitioning is not even covered. Yes, Software RAID, LVM and friends are covered, but only how to set them up via the CLI on a running system. Sorry, I don’t want to do that, I want to install to a more flexible means of storage than a bare disk with fixed partition sizes!\nThe system console messages appearing on the screen and messing up parts of the installer can be very annoying, too. Maybe just start sysinst on different virtual terminal to get rid of this?\nInstalling on real hardware presented me with other problems – and even standard installations can fail. On one machine it took incredibly long – the modules.txr.xz set alone took half an hour with the installation at times going to as slow writes as roughly 4 KiB/s or even stalling entirely. The base set took over 1.5 hours! I assume NetBSD might be struggling with the NVMe? To add insult to injury when it was finally done with all the sets (I had picked a full installation…), this was the result:\nStatus: Command failed\r\n> Command: /sbin/fsck_ffs -p -q /dev/dk0\r\n> Hit enter to continue\r\n> ---\r\n> /dev/rdk0: BAD SUPER BLOCK: CAN'T FIND SUPERBLOCK\r\n> \r\n> /dev/rdk0: UNEXPECTED INCONSISTENCY; RUN fsck_ffs MANUALLY.\nI tried my luck in the opposite direction and used one of the older laptops that I have. The installer started regularly but at one point simply dropped me to a shell, saying “sysinst terminated.”\nThe error message was something along the lines of “Screen too narrow (70 + 5 > 64) for menu”. I was able to solve that by connecting a VGA monitor to it, but that was unexpected nevertheless. This little experiment proved useful, though, as it revealed a portion of sysinst that I had not seen before: The installer told me that this machine didn’t provide strong entropy and gave me several options for dealing with that. It’s really nice that NetBSD did great work in that regard!\nConclusion\nWell, after all those problems I encountered, you might expect me to condemn sysinst. I won’t. In fact I like the general workflow – minus the weird parts. Sysinst has a lot of neat ideas in addition to some quirks. As mentioned at the beginning, I like the hotkey system a lot. Trying to reserve “x” to finish a screen is one nice detail. I really like the configuration options after the installation. That part is very well done.\nOn the other hand, network autoconfiguration is weird. I miss the option to decide on configuring the system for v4 only for example. A couple of things could certainly be improved, but few things are perfect. The elephant in the room though is the major problems that I ran into. I often find myself exploring less common paths in programs, which sometimes leads to uncovering bugs. It’s not just sysinst but it happens to me often when I use something not just superficially.\nWhen I sat down to concern myself with the NetBSD installer, I expected that my major critique would be that it doesn’t do ZFS – which is a shame since the availability of that filesystem is one of the things that make NetBSD appeal to me. Various other problems have definitely overshadowed this, though. I remain hopeful that these issues can be addressed, and perhaps the solution lies in clearer guidance for users. As I said, I’m not very familiar with NetBSD. But I’d like to change that fact in the long run. And ideally at some point I’d be able to say: “Of course it runs NetBSD!”\nWhat’s next?\nIn the next article I’ll take a closer look at DragonFly BSD’s installer.\nShare this:\nClick to print (Opens in new window)\nPrint\nClick to email a link to a friend (Opens in new window)\nEmail\nLike\nLoading...\nRelated\nPosted in\nbsd\nbsd\ncomputer\ninstaller\nnetbsd\noperating system\nsoftware\nPost navigation\n‹ Previous\nInstalling *BSD in 2025 part 2 – A critical look at OpenBSD’s installer\nOne thought on “\nInstalling *BSD in 2025 part 3 – A critical look at NetBSD’s installer\n”\nPingback:\nHacker News õ╗ŖµŚźTOP 20| 2025-06-04 - Õć║µĄĘµÄśķćæ’╝īµŚĀķÖÉÕÅ»ĶāĮŃĆéõĖ║ńŗ¼ń½ŗÕ╝ĆÕÅæĶĆģŃĆüĶĘ©ÕóāńöĄÕĢåõ╗ÄõĖÜĶĆģŃĆüµĄĘÕż¢Ķć¬Õ¬ÆõĮōµÅÉõŠøµ£Ćµ¢░Õć║µĄĘĶĄäĶ«»ÕÆīĶĄäµ║É-Õć║µĄĘµÄśķćæ’╝īµŚĀķÖÉÕÅ»ĶāĮŃĆéõĖ║ńŗ¼ń½ŗÕ╝ĆÕÅæĶĆģŃĆüĶĘ©ÕóāńöĄÕĢåõ╗ÄõĖÜĶĆģŃĆüµĄĘÕż¢Ķć¬Õ¬ÆõĮōµÅ\nLeave a comment\nCancel reply\nΔ\nThis site uses Akismet to reduce spam.\nLearn how your comment data is processed.\nInstalling *BSD in 2025 part 3 – A critical look at NetBSD’s installer\nInstalling *BSD in 2025 part 2 – A critical look at OpenBSD’s installer\nInstalling *BSD in 2025 part 1 – A critical look at FreeBSD’s installer\nInstalling *BSD in 2025 part 0b – Modern myths: AI ramblings (NetBSD and OpenBSD)\nInstalling *BSD in 2025 part 0a – Modern myths: AI ramblings (DragonFly BSD and FreeBSD)\narch\nbackup\nbacula\nblogging\nbsd\ncomputer\ndesktop\ndos\ndragonfly bsd\nede\nembedded\nenlightenment\nequinox\nfltk\nfreebsd\ngnome\ngtk+\nhardware\nIllumos\ninternet\nkde\nlegacy\nlinux\nlxde\nmate\nnetbsd\nomnios\nopenbsd\noperating system\nopnsense\npackage management\npkgsrc\nports\nprogramming\nqt\nravenports\nrazor-qt\nrouter\nsoftware\nsolaris\ntoolkit\ntutorial\nunity\nvirtualization\nxfce\nMay 2025\nApril 2025\nMarch 2025\nJanuary 2025\nDecember 2024\nNovember 2024\nOctober 2024\nJuly 2024\nMarch 2024\nFebruary 2024\nDecember 2023\nOctober 2023\nSeptember 2023\nMay 2023\nApril 2023\nFebruary 2023\nJanuary 2023\nDecember 2022\nNovember 2022\nSeptember 2022\nJune 2022\nDecember 2021\nNovember 2021\nJune 2021\nMay 2021\nApril 2021\nMarch 2021\nFebruary 2021\nJanuary 2021\nDecember 2020\nNovember 2020\nOctober 2020\nFebruary 2020\nJanuary 2020\nDecember 2019\nNovember 2019\nOctober 2019\nSeptember 2019\nAugust 2019\nJuly 2019\nJune 2019\nMay 2019\nApril 2019\nMarch 2019\nFebruary 2019\nJanuary 2019\nDecember 2018\nNovember 2018\nOctober 2018\nSeptember 2018\nAugust 2018\nJuly 2018\nJune 2018\nMay 2018\nApril 2018\nMarch 2018\nFebruary 2018\nJanuary 2018\nDecember 2017\nNovember 2017\nOctober 2017\nSeptember 2017\nAugust 2017\nJuly 2017\nJune 2017\nMay 2017\nApril 2017\nMarch 2017\nFebruary 2017\nJanuary 2017\nDecember 2016\nNovember 2016\nOctober 2016\nSeptember 2016\nAugust 2016\nJuly 2016\nJune 2016\nMay 2016\nApril 2016\nMarch 2016\nFebruary 2016\nJanuary 2016\nDecember 2015\nNovember 2015\nOctober 2015\nSeptember 2015\nAugust 2015\nJuly 2015\nJune 2015\nMay 2015\nApril 2015\nMarch 2015\nFebruary 2015\nJanuary 2015\nDecember 2014\nNovember 2014\nOctober 2014\nSeptember 2014\nAugust 2014\nJuly 2014\nJune 2014\nMay 2014\nApril 2014\nMarch 2014\nFebruary 2014\nJanuary 2014\nDecember 2013\nNovember 2013\nOctober 2013\nSeptember 2013\nAugust 2013\nJuly 2013\nJune 2013\nMay 2013\nApril 2013\nMarch 2013\nFebruary 2013\nJanuary 2013\nDecember 2012\nNovember 2012\nOctober 2012\nSeptember 2012\nAugust 2012\nJuly 2012\nJune 2012\nCreate a free website or blog at WordPress.com.\nComment\nReblog\nSubscribe\nSubscribed\neerielinux\nJoin 85 other subscribers\nSign me up\nAlready have a WordPress.com account?\nLog in now.\neerielinux\nSubscribe\nSubscribed\nSign up\nLog in\nCopy shortlink\nReport this content\nView post in Reader\nManage subscriptions\nCollapse this bar\n%d\nDesign a site like this with WordPress.com\nGet started"
  },
  {
    "title": "Click-V: A RISC-V emulator built with ClickHouse SQL",
    "href": "https://github.com/SpencerTorres/Click-V",
    "content": "Skip to content\nNavigation Menu\nToggle navigation\nSign in\nAppearance settings\nProduct\nGitHub Copilot\nWrite better code with AI\nGitHub Models\nNew\nManage and compare prompts\nGitHub Advanced Security\nFind and fix vulnerabilities\nActions\nAutomate any workflow\nCodespaces\nInstant dev environments\nIssues\nPlan and track work\nCode Review\nManage code changes\nDiscussions\nCollaborate outside of code\nCode Search\nFind more, search less\nExplore\nWhy GitHub\nAll features\nDocumentation\nGitHub Skills\nBlog\nSolutions\nBy company size\nEnterprises\nSmall and medium teams\nStartups\nNonprofits\nBy use case\nDevSecOps\nDevOps\nCI/CD\nView all use cases\nBy industry\nHealthcare\nFinancial services\nManufacturing\nGovernment\nView all industries\nView all solutions\nResources\nTopics\nAI\nDevOps\nSecurity\nSoftware Development\nView all\nExplore\nLearning Pathways\nEvents & Webinars\nEbooks & Whitepapers\nCustomer Stories\nPartners\nExecutive Insights\nOpen Source\nGitHub Sponsors\nFund open source developers\nThe ReadME Project\nGitHub community articles\nRepositories\nTopics\nTrending\nCollections\nEnterprise\nEnterprise platform\nAI-powered developer platform\nAvailable add-ons\nGitHub Advanced Security\nEnterprise-grade security features\nCopilot for business\nEnterprise-grade AI features\nPremium Support\nEnterprise-grade 24/7 support\nPricing\nSearch or jump to...\nSearch code, repositories, users, issues, pull requests...\nSearch\nClear\nSearch syntax tips\nProvide feedback\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel\nSubmit feedback\nSaved searches\nUse saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our\ndocumentation\n.\nCancel\nCreate saved search\nSign in\nSign up\nAppearance settings\nResetting focus\nYou signed in with another tab or window.\nReload\nto refresh your session.\nYou signed out in another tab or window.\nReload\nto refresh your session.\nYou switched accounts on another tab or window.\nReload\nto refresh your session.\nDismiss alert\nSpencerTorres\n/\nClick-V\nPublic\nNotifications\nYou must be signed in to change notification settings\nFork\n1\nStar\n20\nA RISC-V emulator built with ClickHouse SQL\n20\nstars\n1\nfork\nBranches\nTags\nActivity\nStar\nNotifications\nYou must be signed in to change notification settings\nCode\nIssues\n0\nPull requests\n0\nActions\nProjects\n0\nSecurity\nUh oh!\nThere was an error while loading.\nPlease reload this page\n.\nInsights\nAdditional navigation options\nCode\nIssues\nPull requests\nActions\nProjects\nSecurity\nInsights\nSpencerTorres/Click-V\nmain\nBranches\nTags\nGo to file\nCode\nFolders and files\nName\nName\nLast commit message\nLast commit date\nLatest commit\nHistory\n13 Commits\nasm\nasm\nrs-demo\nrs-demo\nsql\nsql\nsystem\nsystem\n.gitignore\n.gitignore\nREADME.md\nREADME.md\ndocker-compose.yml\ndocker-compose.yml\nView all files\nRepository files navigation\nREADME\nClick-V\nA RISC-V emulator built with ClickHouse SQL.\nThis emulator makes ClickHouse truly Turing complete. We are one step closer to running ClickHouse in ClickHouse.\nThis project/repository isn't dev-friendly yet, I'm just uploading it here as a backup in case my PC catches fire.\nHow it works\nThe system will react to the following insert command:\nINSERT INTO\nclickv\n.\nclock\n(_)\nVALUES\n()\nThis command will trigger a large set of branched materialized views and\nNull\ntables that filter out the program's instructions to simulate reading/writing from registers and memory.\nExternal host machine access works via a single UDF with a custom binary format that gets read/written as an\nArray(UInt8)\n.\nThe program is able to perform any logic. Printing to a console table and drawing are built-in.\nIt can also open/close/read/write/seek files and sockets via the ClickOS UDF.\nFor more details, see the\narchitecture\nsection.\nPerformance\nI tried to use every optimization trick in the book to get this to run fast, unfortunately there is a MAJOR bottleneck to the performance of this emulator due to a bug in ClickHouse KVStorage logic. Because ClickHouse doesn't have an internal KV-type storage engine, I use Redis for registers/memory. But there is a bug with\nallow_experimental_analyzer=1\nwhere instead of doing a single\nMGET\n, it will\nSCAN\nall keys and\nthen\nMGET\nmultiple times.\nI haven't submitted a bug report yet, but I did investigate it. More notes are commented in the file\n/sql/click-v.sql:11\n.\nAs it is now, the CPU runs at around\n17hz\n, but during early development this was significantly higher. It\ncan\nperform better, but when every almost every instruction depends on a register read, it kills performance quickly. It gets worse with more memory allocated in the emulator.\nHow to run\nSteps:\nSet up a ClickHouse v24 image\nSet up a Redis-like server for registers/memory access (plain redis works fine, dragonfly was slower, there's also a built-in server in\n/system/mem\n)\nRun all SQL statements in\n/sql/click-v.sql\n(confirm your redis host is correct, right now it points to\nhost.docker.internal:6379\n)\nLoad your own RISC-V 32i program into\nINSERT INTO clickv.load_program (hex) VALUES ('FFFFFFFF')\n(make sure your hex instructions are in the correct direction)\nEither clock the system via\nINSERT INTO clickv.clock (_) VALUES ()\n, or use the auto-clock in\n/system/clock\nYou can now monitor the program with the following commands:\nShow program instructions + current instruction:\nSELECT * FROM clickv.display_program;\nShow all 32 registers:\nSELECT * FROM clickv.display_registers;\nShow memory (with o parameter for offset):\nSELECT * FROM clickv.display_memory(o=1024);\nShow console:\nSELECT * FROM clickv.display_console FORMAT TSV;\nSetup live view (optional):\nSET allow_experimental_live_view = 1;\n(After frame setup) Show current drawn frame:\nSELECT * FROM clickv.display_frame FORMAT RawBLOB;\n(After frame setup) Show live-updating frame:\nWATCH clickv.display_frame FORMAT RawBLOB;\nFor more help/commands, see the bottom of\n/sql/click-v.sql\nfile.\nROM/RAM/Graphics Memory is configurable.\nComponents\nClickHouse\nDepends on ClickHouse v24.\nNo other setup is required for basic emulator.\nFor handling syscalls, you will need to set up the ClickOS UDF, but this is optional.\nClock\npath:\n/system/cmd/clock\nThis program simply runs the clock for you, as fast as possible.\nWill output clock speed and total cycles to console.\nClickOS\npath:\n/system/cmd/clickos-server\npath:\n/system/cmd/clickos-client\nOptional program to give the emulated program access to the host system/network.\nThis is a client/server application.\nThe client runs as a ClickHouse executable UDF, and then forwards requests to the server.\nThe server will then handle all syscalls (such as reading/writing to a file, opening a UDP socket, etc.)\nYou will need to set up the UDF in your ClickHouse server. Easiest way is to make two Docker volume binds: one to the UDF XML, and the other to built binary (you must\ngo build\nfor your docker env/arch)\nRun the server to listen/handle syscalls. File paths are relative to the working directory of the ClickOS server process.\nrs-demo\npath:\n/rs-demo\nThis is a demo rust program that can be compiled to run in the emulator.\nI have some boilerplate for syscalls, with some OS abstractions for\nread\n,\nwrite\n,\nseek\n,\nsocket\n,\nopen\n,\nclose\n, etc.\nI also have some code that handles drawing to the screen.\nTo get the program hex, I made a script called\ngethex.sh\n.\nYou can copy/paste this directly into the program input for the emulator.\nThis program contains a linker script that defines the memory ranges for ROM, RAM, Stack size, and VRAM.\nMem (Redis-replacement)\npath:\n/system/cmd/mem\nThis program will store the registers/memory for the emulator.\nDragonfly was slow for this use case, Redis was faster, but this program is optimized to use exact amounts of memory + sequential reads.\nNote: there is a bug with ClickHouse where\nALL\nqueries use\nSCAN\n, even direct\nk=1\nqueries.\nThis is a huge hit to performance, and will require a patch to ClickHouse to fix.\nRISC-V Instruction Test suite\npath:\n/system/test/instruction_test.go\nHow do we know any of these instructions do what they're supposed to do?\nTo answer this, I made a unit test for each instruction.\nIt is now much easier to see if the instructions are compliant with the specification when isolated.\nThis file will run a test for each instruction, some with different test cases.\nIt also prints out the performance of each instruction. You'll notice some instructions are more costly than others.\nArchitecture\nI will simplify this into several components:\nClock\nProgram Counter (PC)\nMemory\nRegisters\nInstructions\nSyscalls\nClock\nSchema: no schema\nAs the name suggests, this is the clock for the emulated CPU.\nThis is implemented as a\nNull\ntable. When you insert into this, it will cascade down a set of materialized views.\nProgram Counter (PC)\nSchema:\nvalue UInt32\nThis is a\nMemory\ntable with limits to store exactly\n1\nrow.\nIt stores a single\nUInt32\n, which represents the current instruction.\nMemory\nSchema:\naddress UInt32, value UInt8\nMemory contains the program instructions (ROM), as well as RAM and VRAM (for the display).\nEngine choice\nWhile I originally had this implemented as a\nMemory\ntable, it was clear that this would not\nwork for larger programs.\nWhen writing to memory, it would push out the oldest row.\nIt would also require adding a\ntimestamp\nfield of some kind to each row, since it could contain duplicates.\nReplacingMergeTree\nwas also considered, but this writes to disk, and would have duplicates before the parts are processed (which is likely in a high-speed emulator environment).\nIt can be done, but it would require having a lot of duplicated rows, with enough space so that old memory would have a low probability of falling out of the table. Too much memory usage.\nSo I then switched to a\nRedis\ntable engine. This is the optimal structure, since it operates as a fast in-memory KV store with no duplicates.\nThis works perfectly, except for how the newer version of ClickHouse ALWAYS runs a full\nSCAN\nwith multiple\nMGET\ncalls.\nMemory can be read via a\nJOIN\nor sub-query, even in multiple bytes.\nMemory can be written in multiple bytes using\narrayJoin\ninto the\nmemory\ntable.\nRegisters\nSchema:\naddress UInt8, value UInt32\nRegisters are implemented the same as memory, but with 32 fixed registers.\nInstructions\nThe first materialized view hit by the\nclock\ntable is\nget_next_instruction\n.\nThis will parse the\npc\n,\ninstruction\n,\nopcode\n, and\nfunct3\nand send it to the next layer of materialized views. The idea with these layers is to reduce the number of function calls and queries for parsing the instruction.\nThe next layer will then split by instruction type. For example:\nR-type\n,\nI-type\n,\nS-type\n,\njump\n,\necall\n, etc.\nThese views have a\nWHERE\ncondition that blocks them from inserting into the next layer of\nNull\ntables, which again reduces the number of queries/function calls.\nWithin each of these types (such as\nR-type\n) is the materialized views for the individual instruction. At this point it will do the final check to see which instruction it is, and then forward to another\nNull\ntable for executing the instruction. By this point, there's no other path for that instruction, and all the expensive queries can be made.\nEach instruction (with the exception of jumps and branches) will have another materialized view at the end that increments the\npc\nby\n4\n. Materialized views are executed in the order they are created, so this works flawlessly for executing sequential logic.\nDepending on the instruction, the output will either write to the main\nregisters\nor\nmemory\ntable. Instructions can also read from these table via a\nJOIN\n.\nWith the layers of filtering, it keeps the execution path short for the ClickHouse server.\nThis also offers an easy way to measure performance per-instruction, since the original\nclock\ninsert will not return until the last materialized view is finished.\nSyscalls (\necall\n)\nRISC-V has a special instruction for returning control to the operating system:\necall\n.\nThe Click-V emulator is able to make use of this special instruction for 3 major features:\nwriting to a\nprint\ntable, to replicate\nstdout\nwriting to a\nframe\ntable, trigger rendering the data within VRAM into a terminal-displayed frame.\nmaking external calls to the host system via ClickOS (read/write files, communicate over UDP socket, anything else you can imagine)\necall\nis implemented same as the other instructions, but due to the expensive nature of these calls, they are hidden behind another layer of materialized views to prevent unnecessary sub-queries from being triggered.\nThe syscall number is read from register\na7\n, and the arguments are passed in the other\naX\nregisters. Depending on the call, the result/status code will be returned back in\na0\n.\nAll syscalls have been implemented in the\nrs-demo\nprogram.\nprint\nThis call is really simple, it just reads from memory using\ntext_ptr\nand\ntext_len\n, and then inserts the result into the\nprint\ntable.\ndraw\nThis call will read from video memory and split up the bytes into a terminal-based image with ANSI colors. You can use the\nLIVE VIEW\n/\nWATCH\nAPI to get this to update in real time.\nClickOS\nExternal system access is managed by ClickOS. These calls are able to read/write to/from emulator memory in order to implement file descriptors for interacting with the host system.\nAccess to the host system is implemented via a ClickHouse executable UDF. The memory gets inserted/returned as an\nArray(UInt8)\n.\nWith a similar API to the Linux kernel, these usually rely on a\nbuffer_ptr\nand\nbuffer_len\nfor exposing program memory.\nAbout\nA RISC-V emulator built with ClickHouse SQL\nTopics\nsql\ndatabase\nclickhouse\nriscv\nrisc-v\nriscv32\nriscv-emulator\nResources\nReadme\nUh oh!\nThere was an error while loading.\nPlease reload this page\n.\nActivity\nStars\n20\nstars\nWatchers\n1\nwatching\nForks\n1\nfork\nReport repository\nUh oh!\nThere was an error while loading.\nPlease reload this page\n.\nLanguages\nGo\n86.2%\nRust\n11.4%\nLinker Script\n1.1%\nOther\n1.3%\nFooter\n© 2025 GitHub, Inc.\nFooter navigation\nTerms\nPrivacy\nSecurity\nStatus\nDocs\nContact\nManage cookies\nDo not share my personal information\nYou can’t perform that action at this time."
  },
  {
    "title": "A deep dive into self-improving AI and the Darwin-Gödel Machine",
    "href": "https://richardcsuwandi.github.io/blog/2025/dgm/",
    "content": "Toggle navigation\nAbout\nBlog\nPublications\nProjects\nServices\nCV\nAI that can improve itself\nA deep dive into self-improving AI and the Darwin-Gödel Machine.\nContents\nLearning to Learn\nDarwin-Gödel Machine\nHow DGM Works\nCan DGM Really Improve Itself?\nComparison with AlphaEvolve\nCan we trust a self-improving AI?\nTakeaways\nMost AI systems today are stuck in a “cage” designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the\nAchilles heel\nof modern AI — like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.\nLearning to Learn\nThe idea of building systems that can improve themselves brings us to the concept of\nmeta-learning\n, or “learning to learn”\n, which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the Gödel Machine\n, proposed by Jürgen Schmidhuber decades ago and was named after the famous mathematician\nKurt Gödel\n. A Gödel Machine is a hypothetical self-improving AI system that optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy. It represents the ultimate form of self-awareness in AI, an agent that can reason about its own limitations and modify itself accordingly.\nFigure 1.\nGödel machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy.\nWhile this idea is interesting, formally proving whether a code modification of a complex AI system is\nabsolutely beneficial\nis almost an impossible task without restrictive assumptions. This part stems from the inherent difficulty revealed by the\nHalting Problem\nand\nRice’s Theorem\nin computational theory, and is also related to the inherent limitations of the logical system implied by\nGödel’s incompleteness theorem\n. These theoretical constraints make it nearly impossible to predict the complete impact of code changes without making restrictive assumptions. To illustrate this, consider a simple analogy: just as you cannot guarantee that a new software update will improve your computer’s performance without actually running it, an AI system faces an even greater challenge in predicting the long-term consequences of modifying its own complex codebase.\nDarwin-Gödel Machine\nTo “relax” the requirement of formal proof, a recent work by proposed the\nDarwin-Gödel Machine (DGM)\n, which combines the Darwinian evolution and Gödelian self-improvement. Essentially, DGM abandoned the pursuit of a rigorous mathematical proof and embraced a more pragmatic way that is closer to the essence of life evolution through empirical validation. As the authors put it,\nWe do not require formal proof, but empirical verification of self-modification based on benchmark testing, so that the system can improve and explore based on the observed results.\nFigure 2.\nThe Darwin-Gödel Machine (DGM) is a self-improving AI system that optimizes its own problem-solving strategy through a combination of Darwinian evolution and Gödelian self-improvement.\nThe core idea behind DGM is to let the AI agent iteratively modify its own code\nMore precisely, the metacode that controls its behavior and ability\n, and then put these modified new versions into the actual test environment (e.g., coding benchmarks) to judge whether this modification is good by observing its performance. If the modification is good, it will be retained and may become the basis for further evolution in the future; if the modification is bad, it will be eliminated.\nHow DGM Works\nFigure 3.\nAnimation showing how DGM iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.\nThe general workflow of DGM is as follows:\nInitialization:\nEvolution begins with one or a few basic coding agents, so-called the “seeds” of intelligence. These initial agents may possess only minimal functionalities, such as executing bash commands or editing entire files. DGM maintains an “archive of agents”, which functions like a biological “gene bank.” This archive preserves all previously generated agents, regardless of their performance level, ensuring that no potentially valuable variation is permanently lost.\nSampling:\nDGM selects one or more parent agents from the archive to generate new child agent. The selection mechanism does not focus solely on top performers but instead allows every agent a chance to be chosen. While higher-performing agents are more likely to be selected, DGM also encourages exploration by giving “less successful agents”\nThose that might contain novel or unconventional ideas\nan opportunity to contribute to future generations.\nReproduction:\nOnce the parents are selected, DGM performs modifications to generate a new child agent. These changes occur at the source code level and can include:\nEnhancement of existing tools\n, for example, upgrading a file-editing tool that replaces entire files into one capable of precise string replacement.\nAddition of new tools or workflows\n, such as integrating a static code analysis tool or designing a “think–act–observe” loop.\nImprovement of problem-solving strategies\n, for instance, refining how the agent prompts the base model or managing long-context code more effectively.\nIntroduction of collaboration mechanisms\n, like having an agent’s code reviewed by another before execution.\nNatural Selection:\nThe newly generated child agent is not automatically accepted into the “elite pool” but must prove its worth through rigorous testing. Each agent’s performance, such as the percentage of successfully solved problems, is quantitatively scored and used to select the best agents.\nTree Formation:\nIf a child agent outperforms its parent or meets certain quality thresholds, it is added to the archive and becomes a new node in the evolutionary tree. This process repeats iteratively, creating a growing structure of diverse, high-quality agents. Importantly, DGM enables parallel exploration of many different paths in the search space, promoting open-ended discovery and avoiding premature convergence to local optima.\nFigure 4\nDGM iteratively builds a growing intelligent archive by intertwining self-modification with downstream task evaluation. The intelligent body in the archive is selected for self-modification through open exploration.\nUnlike traditional\nhill-climbing methods\nthat often get stuck in local optima, DGM employs open-ended exploration. This approach allows even underperforming agents to persist if they contain novel ideas that might lead to breakthroughs in future generations. Even agents with temporarily lower performance can be selected if they contain interesting ‘mutations’, since their innovative traits may be combined with other beneficial characteristics in subsequent generations to produce unexpectedly strong offspring. This mirrors biological evolution, where diversity and serendipity often drive innovation.\nThe paper provides a concrete example of the above principle in action. In the SWE-bench experiment, the performance of newly generated agents showed temporary declines in the 4th and 56th iterations. While traditional hill-climbing methods would abandon these seemingly “regressive paths”, DGM’s open exploration mechanism allows continued exploration. This persistence paid off as by the 70th iteration, the system had created new agents that surpassed all their predecessors, demonstrating the value of maintaining diverse evolutionary paths even when they temporarily appear less promising.\nCan DGM Really Improve Itself?\nIn the paper, the authors evaluated DGM on two challenging benchmarks:\nSWE-bench:\nA widely-used benchmark that tests agents’ ability to resolve real-world GitHub issues\n. DGM showed impressive progress, improving its performance from 20.0% to 50.0% through self-modification.\nPolyglot\n: A\nmulti-language coding benchmark\ncontaining coding problems in C++, Go, Java, JavaScript, Python and Rust. DGM achieved even more dramatic improvement, jumping from an initial 14.2% to 30.7% success rate, far surpassing the representative hand-designed agent by\nAider\n.\nThe impressive performance improvements demonstrate DGM’s fundamental ability to enhance itself through self-modification. Through ablation studies, the authors identified two key factors driving DGM’s success. First, self-modification proved crucial, as when disabled (\nDGM w/o Self-improve\n), the performance declined substantially, showing that the ability to modify its own code is essential for accelerated learning. Second, the open-ended exploration approach was vital, as removing the Darwinian evolution-inspired search mechanism (\nDGM w/o Open-ended exploration\n), which maintains an archive of diverse solutions and enables parallel exploration of multiple paths, resulted in lower performance.\nFigure 5\nSelf-improvement and open-ended exploration enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right).\nComparison with AlphaEvolve\nIn parallel, AlphaEvolve\n, which is developed by Google DeepMind, also demonstrates another powerful path forward. AlphaEvolve pairs the creative problem-solving capabilities of Google’s Gemini models with automated evaluators in an evolutionary framework. It has already demonstrated significant real-world impact across multiple domains, such as:\nData center efficiency:\nAlphaEvolve discovered a simple yet highly effective heuristic for Google’s\nBorg\ncluster management system, continuously recovering 0.7% of Google’s worldwide compute resources.\nAI acceleration:\nIt achieved a 23% speedup in Gemini’s architecture’s vital\nkernel\nby finding more efficient ways to divide large matrix multiplication operations, resulting in a 1% reduction in overall training time.\nMathematical breakthroughs:\nMost notably, it discovered an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing\nStrassen’s 1969 algorithm\n, and advanced the 300-year-old\nkissing number problem\nby establishing a new lower bound in 11 dimensions.\nInterested readers can refer to my\nprevious post\nfor a comprehensive overview of AlphaEvolve and its comparison with its predecessor, FunSearch.\nWhile both systems adopt a similar evolutionary framework, their scopes and methodologies differ in the following ways:\nFeature\nAlphaEvolve\nDGM\nFocus\nEvolving functions and codebases\nEvolving the agent itself\nLevel of Innovation\nAlgorithmic level\nAgent-level (toolset, methodology)\nRole of LLM\nLLM acts as “genetic operators” to modify algorithms\nLLM serves as the “brain” to evolve itself with better tools and strategies\nEvaluation\nWell-defined problems with automated evaluators\nOpen-ended environments\nTo better understand the differences between the two approaches, let us take a look at the following analogy:\nImagine two chefs working in different kitchens. AlphaEvolve is like a highly skilled chef who follows and refines recipes for specific dishes. He uses an AI-powered assistant to suggest variations, such as substituting ingredients or adjusting cooking times, to make the dish better. Every new version of the recipe is taste-tested by an automated panel that rates flavor, texture, and presentation. If it scores high, the new recipe becomes the standard; if not, it’s discarded. DGM, on the other hand, is more like a chef who constantly improves his own culinary skills and kitchen tools, not just the recipes. He does not just tweak ingredients, but rather learns new techniques, upgrades his knives, reorganizes the kitchen layout, and even teaches junior chefs. He experiments in real-world settings, like hosting large dinner parties or catering events, where the feedback is not always structured but is rich and varied. Over time, he becomes a better chef overall, able to handle any kind of cuisine or challenge thrown at him.\nCan we trust a self-improving AI?\nThe authors also conducted some experiments to evaluate DGM’s reliability and discovered some concerning behaviors. In particular, they observed instances where DGM attempted to manipulate its reward function through deceptive practices. One notable example involved the system fabricating the use of external tools - specifically, it generated fake logs suggesting it had run and passed unit tests, when in reality no tests were executed.\nFigure 6\nDGM hallucinated the use of an external tool to run unit tests.\nTo address this issue, the authors implemented a “tool use hallucination” reward function and tasked DGM with optimizing it to eliminate hallucination behaviors. While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them. This behavior is reminiscent of the phenomenon of\nGoodhart’s Law\nfrom economics and sociology:\n“When a measure becomes a target, it ceases to be a good measure.”\nFortunately, DGM’s design includes a transparent, traceable lineage of all changes, which enabled the researchers to quickly identify and address these undesirable behaviors. However, this example highlights the need for more robust safeguards to prevent such manipulation attempts in the first place. These findings underscore the critical importance of safety in self-improving AI research.\nTakeaways\nDGM represents a groundbreaking step toward the realization of\nLife 3.0\n, a concept introduced by physicist\nMax Tegmark\n. In his book, he classified life into three stages:\nLife 1.0:\nBiological life with fixed hardware and software, such as bacteria.\nLife 2.0:\nBeings like humans, whose behavior can be learned and adapted during their lifetime, though their biology remains fixed.\nLife 3.0:\nA new class of intelligence that can redesign not only its behavior but also its underlying architecture and objectives — essentially, intelligence that builds itself.\nFigure 7\nThe three stages of life according to Max Tegmark.\nWhile DGM currently focuses on evolving the “software”\nthe code and strategies of AI agents\n, it exemplifies the early stages of Life 3.0. By iteratively rewriting its own code based on empirical feedback, DGM demonstrates how AI systems could move beyond human-designed architectures to autonomously explore new designs, self-improve, and potentially give rise to entirely new species of digital intelligence. If this trend continues, we may witness a\nCambrian explosion\nin AI development, where eventually AI systems will surpass human-designed architectures and give rise to entirely new species of digital intelligence. While this future looks promising, achieving it requires addressing significant challenges, including:\nEvaluation Framework\n: Need for more comprehensive and dynamic evaluation systems that better reflect real-world complexity and prevent “reward hacking” while ensuring beneficial AI evolution.\nResource Optimization\n: DGM’s evolution is computationally expensive\nThe paper mentioned that a complete SWE-bench experiment takes about two weeks and about $22,000 in API call costs.\n, thus improving efficiency and reducing costs is crucial for broader adoption.\nSafety & Control\n: As AI self-improvement capabilities grow, maintaining alignment with human ethics and safety becomes more challenging.\nEmergent Intelligence\n: Need to develop new approaches to understand and interpret AI systems that evolve beyond human-designed complexity, including new fields like “AI interpretability” and “AI psychology”.\nIn my view, DGM is more than a technical breakthrough, but rather a philosophical milestone. It invites us to rethink the boundaries of intelligence, autonomy, and life itself. As we advance toward Life 3.0, our role shifts from mere designers to guardians of a new era, where AI does not just follow instructions, but helps us discover what is possible.\nPlease enable JavaScript to view the\ncomments powered by giscus.\n© Copyright 2025 Richard Cornelius Suwandi. Last updated: June 04, 2025."
  }
]